\documentclass[bsc,frontabs,oneside,singlespacing,parskip,deptreport]{infthesis}

\batchmode
\usepackage{agda}
\usepackage{catchfilebetweentags}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage{enumitem}

\input{commands}

\theoremstyle{definition}
\newtheorem*{definition}{Definition}

\setcounter{secnumdepth}{3}

\begin{document}

\title{Type-preserving closure conversion of PCF in Agda (more to come)}

\author{Piotr Jander}

\course{Master of Informatics}
\project{{\bf MInf Project (Part 2) Report}}

\date{\today}

\abstract{
This is an example of {\tt infthesis} style.
The file {\tt skeleton.tex} generates this document and can be
used to get a ``skeleton'' for your thesis.
The abstract should summarise your report and fit in the space on the
first page.
%
You may, of course, use any other software to write your report,
as long as you follow the same style. That means: producing a title
page as given here, and including a table of contents and bibliography.
}

\maketitle

\section*{Acknowledgements}
Acknowledgements go here.

\tableofcontents

%\pagenumbering{arabic}


\chapter{Introduction}

The document structure should include:
\begin{itemize}
\item
The title page  in the format used above.
\item
An optional acknowledgements page.
\item
The table of contents.
\item
The report text divided into chapters as appropriate.
\item
The bibliography.
\end{itemize}

Commands for generating the title page appear in the skeleton file and
are self explanatory.
The file also includes commands to choose your report type (project
report, thesis or dissertation) and degree.
These will be placed in the appropriate place in the title page.

The default behaviour of the documentclass is to produce documents typeset in
12 point.  Regardless of the formatting system you use,
it is recommended that you submit your thesis printed (or copied)
double sided.

The report should be printed single-spaced.
It should be 30 to 60 pages long, and preferably no shorter than 20 pages.
Appendices are in addition to this and you should place detail
here which may be too much or not strictly necessary when reading the relevant section.

\section{Using Sections}

Divide your chapters into sub-parts as appropriate.

\section{Citations}

Note that citations
(like \cite{P1} or \cite{P2})
can be generated using {\tt BibTeX} or by using the
{\tt thebibliography} environment. This makes sure that the
table of contents includes an entry for the bibliography.
Of course you may use any other method as well.

\section{Options}

There are various documentclass options, see the documentation.  Here we are
using an option ({\tt bsc} or {\tt minf}) to choose the degree type, plus:
\begin{itemize}
\item {\tt frontabs} (recommended) to put the abstract on the front page;
\item {\tt twoside} (recommended) to format for two-sided printing, with
  each chapter starting on a right-hand page;
\item {\tt singlespacing} (required) for single-spaced formating; and
\item {\tt parskip} (a matter of taste) which alters the paragraph formatting so that
paragraphs are separated by a vertical space, and there is no
indentation at the start of each paragraph.
\end{itemize}






\chapter{Related literature}
\section{Closure conversion}

Closure conversion is a compilation phase where functions or lambda
abstractions with free variables are transformed to /closures/. A
closure consists of a body (code) and the /environment/, which is a
record holding the values corresponding to the free variables in the
body (code). Closure conversion transforms abstractions to closures,
and replaces references to variables with lookups in the environment.

Closure conversion was necessarily used in every compiler for a
language which supports functions with free variables (TODO wording:
scope?). But the first work which provided a rigorous treatment of
closure conversion was the paper "Typed Closure Conversion" by
Minamide et al. \cite{TODO}. It demonstrated type-preserving closure
conversion, where closure environments have existential types (TODO
wording). On top of a proof of type-safety, the paper contains a proof
of operational correctness of the typed closure conversion algorithm
by logical relations.

Another notable paper about closure conversion is "Typed Closure
Conversion Preserves Observational Equivalence" by Ahmed and Blum
\cite{TODO}. The paper's title explains its main result, so we should
explain the title.

(TODO bring up the Reynolds' paper) Within a language L, we have a
program P = C[A], where A is an implementation of an abstraction and C
is the "context", or "the rest of the program". Given some other
implementation A' of the abstraction, we say that A and A' are
contextually equivalent when for all possible contexts C, programs P =
C[A] and P' = C[A'] behave identically.

We say that another abstraction A' is contextually equivalent to A if
for all contexts C, programs C[A] and C[A'] are equivalent. This
corresponds to a programmer's intuition that A and A' behave in the
same way in all possible programs.

TODO OE matters for security and safety: If an attack would be
possible by exposing a certain implementation detail, then this detail
is made inaccessible / private, for example by using an existential
type.

Why this matters: modern software systems are made up of multiple
components, of which some might not be trusted.

// To ensure reliable and secure operation, it is important to defend
against faulty or malicious code. Language-based security is built
upon the concept of abstraction: if access to some private
implementation detail might enable an attack, then this detail is made
inaccessible by hiding it behind an abstract interface, for example
using an existential type. //

TODO I have quite a bit about the paper and we don't want to duplicate
the paper's introduction: how do I make it shorter?

\section{Verified compilation}

Closure conversion is just one possible verification phase, and its
verification constitutes part of a wider effort to verify compilation
end-to-end, which usually entails verifying type preservation or
operational correctness of all compilation phases.

As far as type safety is concerned, the reference is a paper by
Morrisett et al., "From System F to Typed Assembly Language"
\cite{TODO}. It builds upon previous results in type safety of
compilation phases (like the aforementioned \cite{TCC}) and describes
a typed RISC-like assembly (named TAL), which is the target of the
final phases of compilation. As a whole, the paper proves type safety
for a compilation pipeline from System F to TAL. It does not, however,
prove end-to-end operational correctness.

The first compiler which was verified for end-to-end operational
correctness was described by Adam Chlipala in his paper "A Certified
Type-Preserving Compiler from Lambda Calculus to Assembly
Language". The source is a variant of the simply-typed lambda calculus
(STLC). Compilation proceeds through six phases, eventually yielding
idealised assembly code. The compiler is implemented in Coq, where
terms and functions on terms are dependently typed, guaranteeing type
preservation. This is also the approach taken in this project, except
that we use Agda instead of Coq \cite{TODO}. Operational
correctness is proved by adopting denotational semantics, unlike in
this project, which uses operational semantics. Due to unfamiliarity
with operational semantics, we cannot comment on which approach is
better (TODO or can we?).

A final example of a certified compiler is CompCert \cite{TODOcompcert}, which is the result of the first successful attempt to
implement a certified compiler of a real-world (TODO wording)
language. Even compared with the simply-typed lambda calculus (STLC),
which was the source language in Chlipala's work \cite{TODO}, the C
language is in some ways simpler, especially since it does not have
first-class functions with free variables (TODO wording:
scoping?). But, being a fully-fledged language, C presents enough
challenges as the source language of a verified compiler.

\chapter{Background}

This chapter will introduce the relevant concepts. It will start with
closure conversion, then discuss compilation phases and intermediate
languages, and finally explain the Agda definitions and encodings
which were borrowed from ACMM and PLFA.

\section{Closure conversion}

TODO explain and give an example

TODO explain why existential types

\section{Compilation phases and intermediate representations}

In all but the most trivial compilers, compilation proceeds in
phases, or transformations. A compilation phase transforms the
compilation unit to bring it one step closer from the source code to
the target representation.

[diagram here]

\paragraph{Intermediate representations} As illustrated in the figure,
each compilation phase takes a source representation to a target
representation [relate to diagram]. An intermediate representation can
also be called an intermediate language, and abbreviated to IR or
IL. For some phases, the source and target representation may be the
same. Arguably, this is the case for constant expression folding.

However, other phases benefit from using different source and target
representations. An example of such transformation is closure
conversion, which as the reader may recall from [section], transforms
abstractions with free variables to so called closures, which take an
explit environement and can only reference values from that
environment.

\paragraph{Typed and untyped IRs} To question of whether closure
conversion must necessarily use different source and target languages
hinges on the distinction between typed and untyped intermediate
language. Using a typed IR requires that at each point along the
compilation pipeline, intermediate representantions are well-typed.

Suppose that closure conversion is performed on simply typed lambda
calculus (STLC) (without existential types). Then the target representation
cannot be the same (STLC), as STLC does not have existential types,
and closure environments must be existentially typed in order for
programs to be well-typed in general, as discussed in [section]. This
is why an intermediate language with existential types is necessary.

On the other hand, if the source and target representations are
untyped, then the compiler architect might get away with using the
same intermediate language as both source and target (for example
Scheme, which is sometimes used as a compilation target). But even
in this case, compilation process might benefit if the abstract syntax
has explicit closures.

\paragraph{IRs in this project} This project uses a dependently typed
meta language (Agda) to implement compilation phases (specifically,
closure conversion), so typed intermediate representations are a
natural choice. Therefore, in the following sections, we will describe
two intermediate representations, which are both variants of lambda
calculus. The source representation will be simply typed lambda
calculus, which we will refer to as STLC or Œªst. The target
representation will be simply typed lambda calculus with closures,
denoted with Œªcl.

The two intermediate representations are similar, and differ mainly in
having either abstractions with free variables in Œªst or closures with
environments in Œªcl. Unfortunately, this means that formalisations of
Œªst and Œªcl share a lot of duplication. This is a common problem in
formalising languages which has recently been addressed by
\cite{DBLP:journals/pacmpl/AllaisA0MM18}. Whether techniques from
Allais et al. are applicable to this work will be discussed in [related
work]. On the other hand, [section] demonstrates that while two
intermediate languages can only differ in a handful of syntactic
constructs and reduction steps, they can behave very differently with
respect to the ubiquitious operations of renaming and substitution.

\section{Type- and scope-safe representation of simply typed lambda
  calculus Œªst}

This section will discuss the encoding of simply typed lambda calculus
(abbreviated as STLC, denoted with Œªst), which is the source language
of closure conversion. Typing and reduction rules are standard for
call-by-value lambda calculus, so it is the encoding in Agda which is
of interest in this section. As similar encoding is used for the
closure language Œªcl.

Using dependently typed Agda as the meta language allows us to encode
certain invariants in the representation. Two such invariants are
scope and type safety. The representation is scope-safe in the sense
that all variables in a term are either bound by some binder in the
term, or explicitly accounted for in the context. It is type-safe in
the sense that terms are synonymous with their typing derivations,
which makes ill-typed terms unrepresentable. This kind of scope and
type safety is due to \cite{DBLP:conf/csl/AltenkirchR99}. The rest of
this section shows how this is achieved in Agda; the Agda encoding is
based on the one used in \cite{DBLP:conf/cpp/Allais0MM17},
\cite{DBLP:journals/pacmpl/AllaisA0MM18}, and
\cite{DBLP:conf/sbmf/Wadler18}.

TODO STLC as a figure here

To start with, Œªst typed are defined as follows.

\ExecuteMetaData[StateOfTheArt/Types.tex]{type}

The context is simply a list of types.

\ExecuteMetaData[StateOfTheArt/Types.tex]{context}

Variables are synonymous with proofs of context membership. Since a
variable is identified by its position in the context, it is
appropriate to call it a de~Bruijn variable. Accordingly, the
constructors of \AS{Var} are named after \textit{zero} and
\textit{successor}. Notice that the definition assumes that the
leftmost type in the context corresponds to the most recently bound
variable.

\ExecuteMetaData[StateOfTheArt/Types.tex]{var}

We can now present the formulation of Œªst terms, which is synonymous
with their typing derivations:

\ExecuteMetaData[StateOfTheArt/STLC.tex]{terms}

The syntactic variable \AS{V} constructor takes a de~Bruijn variable to
a term. The abstraction constructor \AS{L} requires that the body is
well-typed in the context \AS{Œì} extended with the type \AS{œÉ} of the
variable bound by the abstraction. The application constructor
\AS{A} follows the typing rule for application.


\section{Type- and scope-safe programs}
\label{sec:typ-scop-saf-prog}

Many useful traversals of the abstract syntax tree involve maintaining
a mapping from free variables to appropriate values. Two such
traversals are simultaneous renaming and substitution.

Simultaneous renaming takes a term \AS{N} in the context \AS{Œì}. It maintains
a mapping \AS{œÅ} from variables in the original context \AS{Œì} to
\textit{variables} in some other context \AS{Œî}. It produces a term in
\AS{Œî}, which is \AS{N} with variables renamed with \AS{œÅ}.

Similarly, simultaneous substitution takes a term \AS{N} in the context
\AS{Œì}. It maintains a mapping \AS{œÉ} from variables in the original context
\AS{Œì} to \textit{terms} in some other context \AS{Œî}. It produces a
term in \AS{Œî}, which is \AS{N} with variables substitution for with \AS{œÉ}.

Before we can demonstrate an implementation of renaming and
substitution, we need to formalise the notion of a mapping from free
variables to appropriate values, which we call the
\textit{environment}.

\ExecuteMetaData[STLC.tex]{env}

A environment \AS{(Œì ‚îÄEnv) ùì• Œî} encapsulates a mapping from variables in
\AS{Œì} to values \AS{ùì•} (variables for renaming, terms for
substitution) which are well-typed and -scoped in \AS{Œî}.

An environment which maps variables to variables is important enough
to deserve its own name.

\ExecuteMetaData[STLC.tex]{thinning}

There is a notion of an empty environment \AS{Œµ}, of extending an
environment \AS{œÅ} with a value \AS{v}: \AS{œÅ ‚àô v}, and of mapping a
function \AS{f} over an environment \AS{œÅ}: \AS{f <\$> œÅ},
corresponding to the analogous operations on contexts (which are just
lists). Finally, \AS{select ren œÅ} renames a variable with \AS{ren}
before looking it up in \AS{œÅ}.

\ExecuteMetaData[STLC.tex]{envops}

Notice that those four operations on environments are defined using
copatterns \cite{DBLP:conf/popl/AbelPTS13} by `observing` the
behaviour of \AS{lookup}.

Equipped with the notion of environments, we can give an
implementation of renaming and substitution:

\ExecuteMetaData[StateOfTheArt/STLC.tex]{rename}
\ExecuteMetaData[StateOfTheArt/STLC.tex]{subst}

Notice that those two traversals are indentical except (1)
\textit{renaming} wraps the result of \AS{lookup œÅ x} in \AS{V}, and
\textit{renaming} and \textit{substitution} extend the environment in
a different way: \AS{s <\$> œÅ ‚àô z} vs \AS{rename (pack s) <\$> œÉ ‚àô V
  z}. The observation that renaming and substitution for STLC share a
common structure was a basis was the unpublished manuscript by McBride
\cite{mcbride2005type}, and subsequently motivated the ACMM paper
\cite{DBLP:conf/cpp/Allais0MM17}. In [section], we will show how ACMM
abstracts this common structure of renaming and substitution into a
notion of a semantics.

To see the usefulness of simultaneous renaming and substitution,
consider that once an identity substitution is defined (one which
leaves its argument unchanged):

\ExecuteMetaData[StateOfTheArt/STLC.tex]{id-subst}

Then defining a single substitution is simple. (A single substitution
replaces occurrences of the last-bound variable in the context, and it
is useful for defining the beta reduction for abstractions).

\ExecuteMetaData[StateOfTheArt/STLC.tex]{single-subst}

\section{ACMM's notion of a semantics}

TODO ACMM, synch, fusions

\section{Small-step operational semantics}

The formalisation of small step semantics for a call-by-value lambda
calculus is adapted from \cite{DBLP:conf/sbmf/Wadler18}.

Values are terms which do not reduce further. In this most basic
version of lambda calculus language, the only values are abstractions:

\ExecuteMetaData[StateOfTheArt/STLC.tex]{values}

Our operational semantics include two kinds of reduction rules. Compatibility rules, whose
names start with \AS{Œæ}, reduce parts of the term (specifically, the LHS
and RHS of application). Beta reduction \AS{Œ≤-L}, on the other hand,
describes what an abstraction applied to a value reduces to.

\ExecuteMetaData[StateOfTheArt/STLC.tex]{reductions}

A term which can take a reduction step is called a reducible
expression, or a redex. A property of a language that every well-typed
term is either a value or a redux is called type-safety. This property
is captured by a slogan `well-typed terms don't get stuck` and can be
proved by techniques like `progress and preservation` or logical
relations. Simply typed lambda calculus is type-safe, and so is this
formalisation. For a proof of type safety for a similar formalisation
of STLC, cf. \cite{DBLP:conf/sbmf/Wadler18}.

Operational semantics are needed for the treatment of bisimulation.

\chapter{The Agda development}
\label{cha:agda-development}

This chapter presents the parts of the Agda development which are
original to this project. It starts by discussing the closure language
Œªcl, an intermediate language which is like STLC but with abstractions
replaced by closures. Then it demonstrates a type-preserving
conversion for Œªst to Œªcl which has the property that the obtained
closure environments are `minimal`.  Finally, a result is presented
about the source and target programs of closure conversion being in a
bisimulation.

\section{Closure language Œªcl}
\label{sec:closure-language-cl}

As discussed in the Background [or maybe Intro?] chapter, some
compilation phases must use different source and target intermediate
representations. This is the case with closure conversion, and this
section presents a formalisation of an intermediate language with
closures. The language is very similar the formalised simply typed
lambda calculus, except that abstraction with free variables are
replaced by closures with environments. What might seem like a simple
change has interesting implications for traversals like renaming and
substitution.

The closure language Œªcl shares types, contexts, and
de-Bruijn-variables-as-proofs-of-context-membership, and their
respective Agda formalisations, with the source representation. In
general, two different intermediate representations do not need to
share the same type system, but if they do, this simplifies
formalisation. The descriptions of those formalisations can be found
in Section~[TODO].

\subsection{Terms}
\label{sec:closure-language-cl-1}

The definition of terms of Œªcl differs from terms of Œªst in the \AS{L}
constructor, which, in Œªcl, holds the closure body and the closure
environment.

\ExecuteMetaData[StateOfTheArt/Closure.tex]{terms}

Notice that the typing rule for the closure constructor \AS{L}
mentions two contexts, \AS{Œì} and \AS{Œî}. We call \AS{Œì} the
\textit{outer context} and \AS{Œî} the \textit{inner context} of a
closure.

\begin{minipage}{.5\textwidth}
  \[
  \frac
  {\Gamma , x : \sigma \vdash e : \tau}
  {\Gamma \vdash \lambda x : \sigma . e : \sigma \rightarrow \tau}
  \text{T-abs}
  \]
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \[
  \frac
  {e_{ev} = subst ( \Delta \subseteq \Gamma ) \quad \quad \Delta , x : \sigma \vdash e : \tau}
  {\Gamma \vdash \langle\langle \lambda x : \sigma . e \; , \; e_{ev} \rangle\rangle : \sigma \rightarrow \tau}
  \text{T-clos}
  \]
\end{minipage}

The closure as a whole is typed in \AS{Œì}, but the closure body (also
called the \textit{closure code}) is typed in \AS{œÉ ‚à∑ Œî}. The
relationship between \AS{Œì} and \AS{Œî} is given by the closure
environment.

A closure environment is traditionally implemented as a record, and
variables in the closure code reference fields of that record. In this
development, on the other hand, the environment is represented as a
substitution environment, that is, a mapping from variables in \AS{Œî}
to terms in \AS{Œì}. This representation is isomorphic to the one using
a record, and it has several benefits, especially eliminating the need
for products in the language, and overall simplification of the
formalisation.

Finally, recall from [section] that in order for a closure-converted
program to be well-typed, a closure environment should have an
existential type. It is important to note that in this formalisation,
existential typing is achieved in the meta language Agda, not in the
object language Œªcl, which does not have existential types. Indeed,
existential quantification (including over types) can in achieved in
Agda through dependent products, a datatype constructor is a dependent
product, and the environment is a parameter to the \AS{L} constructor.

\subsection{Renaming and substitution}
\label{sec:renam-subst}

Consider the case for the constructor \AS{L} of renaming and
substitution in Œªcl and how it is different from the corresponding
definition in Œªst.

\ExecuteMetaData[StateOfTheArt/Closure.tex]{rename}
\ExecuteMetaData[StateOfTheArt/Closure.tex]{subst}

Unlike in Œªst, renaming and substitution in Œªcl \textit{do not go
  under binders} (do not change the closure body). This is because
renaming and substitution take a term in a context \AS{Œì} to a term in
a context \AS{Œì'}. But the code (body) of a closure is typed in a
different context \AS{Œî}. So upon recursing on a closure, renaming and
substitution adjust the closure environment and leave the closure body
unchanged. The adjustment to the environment is \AS{rename œÅ <\$> E}
in the case of renaming and \AS{subst œÅ <\$> E} in the case of
substitution. In either case, the adjustment consists of mapping the
renaming/substitution over the values in the environment.

The fact that in Œªcl, renaming and substitution do not go under
binders will allow us to prove `fusion lemmas` in [section] without
using the machinery of ACMM, which will significantly simplify the
proofs.

\subsection{Operational semantics}
\label{sec:oper-semant}

Operational semantics are similar to the semantics for Œªst, except for
adjustments for closures. Values in Œªcl are closures, and the rule for
beta reduction is different:

\ExecuteMetaData[StateOfTheArt/Closure.tex]{beta}

Recall that a closure is a function without free variables,
partially applied to an environment. When the closure argument reduces
to a value, the argument and the values in the environment get
simultaneously substituted into the closure body. The simplicity of
this reduction rule is another benefit of representing environments as
substitution environments.

\subsection{Conversion from Œªst to Œªcl}
\label{sec:conversion-from-st}

This project's approach to typed, or type-preserving, closure
conversion follows \cite{DBLP:conf/popl/MinamideMH96}. An important
point here is that the specification of typed closure conversion
allows for different implementations which might differ in their
treatment of environments. The only requirement in the specification
is that

\begin{enumerate}
\item If the source term is an abstraction typed in the context
  \AS{Œì};
\item if the body of the source abstraction can be typed in a smaller
  context \AS{Œî}, such that \AS{Œî ‚äÜ Œì};
\item then the target terms is a closure whose environment is a
  substitution from \AS{Œî} to \AS{Œì}.
\end{enumerate}

This is given by the following conversion rule:

\[
  \frac
  {e_{ev} = subst (\Delta \subseteq \Gamma) \quad \quad \Delta , x : \sigma \vdash e \leadsto e' : \tau }
  {\Gamma \vdash \lambda x : \sigma . e \leadsto
    \langle\langle \lambda x : \sigma . e' \; , \; e_{ev} \rangle\rangle : \sigma \rightarrow \tau}
\]

It is up to the implementation of closure conversion to decide how big
to make \AS{Œî}, on the spectrum between (1) \AS{Œî} being equal to
\AS{Œì}, and (2) \AS{Œî} being `minimal`, i.e. only containing the parts
of \AS{Œì} which are necessary to type the term. We present two Agda
implementation of closure conversion, corresponding to the two ends of
the spectrum.

Closure conversion where \AS{Œî} is the same as \AS{Œì} is a simple
transformation:

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{convert}

where \AS{T.id-subst} is the identity substitution which maps a term
in \AS{Œì} to itself, defined as:

\ExecuteMetaData[StateOfTheArt/Closure.tex]{id-subst}

We call the other end of the spectrum \textit{minimising closure
  conversion}. Its implementation in Agda is rather more involved and
is described in the next section.

\subsection{Minimising closure conversion}
\label{sec:minim-clos-conv}

Minimising closure conversion is given by the following deduction
rules, where a statement \AS{Œì ‚ä¢ e : œÉ ‚Üù Œî ‚ä¢ e' : œÉ} should be read as:
`the term \AS{e} of type \AS{œÉ} in the context \AS{Œì} can be closure
converted to the term \AS{e'} in \AS{Œî}`:

\begin{minipage}{.5\textwidth}
  \[
    \frac
    {}
    {\Gamma \vdash x : \sigma \leadsto \emptyset , x : \sigma \vdash x : \sigma}
    \;\text{(min-V)}
  \]
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \[
    \frac
    {
      \begin{matrix}
        \Gamma \vdash e_1 : \sigma \to \tau \leadsto \Delta_1 \vdash
        e_1' : \sigma \to \tau \\
        \Gamma \vdash e_2 : \sigma \leadsto \Delta_2 \vdash e_2' :
        \sigma \\
        \Delta = merge \; \Delta_1 \; \Delta_2
      \end{matrix}
      }
    {\Gamma ‚ä¢ e_1 e_2 : \tau  \leadsto \Delta  \vdash e_1' e_2' : \tau}
    ‚ÄÑ\;\text{(min-A)}
  \]
\end{minipage}

\[
  \frac {\Gamma , x : \sigma ‚ä¢ e : \tau \leadsto \Delta , x : \tau
    \vdash e : \tau \quad \quad e_{id} = subst ( \Delta \subseteq
    \Delta )}
  {\Gamma \vdash \lambda x : \sigma . e : \sigma \to \tau \leadsto
    \Delta \vdash \langle\langle \lambda x : \sigma . e \; , \; e_{id}
    \rangle\rangle : \sigma \to \tau} \; \text{(min-L)}
\]

\textbf{min-V}: Any variables can be typed in a singleton context
containing just the type of the variable.

\textbf{min-A}: If the conversion \AS{e‚ÇÅ'} of \AS{e‚ÇÅ} can be typed in
$\Delta_1$, and the conversion \AS{e‚ÇÇ'} of \AS{e‚ÇÇ} can be typed in \AS{Œî‚ÇÇ},
then the application \AS{e‚ÇÅ' e‚ÇÇ'} can be typed in \AS{Œî}, where \AS{Œî}
is the result of merging \AS{Œî‚ÇÅ} and \AS{Œî‚ÇÇ}.

\textbf{min-L}: If the conversion \AS{e'} of the abstraction body
\AS{e} can be typed in context \AS{œÉ ‚à∑ Œî} (or $\Delta, x : \sigma$,
using the notation with names), then the closure resulting from the
conversion of the abstraction can be typed in \AS{Œî}, and it has the
identity environment $\Delta \subseteq \Delta$.

To formalise this conversion in Agda, we need several helper
definitions.

\subsubsection{Merging subcontexts}
\label{sec:merging-subcontexts}

The deduction rules for minimising closure conversion contained
statements of the form \AS{Œî ‚äÜ Œì}, which reads: `\AS{Œî} is a
subcontext of \AS{Œì}`. Since in this development, a context is just a
list of types, the notion of subcontexts can be captured with the
\AS{\_‚äÜ\_} (sublist) relation from Agda's standard library. The
inductive definition of the relation is:

\ExecuteMetaData[StateOfTheArt/Sublist.tex]{sublist}

This project's contribution is to define the operation of merging two
subcontexts. Given contexts \AS{Œì}, \AS{Œî}, and \AS{Œî‚ÇÅ} such that
\AS{Œî ‚äÜ Œì} and \AS{Œî‚ÇÅ ‚äÜ Œì}, the result of merging the subcontexts
\AS{Œî} and \AS{Œî‚ÇÅ} is a context \AS{Œì‚ÇÅ} which satisfies the following
conditions:

\begin{enumerate}
\item It is contained in the big context: \AS{Œì‚ÇÅ ‚äÜ Œì}.
\item It contains the small contexts: \AS{Œî ‚äÜ Œì‚ÇÅ} and \AS{Œî‚ÇÅ ‚äÜ Œì‚ÇÅ}.
\item The proof that \AS{Œî ‚äÜ Œì} obtained by transitivity from \AS{Œî ‚äÜ
    Œì‚ÇÅ} and \AS{Œì‚ÇÅ ‚äÜ Œì} is the same as the input proof that \AS{Œî ‚äÜ
    Œì}; similarly for \AS{Œî‚ÇÅ ‚äÜ Œì}.
\end{enumerate}

All those requirements are captured by the following dependent record
in Agda:

\ExecuteMetaData[StateOfTheArt/SubContext.tex]{sublistsum}

The type of the function which merges two subcontexts can be stated
as:

\ExecuteMetaData[StateOfTheArt/SubContext.tex]{merge}

We argue that the type of the function completely captures its
behaviour (TODO how would we prove this?). The fact that a type can
completely capture the behaviour of a function is a remarkable feature
of programming with dependent types. Even more remarkable is the fact
that the logical properties of \AS{Œì‚ÇÅ} are useful computationally. E.g
the proof that \AS{Œî ‚äÜ Œì‚ÇÅ} determines a renaming from \AS{Œî} to
\AS{Œì‚ÇÅ}, which is used in the minimising closure conversion
algorithm. A further example: the fact that \AS{‚äÜ-trans Œî‚äÜŒì‚ÇÅ Œì‚ÇÅ‚äÜŒì ‚â°
  Œî‚äÜŒì} is used in proofs of certain equivalences involving subcontexts
and renaming.

\subsection{Agda implementation of minimising closure conversion}
\label{sec:agda-impl-minim}

Recall that terms of our intermediate languages are explicitly typed
in a given context. For that reason, the result type of minimising
closure conversion must be existentially quatified over a
context. In fact, the context should be a subcontext of the input
context \AS{Œì}. This is captured with the dependent record
\AS{\_‚ä©\_}:

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{ex-subctx-trm}

For example, a term \AS{N} in a context \AS{Œî} which is a subcontext
of \AS{Œì} by \AS{Œî‚äÜŒì}, would be constructed as \AS{‚àÉ[ Œî ] Œî‚äÜŒì ‚àß N}.

With this data type, the type of the minimising closure conversion
function is:

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{min-cc}

The function definition is by cases:

\textbf{Variable case}

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{min-cc-v}

Following \textit{min-V}, a variable is typed in a singleton
context. The proof of the subcontext relation is computed from the
proof of the context membership by a function \AS{Var‚Üí‚äÜ}.

\textbf{Application case}

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{min-cc-a}

Given an application \AS{e‚ÇÅ e‚ÇÇ}, \AS{e‚ÇÅ} and \AS{e‚ÇÇ} are closure
converted recursively, resulting in terms \AS{e‚ÇÅ'} and \AS{e‚ÇÇ'}, which
are typed in \AS{Œî‚ÇÅ} and \AS{Œî‚ÇÇ}, respectively. Following
\textit{app-V}, the result of closure-converting the application is
typed in the context \AS{Œî}, which is the result of merging \AS{Œî‚ÇÅ}
and \AS{Œî‚ÇÇ}. As terms are explicitly typed in a context, \AS{e‚ÇÅ'} and
\AS{e‚ÇÇ'} have to be renamed from \AS{Œî‚ÇÅ} to \AS{Œî}, and from \AS{Œî‚ÇÇ}
to \AS{Œî}, respectively. A renaming environment is computed from a
subcontext relation proof by the function \AS{‚äÜ‚ÜíœÅ} which is given by:

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{subctx-to-ren}

\textbf{Abstraction case}

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{min-cc-l}

Following \textit{min-A}, the result of closure-converting an
abstraction depends on the result \AS{N‚Ä†} of closure-clonverting its
body. A recursive call on the body of the abstraction yields a term
typed in some context \AS{Œî}. But looking at the typing rule for
closures (\textit{T-clos}), the closure body is typed in a context
\AS{œÉ ‚à∑ Œî‚ÇÅ} (or \AS{Œî‚ÇÅ, x : œÉ} using named variables), where \AS{œÉ} is
the type of the last bound variable and \AS{Œî‚ÇÅ} is the context
corresponding to the closure environment. Thus, we need a way of
decomposing \AS{Œî} into \AS{œÉ} and \AS{Œî‚ÇÅ}, together with an
appropriate proof of membership in the input context \AS{Œì}.

This task is achieved by the function \AS{adjust-context}:

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{adjust-context-f}

whose specification is captured by its return type which uses the
dependent record \AS{AdjustContext}:

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{adjust-context-t}

The specification is: given \AS{Œî ‚äÜ A ‚à∑ Œì}, there exists a context
\AS{Œî‚ÇÅ} such that \AS{Œî‚ÇÅ ‚äÜ Œì} and \AS{Œî ‚äÜ A ‚à∑ Œî‚ÇÅ}, such that the
proof \AS{Œî ‚äÜ A ‚à∑ Œì} obtained by transitivity is the same as the input
proof.

The evidence that \AS{Œî ‚äÜ A ‚à∑ Œî‚ÇÅ} is used to rename \AS{N‚Ä†} so that
the final inherently-typed term is well-typed.

\subsection{Fusion lemmas for the closure language Œªcl}

When studying the meta-theory of a calculus, one systematically needs
to prove fusion lemmas for various traversals. A fusion lemma relates
three traversals: the pair we sequence and their sequential
composition. The two traversals which have to be fused in later proofs
are renaming and substitution. There are four ways we can sequence
renaming and substitution, and each of those four sequencing can be
expressed as a single renaming or substitution:

\begin{enumerate}[nolistsep]
  \item A renaming followed by a renaming,
  \item A renaming followed by a substitution,
  \item A substitution followed by a renaming,
  \item A substitution followed by a substitution.
\end{enumerate}

We state the results as signatures of Agda functions, using the
environment combinators \AS{\_<\$>\_} and \AS{select} which are described
in Section~\ref{sec:typ-scop-saf-prog}.

\ExecuteMetaData[StateOfTheArt/Closure-Thms.tex]{rename-rename}
\ExecuteMetaData[StateOfTheArt/Closure-Thms.tex]{subst-rename}  
\ExecuteMetaData[StateOfTheArt/Closure-Thms.tex]{rename-subst} 
\ExecuteMetaData[StateOfTheArt/Closure-Thms.tex]{subst-subst}

Rather than include Agda proofs of all four lemmas, here we outline
the proof structure, analyse just one of the four proofs, and compare
fusion lemmas for Œªcl with the corresponding lemmas for Œªst.

A generic technique to prove fusion lemmas for STLC, including the
ones about renaming and substitution, is one of the main contributions
of ACMM \cite{DBLP:conf/cpp/Allais0MM17}. Their proof uses Kripke
logical relations and it relies on the invariant that corresponding
environment values are in appropriate relations, including when
environments are extended when going under a binder. Maintaining this
invariant is possible thanks to the generic framework for writing
traversals introduced by ACMM.

As it turns out, fusion lemmas for the closure language are simpler,
as they do not require the logical relation machinery of ACMM. This is
because renaming and substitution in Œªcl `do not go under binders`, as
can be seen from their definitions in
Section~\ref{sec:renam-subst}. For both renaming and substitution, in
the closure case (\AS{L}), the closure body is left untouched; only
the closure environment is modified.

We are now ready to take a closer look at the proof of the fusion
lemma stating that a renaming followed by a subsitution is a
substitution:

\ExecuteMetaData[StateOfTheArt/Closure-Thms.tex]{subst-rename} 
\ExecuteMetaData[StateOfTheArt/Closure-Thms.tex]{subst-rename-proof} 

The proof is by induction on the typing derivation of the term:

\begin{itemize}
\item In the variable case, the LHS and the RHS normalise to the same
  term, so \AS{refl} suffices.
\item In the application case, the proof is by induction and
  congruence.
\item In the closure case, the proof is also by congruence, but an
  equational proof is required to show that the LHS and RHS act in the
  same way on the environment \AS{E}.
\end{itemize}

The equational proof proceeds as follows:

\begin{enumerate}[nolistsep]
\item It uses the fact that function composition \AS{\_‚àò\_}
  distributes through mapping over environments \AS{\_<\$>\_}: we have
  \AS{f <\$> g <\$> E ‚â° f ‚àò g <\$> E} which is capture by the lemma
  \AS{<¬†\$>-distr},
\item It uses the fact that when \AS{f} and \AS{g} are extensionally
  equal (\AS{‚àÄ \{x\} ‚Üí f x ‚â° g x}), then \AS{f <\$> E ‚â° g <\$> E} which
  is captured by the lemma \AS{<\$>-fun},
\item \AS{<\$>-fun} is instantiated with the inductive hypothesis.
\end{enumerate}

Unfortunately, Agda does not recognise this project's fusion lemmas as
terminating, and we were unable to provide a termination proof. Still,
we believe that the function does in fact terminate.

\section{Bisimulation}
\label{sec:bisimulation}

Preceding sections defined the source and target languages of the
closure conversion, Œªst and Œªcl, together with reduction rules for
each, and a closure conversion function \AS{min-cc} from Œªst to Œªcl.

This project's implementation of closure conversion is type- and
scope-preserving by construction. The property of type preservation
provides confidence in the compilation process, but in this
theoretical development which deals with a small, toy language, it is
within the reach of this project to prove properties about operational
correctness.

One such operational correctness property of a pair of languages is
\textbf{bisimulation}. Intuition about bisimulation is captured by a
slogan: similar terms reduce to similar terms.

[TODO outline]

\subsection{Similarity relation}
\label{sec:similarity-relation}

Before we can give an exact statement of bisimulation, we need a
definition which captures the notion of similarity between the source
terms of Œªst and target terms of Œªcl.

\begin{definition}
  Given a term \AS{M} in Œªst and a term \AS{M‚Ä†} in Œªcl,
  the similarity relation \AS{M \tis M‚Ä†} is defined inductively as
  follows:

  \begin{itemize}
  \item (\textit{Variable}) For any given variable (proof of context
    membership) \AS{x}, we have \AS{S.` x \tis T.` x}.

  \item (\textit{Application}) If \AS{M \tis M‚Ä†} and \AS{N \tis N‚Ä†},
    then \AS{M ¬∑ N \tis M‚Ä† ¬∑ N‚Ä†}.

  \item (\textit{Abstraction}) If \AS{N \tis subst (exts E) N‚Ä†},
    then \AS{S.L N \tis T.L N‚Ä† E}.
    
  \end{itemize}
\end{definition}

Recall that Œªst and Œªcl share types, contexts, and variables (proofs
of context membership). In fact, similarity is only defined for source
and target terms of the same type in the same context (this is
explicit in the Agda definition).

Therefore, similarity of (syntactic) variables can be defined in terms
of identity of proofs of membership.

Similarity of applications is defined by `compatibility`: given
similar functions and similar arguments, the applications are similar.

Finally, the non-trivial case of abstractions. It uses a function
\AS{exts}, which extends a substitution environment with a newly
bound variable.

\ExecuteMetaData[StateOfTheArt/Closure.tex]{exts}

What are the necessary conditions for \AS{S.L N \tis T.L N‚Ä† E}, where
\AS{S.L N} and \AS{T.L N‚Ä† E} are defined in a context \AS{Œì}? Clearly,
we cannot require that \AS{N \tis N‚Ä†}, as the context \AS{Œî} in which
the closure body is defined is different from \AS{Œì}. However, recall
that the closure environment \AS{E} is defined as a substitution from
\AS{Œî} to \AS{Œì}. Applying this substitution, extended with a newly
bound variable, to the closure body (\AS{subst (exts E) N‚Ä†})
results in a term in \AS{Œì} which can be in a similarity relation with
\AS{N}, and this is precisely what is required in the definition.

The application of \AS{exts} accounts for the fact that the closure
body is defined in the context \AS{œÉ ‚à∑ Œî} (or \AS{Œî, x : œÉ}, using a
notation with names).

The definition of similarity is reminiscent of `compatibity lemmas`,
which is a usual name for a kind of result which states that similar
terms can be composed to form similar terms. Except here, similarity
is an inductive definition, not a lemma.

Similarity is defined in Agda as follows:

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{tilde}

The definition of similarity might seem arbitrary, but we argue that
the graph relation of any well-behaved closure conversion function is
contained within the similarity relation.

For example, consider the trivial closure conversion algorithm
\AS{simple-cc}, which uses full contexts as environments (through
identity substitutions \AS{id-subst}):

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{convert}

Indeed, the graph relation of \AS{simple-cc} is contained in the
similarity relation. The proof is by straightforward induction; in the
abstraction case, we need to argue that applying an identity
substitution leaves the argument term unchanged.

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{graph}

\subsubsection{The minimising closure conversion and the
  similarity relation}
\label{sec:minim-clos-conv-1}

Similarly, the graph relation of the minimising closure conversion
function is also contained in the similarity relation.

To be precise, we define the function \AS{\_‚Ä†}:

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{dag}

This function is a wrapper over the \AS{min-cc} function which undoes
the minimisation on the outer level. In other words, all closures in
the term are still minimised, but the outer term is typed in the same
context as the input source term, so that the statement `contained in
the similarity relation` type-checks.

The claim is that 

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{min-cc-sim-t}

The proof of the claim is too long to discuss here, but the reader can
find it in the technical appendix of this report.

***

With the notion of similarity formalised, bisimulation can be defined.

\section{Bisimulation}

Bisimulation is a two-way property which is defined in terms of a
simpler one-way property of simulation.

\begin{definition}{}
Given two languages \AS{S} and \AS{T} and a similarity relation
\AS{\_\textasciitilde\_}
between them, \AS{S} and \AS{T} are in \textbf{simulation} if and only
if the following holds:
Given source language terms \AS{M} and \AS{N}, and a target language
term \AS{M‚Ä†} such that \AS{M} reduces to \AS{N} in a single step (\AS{M ‚Äî‚Üí N}) and
\AS{M} is similar to \AS{M‚Ä†} (\AS{M \~ M‚Ä†}), there exists a target
language term \AS{N‚Ä†} such that \AS{M‚Ä†} reduces to \AS{N‚Ä†} in some
number of steps (\AS{M‚Ä† ‚Äî‚Üí* N‚Ä†}) and \AS{N} is similar to \AS{N‚Ä†} (\AS{N \~ N‚Ä†}).
\end{definition}

\begin{definition}
Given two languages \AS{S} and \AS{T},  \AS{S} and \AS{T} are in a
\textbf{bisimulation} if and only if \AS{S} is in a simulation with \AS{T} and
\AS{T} is in a simulation with \AS{S}.
\end{definition}

The essence of simulation can be captured in a diagram.

TODO diagram here

TODO give names to the source and target langs

In fact, our source and target languages of closure conversion have a
stronger property: \textit{lock-step} bisimulation, which is defined
in terms of \textit{lock-step} simulations. A lock-step simulation is
one where for each reduction step of the source term, there is exactly
one corresponding reduction step in the target language. We illustrate
this at another diagram:

TODO another diagram

Before we can prove that Œª and ŒªT are in simulation, we need three
lemmas:

\begin{enumerate}
\item
  Values commute with similarity. If \AS{M \ti M‚Ä†} and \AS{M} is a
  value, then \AS{M‚Ä†} is also a value.

\item
  Renaming commutes with similarity. If \AS{œÅ} is a renaming from
  \AS{Œì} to \AS{Œî}, and \AS{M \ti M‚Ä†} are similar terms in the context
  \AS{Œì}, then the results of renaming \AS{M} and \AS{M‚Ä†} with \AS{œÅ}
  are also similar: \AS{S.rename œÅ M \ti T.rename œÅ M‚Ä†}.

\item
  Substitution commutes with similarity. Suppose \AS{œÅ} and \AS{œÅ‚Ä†} are two
  substitutions which take variables \AS{x} in \AS{Œì} to terms in \AS{Œî},
  such that for all \AS{x} we have that \AS{lookup œÅ x \ti lookup œÅ‚Ä†
    x}. Then given similar terms \AS{M \ti M‚Ä†} in \AS{Œì}, the results
  of applying \AS{œÅ} to \AS{M} and \AS{œÅ‚Ä†} to \AS{M‚Ä†} are also
  similar: \AS{S.subst œÅ M \ti T.subst œÅ‚Ä† M‚Ä†}.
\end{enumerate}

The proof that values commute with similarity is straightforward.

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{val-comm} 

Before we will be able to prove the lemmas about renaming and substitution, we need an interlude
where we discuss so-called fusion lemmas for the closure language ŒªT.

TODO acknowledge PLFA here


\section{Back to \ti rename and \ti subst}

Recall the two result which we said would be needed for showing
bisimulation. We start with:

\textit{Renaming commutes with similarity.} If \AS{œÅ} is a renaming from
\AS{Œì} to \AS{Œî}, and \AS{M \ti M‚Ä†} are similar terms in the context
\AS{Œì}, then the results of renaming \AS{M} and \AS{M‚Ä†} with \AS{œÅ}
are also similar: \AS{S.rename œÅ M \ti T.rename œÅ M‚Ä†}.

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{rename-comm} 

The proof is by induction on the similarity relation. The variable and
application cases are easy, but as ever, the abstraction case is worth
looking at. Recall that a source abstraction is similar to the target
closure \AS{S.L N \ti T.L N‚Ä† E} when \AS{N \ti T.subst (T.exts E) N‚Ä†}
by the inductive constructor \AS{\ti L}.

In the abstraction case, we have that

\AS{S.L N \ti T.L N‚Ä† E} (1)

We need to show that

\AS{S.rename œÅ (S.L N) \ti T.rename œÅ (T.L N‚Ä† E)} (2)

but (2) simplifies to

\AS{S.L (S.rename (S.exts œÅ) N) \ti T.L N‚Ä† (T.rename œÅ <\$> E)} (3)

which holds by \AS{\ti L} when the following holds

\AS{S.rename (S.exts œÅ) N \ti T.subst (T.exts (T.rename œÅ <\$> E)) N‚Ä†} (4)

On the other hand, from (1) by \AS{\ti L} we have that

\AS{N \ti T.subst (T.exts E) N‚Ä†} (5)

Applying the induction hypotesis to (5), we get

\AS{S.rename (S.exts œÅ) N \ti T.rename (S.exts œÅ) (T.subst (T.exts E) N‚Ä†)} (6)

Thus we require (4) and have (6), so to complete the proof, we need to
show that 

\AS{T.subst (T.exts (T.rename œÅ <\$> E)) N‚Ä† ‚â° T.rename (S.exts œÅ) (T.subst
  (T.exts E) N‚Ä†)}

or, in Agda

\ExecuteMetaData[StateOfTheArt/Closure-Thms.tex]{lemma-ren-comm}

This indeed holds, and the proof uses the fusion lemma lemmas for
renaming and substitution in several places.

The remaining result to prove is quite similar, but the concept of a
pointwise similar substitution makes it worth analysing.

\textit{Substitution commutes with similarity.} Suppose \AS{œÅ} and \AS{œÅ‚Ä†}
are two 
substitutions which take variables \AS{x} in \AS{Œì} to terms in \AS{Œî},
such that for all \AS{x} we have that \AS{lookup œÅ x \ti lookup œÅ‚Ä†
  x}. Then given similar terms \AS{M \ti M‚Ä†} in \AS{Œì}, the results
of applying \AS{œÅ} to \AS{M} and \AS{œÅ‚Ä†} to \AS{M‚Ä†} are also
similar: \AS{S.subst œÅ M \ti T.subst œÅ‚Ä† M‚Ä†}.

The notion of pointwise-similar substitutions \AS{œÅ} and \AS{œÅ‚Ä†} from
\AS{Œì} to \AS{Œî}
can be captured by a function which, for each variables \AS{x} in
\AS{Œì}, produces a proof that that the corresponsing terms are
similar: \AS{lookup œÅ x \ti lookup œÅ‚Ä† x}. We encapsulate this function
in an Agda record:

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{pointwise-sim}

The notion of a pointwise relation between environments (like
substitutions) is used by ACMM to prove synchronisation and fusion
lemmas for STLC. Unlike for STLC, the closure language \lcl does not
require us to prove that pointwise similarity is preserved as a
traversal goes under a binder.

We can, however, show that pointwise similarity is preserved by
applying \AS{exts} to both substitutions:

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{pointwise-sim-exts}

In fact, exteding pointwise-similar substitutions with similar terms
preserves pointwise similarity:

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{pointwise-sim-extend}

With the notion of pointwise similarity, we can prove that
substitution commutes with similarity:

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{subst-comm}

TODO finish

\chapter{Proof by logical relations}

As we mentioned in \ref{TODO}, there are two standard methods for
proving operational correctness of a translation: bisimulations and
logical relations [TODO wording from Dreyer's
paper]. Chapter~\ref{cha:agda-development} discussed an Agda
mechanisation of a proof of bisimulation for [TODO wording] closure
conversion. This chapter presents a mechanisation of the other proof
method, [by/with?] logical relations?

The chapter starts with a presentation of an modified formalisation
of the source and target languages of closure conversion. Then, a
pen-and-paper proof by logical relations is given, and finally, its
Agda formalisation.

\section{Alternative formalisation of the intermediate languages}
\label{sec:altern-form-interm}

This section presents an alternative formalisation of the source and
target languages of closure conversion. We call the new formalisation
of the source language Œªst', and the new formalisation of the target
language - Œªcl'. Compared with Œªst and Œªcl in
Chapter~\ref{cha:agda-development}, Œªst' and Œªcl' are different in two
ways.. Firstly, the distinction between values and non-values is
made explicit in the definition of terms in Œªst' and Œªcl', replacing a predicate on
terms in Œªst and Œªcl. Secondly, we give big-step semantics for Œªst'
and Œªcl', in contrast to small-step semantics for Œªst and Œªcl. These
two differences simplify mechanisation of a proof by logica relation.

These improvements in formalisation are inspired by an Agda
formalisation accompanying \cite{DBLP:conf/cpp/McLaughlinMS18}.

[TODO maybe only discuss STLC?]

The definitions of types, contexts, variables as proofs of context
membership, and environments, are the same as for Œªst and Œªcl in the
previous chapter. The definition of language expressions is different,
however, in that it makes an explicit distinction between values \AS{Val} and
non-values \AS{Trm}. This is achieved by indexing the \AS{Exp} data
type by a \AS{Kind}:

\ExecuteMetaData[LR/Base.tex]{kind}

\ExecuteMetaData[LR/STLC.tex]{exp}

Notice that there are two new constructors for language
expressions. The first one is \AS{`val}, which takes a value \AS{Val}
to a term \AS{Trm} and thus makes it possible to use values in
positions where terms are expected. The second is \AS{`let}, which is
a standard let construct. The let is necessary to make the evaluation
order explicit: function application applies a value to a value, so
nested computations need to be factored out and bound as values by a
let expression. This representation is known as A-normal form
\cite{DBLP:conf/lfp/SabryF92}, and it is used for Œªst' and Œªcl' as it
simplifies the definition as big-step semantics.

Definition of renaming and substitution are similar to those for
Œªst, so we do not include the updated versions here. Instead, we define
aliases for closed values \AS{Val‚ÇÄ} and closed terms \AS{Trm‚ÇÄ} (typed
in an empty context):

\ExecuteMetaData[LR/STLC.tex]{ground}

Like it was mentioned, the semantics of Œªst are defined as big-step
semantics. Given a term \AS{M} and a value \AS{V}, the inductive
definition \AS{M ‚áì V} states the conditions for \AS{M} to reduce to a
value \AS{V}:

\ExecuteMetaData[LR/STLC.tex]{big-step}

It is worth explaining the \AS{‚áìstep} constructor and the \AS{M ‚Üí‚ÇÅ M'}
data type. The \AS{M ‚Üí‚ÇÅ M'} data type describes the small-step
reducton relation and has a single constructor which captures beta
reduction for functions. The \AS{‚áìstep} constructor is similar to the
transitive closure of the small-step reduction relation: if \AS{M}
reduces to \AS{M'} in a single step, and \AS{M'} reduces to \AS{V} in
multiple steps, then \AS{M} reduces to \AS{V} in multiple steps. [TODO
maybe complete the explanation].

Finally, ... [TODO what to make of a non-terminating proof of
termination?]

\ExecuteMetaData[LR/STLC.tex]{sn}

Differences between Œªcl and Œªcl' are analogous.

\section{Proving correctness of a translation with logical relations}
\label{sec:logical-relations}

This section defines two relations between terms of Œªst' and Œªcl', one
syntactic, and one operational. The syntactic relation, which we call
a \textit{compatibility relation}, subsumes the graph relation of any
closure conversion, and the operational relation captures the
intuition that similar terms reduce to similar values. We prove that
the syntactic relation entails the operational relation, and thus,
that any well-behaved closure conversion function preserves
operational correctness. The proof uses type-indexed logical relations
and is inspired by a sketch of a similar proof from
\cite{DBLP:conf/popl/MinamideMH96}.

For brevity, we do not duplicate translation functions from Œªst to
Œªst, but just like we did in Section~\ref{sec:similarity-relation}, we
expect that every the graph relation of every well-behaved translation
from Œªst' to Œªcl' will be contained in the compatibility relation
\AS{‚âÖ}. In general, given a term \AS{M‚ÇÅ} in Œªst' and a term \AS{M‚ÇÇ} in
Œªcl', \AS{M‚ÇÅ} and \AS{M‚ÇÇ} are in the compatibility relation (M‚ÇÅ ‚âÖ M‚ÇÇ)
when their subterms are in the compatibility relation. In the special
case of abstractions/closures, the closure body is renamed with the
environment in the premise.

\ExecuteMetaData[LR/LR.tex]{compat}

The compatibility relation provides handy inductive hypotheses in the
proof by logical relations.

While the compatibility relation captures syntactic correspondence, we
need another relation on (closed) language expressions which captures
operational correspondence. In fact, this relation \AS{‚áî} is
comprised of (1) a relation \AS{\ti} between closed source terms and
closed target terms, and (2) a relation \AS{‚âà} between closed source
values and closed target values. Those two relations are defined by
mutual induction and induction on types [TODO both at the same time?]
as follows:

\AS{œÑ ‚àã M‚ÇÅ \tis M‚ÇÇ} iff \AS{M‚ÇÅ ‚áì V‚ÇÅ}, \AS{M‚ÇÇ ‚áì V‚ÇÇ}, and \AS{œÑ ‚àã V‚ÇÅ ‚âà
  V‚ÇÇ}

\AS{œÉ ‚áí œÑ ‚àã U‚ÇÅ ‚âà U‚ÇÇ} iff for all \AS{œÉ ‚àã V‚ÇÅ ‚âà V‚ÇÇ}, we have \AS{œÑ ‚àã U‚ÇÅ
  `\$ V‚ÇÇ \tis U‚ÇÇ `\$ V‚ÇÇ }

There is no case for \AS{‚âà} at the ground type \AS{Œ±} as only
variables can have the ground type, and the values in \AS{‚âà} are
closed.

In Agda, \AS{\ti} and \AS{‚âà} are defined as specialisations of a
relation \AS{‚áî} on expressions of Œªst and Œªcl. 

\ExecuteMetaData[LR/LR.tex]{related}

Finally, we extend the \AS{‚âà} relation to source and target
substitution environments, similar to what we did in
Section~\ref{TODO-bisim-pointwise-sim}:

\ExecuteMetaData[LR/LR.tex]{pointwise-related}

We also provide a function \AS{‚àôR} which extends two related
substitution environments with a related pair of values:

\ExecuteMetaData[LR/LR.tex]{pointwise-ext}

With those definitions at hands, we can state the Fundamental Theorem
of Logical Relations. Recall that we are trying to show that, given a
closed term \AS{M‚ÇÅ} in Œªst' and a closed term \AS{M‚ÇÇ} in Œªcl, if
\AS{M‚ÇÅ ‚âÖ M‚ÇÇ}, then \AS{M‚ÇÅ \tis M‚ÇÇ} (*). The Fundamental Theorem is:

\ExecuteMetaData[LR/LR.tex]{fund-t}

Thus, the Fundamental Theorem is a stronger statement than (*), and
one which instatiated with closed terms and and empty substitution
environments yields precisely (*).

We do not include the Agda proof here as it is not very readable;
instead, we present several cases on paper: TODO

\chapter{Reflections and evaluation}

This project is a case study on verification of transformations of
functional programs using two different techniques: bisimulations and
logical relations. The implemented transformation is closure
conversion. Both proofs of operational correctness are mechanised with
state-of-the-art techniques.

The \textbf{original objectives} of the project were:

\begin{enumerate}
\item To implement a compiler transformation for a variant of
  simply-typed lambda calculus in Agda.
\item To use scope-safe and well-typed representation for the object
  languages.
\item To prove that the transformation is correct: that the output
  program of the transformation behaves `the same` as the input
  program.
\item To use generic programming techniques from ACMM.
\end{enumerate}

The \textbf{contributions} of this project are as follows:

\begin{enumerate}
\item All the original objectives were achieved.
\item It is demonstrated that languagues with closures and closure
  conversion are problematic for current state-of-the-art techniques
  for mechanising language meta-theory.
\end{enumerate}

\paragraph{(Objective 1) Capturing the essence of closure conversion}
The implemented tranformation --- closure conversion --- requires a
different source and target language. While the formalisation of the
source language is largely borrowed from ACMM, and the formalisation
of the target language is similar except for the difference between
abstractions and closures, this project's contribution was to capture
the essence of closure conversion in what we believe is the simplest
and most elegant way possible.

\paragraph{Inherently typed closures}
A traditional representation of closure conversion replaces variables
in the source program with references to a record containing the
environment in the target program. This project's use of scope-safe
and well-typed terms allowed for a more elegant solution where the
closure body is typed in a context corresponding to the closure's
environment, and variables remain variables.

\paragraph{Closure environments as substitution environments}
Furthermore, while a closure environment is traditionally represented
as a record which stores environment values, this project captures the
essence of an environment by representing it as a substitution
environment, i.e. a mapping from variables to values.

\paragraph{Existential types for closure environments}
As this report points out, closure environment must have existential
types in order for a program with closures to be well-typed. This
observation was made by \cite{DBLP:conf/popl/MinamideMH96}, which
deals with this fact by equipping the closure language with
existential types. This project uses a different, arguably simpler
approach, whereby closure environment are existentially typed
\textit{in the meta language (Agda)}, which allows us to keep object
langauge types simple.

\paragraph{Comparison with traditional closure conversion}
In comparison with traditional closure conversion which represents
environments as records, this formulation, which represents closure
environments as substitution environments, i.e. meta-language
functions, is further removed from the eventual target, which is
machine code. But one can imagine a subsequent compilation phase which
replaces substitution environments with records, and variables with
record lookups (the object language would need existential types
then). In general, splitting the compilation process into many
specialised passes facilitates verification, as each compilation phase
is easier to verify, and composing correctness results about phases
gives rise to a end-to-end correctness result.

\paragraph{(Objetive 2) Scope-safe and well-typed representation}
Both the source and target language have scope-safe and well-typed
representation, which were possible thanks to using dependently-typed
Agda as the meta language. Using inherently scoped and typed terms has
many benefits, which include the fact that when programs are synonymous
with their typing derivations, transformations on programs are
synonymous with proofs of type preservation. Additionally, many
techniques for reasoning about operational correctness are type
directed, e.g. the type-indexed logical relations which we used, and
inherently typed representations are well-matched to such techniques.

\paragraph{(Objetive 3) The closure conversion preserves operational correctness}
This project uses two standard techniques to show that the implemented
closure conversion is correct: bisimulation and logical relations. In
an informal setting of pen-and-paper proofs, both of those techniques
have rather straighforward proofs. However, mechanisation of those
proofs involves proving several lemmas about the interactions between
renaming, substitution, closure conversion, and the compatibility
relation.

\paragraph{Mechanising the meta-theory of a language}
As observed in ACMM, mechanising the meta-theory of a language most
often requires proving lemmas about the interactions between different
transformations, or semantics, like renaming and substitutions. ACMM
singles out synchronisation lemmas, which relate two semantics
(e.g. for every renaming there exists a substitution which behaves the
same), and fusion lemmas (e.g. for every composition of two
substitutions, there exists a substitution which behaves the
same). 

\paragraph{(Objetive 4) ACMM}
ACMM exploit similarity between various traversals (semantics) on
simply typed lambda calculus (STLC) to come up with a generic way to
prove synchronisation and fusion lemmas for STLC. Indeed, this project
uses fusion lemmas about STLC from ACMM in the proof with logical
relations.

Intermediate languages other than STLC require their own definitions
of renaming and substitution, and proofs of correctness lemmas. For
example, the proofs of operational correctness with bisimulations and
logical relations depend on four fusion lemmas relating renaming and
substitution for the language with closures. In fact, proving those
correctness lemmas was the biggest effort in the whole proof.

\paragraph{Possible remedy: AACMM and generic programming}
The problem of having to define renaming and substitution for each new
language, and proving correctness lemmas about the interactions
between renaming, substitution, and transformations, is address by a
follow-up paper, which we will refer to as AACMM
\cite{DBLP:journals/pacmpl/AllaisA0MM18}. AACMM provides a way to
supply a definition of a syntax with bindings, and then derives
meta-theoretical correctness lemmas from that definition. The paper
repository contains an example of an elaboration whose source is a
language with a let construct, and whose target is simply-typed lambda
calculus with let-expressions inlined. The example demonstrates how a
proof of simulation is drastically simplified thanks to the use of the
AACMM library and its generic programing capabilities.

\paragraph{Feasibility of closure conversion in AACMM}
AACMM demonstrates that transformations like let-inlining and CPS
conversion can be expresses in their generic framework. They pose an
open question about which compilation passes can be implemented
generically. Unfortunately, this work suggests that closure conversion
might not fit well into the AACMM framework. Specifically, the
closure language in this project --- with aforementioned features like
syntax being mutually dependent on substitution environments, or
environments being existentially quatified in the meta language (Agda)
--- is not expressible as an AACMM generic syntax. The traditional
representation of a languages with closures --- with environments as
records and existential types in the object language --- would not fit
either as syntaxes in AACMM cannot contain existential types.

\paragraph{Bisimulation vs logical relations}
[TODO what could I say about the pros and cons of both?]

\paragraph{Summary}
This work and ACMM/AACMM are both concerned with mechanising the
meta-theory of languages, and applying this metatheory to reason about
the language. While AACMM shows that a certain class of languages /
syntaxes can be treated generically, this work contains a negative
result which indicates that a language with closures might not benefit
from current techniques for relieving the burden of mechanising
meta-theory. This is an open question, however, whether there exist
feasible generic syntaxes which would encompass a language with
closures, or whether an alternative formalisation of a language with
closures exists which is compatible with AACMM.

[TODO the conclusions of this chapter depend on my understanding of
AACMM --- is it correct?]

\chapter{Relationship to the UG4 project}

This work is a natural continuation of the UG4 project, but it
admittedly takes the project in a new direction. In terms of its
goals, the UG4 project was concerned with program
derivations. Derivations are distinct from program transformations or
traversals, as found in compilers or in this UG5 project.

Program transformations like closure conversion, or continuation
passing style (CPS) transformation, have two distinct characteristics.

\section{Program transformation vs derivation}
\label{sec:progr-transf-vs}

Firstly, the source and target languages can be different, e.g. the
source language may have lambda abstractions with free variables, and
the target language may have closures with environments. Of course,
many transformations in compilers happen within the same language,
e.g. constant expression folding.

Secondly, compiler transformations are usually characterised by
replacing one program construct with another, in a one-way fashion,
e.g. lambda abstractions with closures. It would be strange if changes
happened both ways. One might imagine a language with both lambda
abstractions and closures, and a transformation which replaces some
closures with lambda abstractions, and some abstraction with closures,
according to arbitrary rules. It is difficult to see how such a
transformation would be useful in a compiler.

Of course, one might imagine a transformation which replaces some
occurrences of constructs A and constructs B, and vice versa, in order
to optimise the program. But any such optimising transformation is
still guided by some measure of performance.

In contrast, program derivation consists of transforming a program in
arbitrary places, and using arbitrary rules, with a specific goal of
obtaining one program from another, so that the obtained program has
some desirable features, like efficiency, or even faster asymptotic
running time. Importantly, the derivation happens within a single
language. (TODO but specification can be in terms of relations, which
are not part of the language).

\section{Program derivations in the UG4 project}
\label{sec:progr-deriv-ug4}

The UG4 project analyses two instances of program derivation in
detail. The first one is a derivation of an efficient implementation
of the maximum segment sum problem (MSS). While the input
specification (which is also a runnable program) runs in cubic time in
the length of the input list, the output program runs in linear
time. The asymptotic speed-up is achieved by applying several "rewrite
rules" involving higher-order functions on lists such as map, foldr,
and filter.

The second case study involved a derivation for a program for
matrix-vector multiplication. The input program takes a dense matrix,
and the output program takes a sparse matrix in the compressed sparse
row (CSR) format. Or, to be precise, the input program which acts on a
dense matrix, is transformed into a composition of two programs: (a) a
conversion from a dense to a CSR-sparse matrix, and (b) a
matrix-vector multiplication program which acts on a CSR-sparse
matrix. This is because, as a rule, the input and output types of the
program must stay the same in the course of the derivation. This
second derivation was similarly accomplished with "rewrite rules"
involving higher-order functions.

\section{Rewrite rules}
\label{sec:rewrite-rules}

The notion of a rewrite rule is central to program derivation. A
classical example of a rewrite rule for a function program is one
stating: "a composition of mappings is a mapping with a composition":

\begin{verbatim}
forall f g xs. map f (map g xs) == map (f . g) xs (*)
\end{verbatim}

where f, g, xs are metavariables. An application of a rewrite rule
consists of unifying the LHS of the rule with a subterm of the program
(and thus obtaining a substitution œÉ), and then replacing the
subterm with the RHS of the rule, instantiated with the substitution
œÉ.

Notably, such simple form of a rewrite rule only supports first-order
abstract syntax trees, but not higher-order abstract syntax. (TODO
elaborate on what it would mean to have a context and go under a
binder in a rewrite rule). (TODO can't express conditions)

In their simplest form, a program derivation is a sequence of
intermediate forms of the program, intertwined with rewrite rules
which justify each step of the derivation. For examples, the reader
may consult the UG4 project \cite{TODO}.

\section{Program derivations in compilers and their limitations}
\label{sec:progr-deriv-comp}

While we argued that compiler transformations and program derivations
are distinct in their character, the lines may arguably be blurry at
times. A good example of this is the support for rewrite rules in GHC,
a Haskell compiler. A Haskell programmer may specify a rule like (*)
as part of the code, and in one of early compilation passes, GHC will
apply the rule wherever possible (i.e. replace the occurrences of the
LHS with the RHS). Such compiler pass may be considered an instance of
program derivation, and it would go some way towards deriving the
aforementioned efficient implementation for the maximum segment sum
problem (MSS).

However, rewriting as implemented in compilers is too limited to carry
out most derivations. To see one limitation, consider a derivation
which needs to apply the rule (*) right to left: replace an occurrence
of the RHS with the LHS. But clearly, unguided application of rewrite
rules can only be one way, otherwise it would not terminate.

Another limitation is the fact that "the right" derivation can require
that rules be applied in a specific order. Thus, rewriting a program
becomes a search problem, where rewrites are applied until a program
satisfying some (performance) objective is found. This is the approach
taken by the Lift compiler \cite{TODO}

Yet another limitation is that many derivations elude the notion of an
objective function, thus rendering a search futile. It seems that
human insight is required to guide a derivation.

A final limitation is that there are conceivable rewrite rules which
are only applicable when certain conditions are met. Such conditions
could range algebraic properties of operators to general predicates
and relations on terms. And these are undecidable in general (TODO how
to phrase this).

These considerations, taken together, mean that the technique of
program derivation is more useful to the programmer than the
compiler. Benefits of employing a sort of program derivation (more or
less formal) for the programmer include: (a) a structured process of
obtaining an implementation from specification, (b) greater confidence
about correctness of an implementation, (c) possibility of discovering
further optimisations, and finally (d) a framework for a proof of
correctness. This last use case could be explained as follows: suppose
we can prove the correctness of the "specification" program, and the
correctness of each rewrite rule. Then we can obtain correctness of
the "implementation" program.

\section{Implementation of rewriting in the UG4 project}
\label{sec:impl-rewr-ug4}

The UG4 project included a purpose-built framework for specifying
derivations. The framework included:

\begin{enumerate}
\item A simple functional language with parametricity. The language is
point-free, that is, based on function composition rather than on
lambda abstractions. This was because variables and abstraction are
difficult to implement correctly, as demonstrated by this UG5 project,
and even more difficult to rewrite.

\item A type-checker for the language.

\item Rewriting functionality and declaring derivations as sequences of
  rewrites.

\item An interpreter for the language, which was used to empirically
  verify claims about performance gains from derivations.
\end{enumerate}

Writing the framework was a good exercise in implementing routine
parts of compiler front-ends, such as type checking and
unification. Writing it in Scala made sense given the stretch
objective of compiling the language to Lift, which was not realised,
however.

Importantly, rewrite rules were stated without justification, much as
postulates in Agda. One could prove the rules externally ‚Äì but then
one is pressed to ask, why not express a derivation in a proof
assistant, which supports unification and rewriting natively? Indeed,
with hindsight, we can say with certainty that a proof assistant is
perfectly suited for the job, its only downside being that it requires
considerable expertise, which I did not have during my fourth
year. (TODO I/me?)

\section{Program derivation in Agda: sparse matrix-vector
  multiplication}
\label{sec:progr-deriv-agda}

To complete last year's work, we conduct a derivation of the program
for matrix-vector multiplication which acts on CSR-sparse
matrices. Unlike last year, we can now provide proofs of individual
rewrite rules. Indeed, some proofs are quite involved. TODO whether
and how to do it.

% use the following and \cite{} as above if you use BibTeX
% otherwise generate bibtem entries
\bibliographystyle{plain}
\bibliography{main}

\chapter{Technical appendix}
\label{cha:technical-appendix}

\section{Minimising closure conversion and the similarity relation}
\label{sec:minim-clos-conv-2}

Below is the Agda proof that the graph relation of the minimising
closure conversion function is contained in the similarity relation.

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{min-cc-sim}

\end{document}
