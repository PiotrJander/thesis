\documentclass[bsc,frontabs,oneside,singlespacing,parskip,deptreport]{infthesis}

\batchmode
\usepackage{agda}
\usepackage{catchfilebetweentags}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{url}
\usepackage{enumitem}
\usepackage{tikz-cd}
\usepackage[hidelinks]{hyperref}

\input{commands}

\theoremstyle{definition}
\newtheorem*{definition}{Definition}

\theoremstyle{lemma}
\newtheorem*{lemma}{Lemma}


\setcounter{secnumdepth}{3}

\begin{document}

\title{Verifying Type-and-Scope Safe Program Transformations}

\author{Piotr Jander}

\course{Master of Informatics}
\project{{\bf MInf Project (Part 2) Report}}

\date{\today}

\abstract{ There is an ongoing effort in the programming languages
  community to verify correctness of compilers. A typical compiler
  consist of several compilation passes which use different
  intermediate languages. Type-and-scope safe representation is a
  commonly used encoding for such intermediate languages; it
  facilitates proofs of correctness of compilation phases, including
  proofs by logical relations. However, using such representation
  requires repeatedly implementing and proving considerable
  meta-theoretical boilerplate for each intermediate language used by
  the compiler.

  This project formalises an intermediate language with closures,
  implements a closure conversion algorithm, and mechanises two proofs
  of its correctness: with bisimulations and with Kripke logical
  relations.

  This work builds on a line of research which culminated in a
  state-of-the-art framework for representing languages with binders
  and generically proving their meta-theoretical properties
  \cite{DBLP:journals/pacmpl/AllaisA0MM18}. While this technique is
  useful for certain intermediate langauges, this project shows that
  an otherwise appealing representation of an intermediate language
  with closures is not compatible with the framework.  }

\maketitle


\tableofcontents

%\pagenumbering{arabic}


\chapter{Introduction}
\label{cha:introduction}

This project, at its most general, concerns verifying transformations of
functional programs in compilers.

Functional language, with their rich semantics, expressive types, and
restrictions such as purity, are particularly well-suited to
verification, i.e. proving that a program conforms to a
specification. However, verification is performed with respect to the
semantics of the source program, whereas guarantees are needed about
the compiled machine code. To bridge the
gap between verification of the source code and assurances about the
executable code, the compiler should be proved to preserve the
semantics of the source language.

Verification of compilers is typically achieved by mechanising a proof
of their correctness in a proof assistant like Coq, Isabelle/HOL, or
Agda. Reasoning about individual compilation phases depends on
formalising the semantics of the source and target languages of the
phase. But most languages have a notion of a variable binder and
binding, and a language representation which facilitates reasoning
about the binding structure greatly simplifies the task of showing
semantics preservation.

This project applies a state-of-the-art technique for representing
languages with binders to implement a type-preserving compiler
transformation and prove its correctness with two distinct techniques: 
bisimulation and logical relations.

\section{Motivation}
\label{sec:motivation}

Closure conversion is a compilation phase performed by a typical
compiler which compiles a language with first-class, nested functions to
machine code. Suppose the source and target languages are given by:

\begin{minipage}{.5\textwidth}
  {\centering \AS{S ‚à∑= x | S S | Œªx.S}}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  {\centering \AS{T ‚à∑= x | T T | ‚ü™ Œªx.Œªe.T , E ‚ü´ }}
\end{minipage}

The source language is a variant of simply typed lambda calculus, and
the target language is similar, except that it does not allow
abstractions with free variables. Instead, it has closures,
i.e. records which consist of (a) a function which takes an argument
and an environment record and has its bodies defined in terms of
the argument and the environment, and (b) an environment record. A
closure is a first-class value which simulates a function with free
variables.

Suppose we implement a translation function which transforms a source
program to a target program with closures. We would like to show that
the transformation is correct according to an appropriate definition of
correctness, e.g. that each reduction of the source program is
simulated by a reduction of the target program.

To formalise this claim, we need to define operational semantics for
both languages. And since our representations of the source and target
langauge are type-and-scope safe, we need to define substitution, and
hence renaming. Futhermore, the correctness result depends on multiple
smaller results about the interplay between renaming, substitution,
the translation function, and others.

This project reports on the experience of mechanising two different
correctness proofs for closure conversion. It concludes with a
reflection on a possibility of reducing the mechanisation effort.

\section{Goals and contributions}
\label{sec:goals-contributions}

The main \textbf{goal} of this work is to mechanise a proof of
correctness of a compiler transformation using type-and-scope safe
representation for the intermediate languages. Specifically, the
\textbf{objectives} of this project, all of which have been achieved,
are:

\begin{enumerate}
\item To implement a compiler transformation for a variant of
  simply-typed lambda calculus in Agda.
\item To use scope-safe and well-typed representation for the object
  languages.
\item To prove that the transformation is correct: that the output
  program of the transformation behaves ``the same'' as the input
  program.
\item To use generic programming techniques from ACMM.
\end{enumerate}

Additionally, a \textbf{contribution} of this project is to
demonstrate that languagues with closures and closure conversion are
problematic for current state-of-the-art techniques for generically
proving properties of languages' meta-theory.

\section{Overview and organisation}
\label{sec:overv-organ}

This report is organised as follows:

Chapter~\ref{cha:related-work} contains a literature review and
evaluation of exisiting work on the subject.

Chapter~\ref{cha:background} sets the stage for the rest of the report
by explaining background topics.

Chapter~\ref{cha:agda-development} defines the language with closures
and explains the implementation of the type-preserving,
environment-minimising closure conversion function.

Chapter~\ref{cha:prov-corr-clos} describes a mechanisation of a proof
of bisimulation between the source and target languages of closure
conversion.

Chapter~\ref{cha:proof-logic-relat} describes a mechanisation of a
proof of correctness of closure conversion by logical relations.

Chapter~\ref{cha:refl-eval} evaluates the work by comparing it to the
state-of-the-art, looks at possible improvements, and explains how
this project demonstrates limitations of existing generic proving
techniques.

Chapter~\ref{cha:relat-ug4-proj} explains the relation between this
project and the UG4 project, and applies skills learned this year to
improve last year's solutions. It also discusses differences and
similarities between compiler transformations and program
derivations.

Finally, Chapter~\ref{cha:conclusion} contains a summary of this
work. 

\chapter{Related work}
\label{cha:related-work}

There are several topics within the broad field of programming
languages theory and verification which have special relevance to this
project: (a) representatin languages with bindings, (b) generic
proving of properties of program traversals, and (c) compiler
verification. This chapter provides an overview of work on those
topics.

\section{Representating languages with bindings}
\label{sec:repr-lang-with}

There are clear benefits to mechanising proofs which arise in
programming languages (PL) theory. A typical PL proof is relatively
simple in terms of techniques used, but complex in terms of the number
of cases and bookkeeping burden. An error in a pen-and-paper proof can
invalidate a whole theory. Proof tools address the problem by checking
proofs and helping the user with bookkeeping. And yet currently, a
large proportion of papers submitted to PL conferences do not have an
accompanying mechanisation.

In 2005, a group of PL researchers put forward a hypothesis that
limited adoption of mechanised proofs is caused by a lack of consensus
about optimal ways to mechanise meta-theory of languages, in
particular, languages with bindings. They issued a challenge
\cite{poplmarkreloaded} whose goal was to try to compare different
representations of a particular language with bindings,
F\textsubscript{<:}. In response, a dozen solution were submitted,
using techniques for representing binders and bindings like named
variables, de Bruijn variables, (parametric) higher-order abstract
syntax \cite{DBLP:conf/pldi/PfenningE88}
\cite{DBLP:conf/icfp/Chlipala08}, nominal sets
\cite{pitts2013nominal}, and other.

This project follows ACMM \cite{DBLP:conf/cpp/Allais0MM17} in using
well-typed and scope-safe de Bruijn indices
\cite{DBLP:conf/csl/AltenkirchR99}. Formalised in
Section~\ref{sec:type-scope-safe}, this representation is a deep
embedding of the object language, and as such, it can be inspected and
modified. However, to ensure that transformations on programs preserve
well-typedness and scope-safety, this representation relies on the
operations renaming and substitution. In meta-theoretical proofs,
there frequently arises a need to prove correctness lemmas about
interactions between renaming, substitution, and other traversals. The
next section discusses research in proving such kind of results
generically.

\section{Generic transformations of and proofs about type-and-scope
  safe programs}
\label{sec:gener-trav-proofs}

McBride's observation \cite{mcbride2005type} that in a type-and-scope
safe language the operations of renaming and substitution share a
common structrure gave rise to a line of research on generic
implementation of such traversals, and generic proofs of their
properties.

A paper by Allais et al. which we will refer to as ACMM
\cite{DBLP:conf/cpp/Allais0MM17} deals with those problems in the
setting of simply typed lambda calculus (STLC). It introduces a notion
of a \textit{semantics}, which is a record defining a traversal in
terms of (a) its result type, (b) the type of values values mapped to
the variables in the environment, (c) semantic counterparts to the
syntactic constructors of STLC, and (d) an operation of
\textit{weaking} which ensured that the traversal remains well-typed
and scope-safe when it recurses on a term under a binder.

In a dependently-typed proof assistant like Agda, the structure of the
proof often mirrors the structure of the program whose properties are
being proved. Therefore, when different traversals share a common
structure, proofs which relate such traversals may be treated
generically. ACMM exploits this fact and provides a generic way to
prove certain classes of properties relating traverals on programs in
STLC.

A follow-up paper by Allais et al., which we will refer to as AACMM,
generalises the contributions of ACMM from the setting of STLC to a
family of languages (syntaxes) which satisfy appropriate
constraints. A framework accompanying the AACMM paper allows the user
to describe a syntax, and then uses generic programming and proving to
generate the operations of renaming and substitutions for the syntax,
together with correctness lemmas describing interactions between
different traversals in the language.

The repository accompanying AACMM has an example demonstrating its
contributions in action. The problem is: given two variants of STLC,
with and without a \textit{let} construct, implement a traversal which
inlines \textit{let} expressions, and prove it correct with a
simulation. Two solution are given. A naive solution contains manual
proofs of correctness lemmas relating different traversals. A solution
using the AACMM framework is able to use generic proofs, and is many
times shorter.

This work relies on results from ACMM and does not attempt to use
AACMM. However, Chapter~\ref{cha:refl-eval} reflects on the
feasibility of applying AACMM-like techniques to closure conversion.

\section{Verified compilation}
\label{sec:verified-compilation}

In his 2003 paper \cite{hoare2003verifying}, Hoare argues that
creating a verifying compiler is one of ``grand challenges'' of
science, comparable to sending a man to the Moon, mapping the human
genome, or finding the Higgs boson. By a verifying compiler, Hoare
meant a suite of tools for specifying program behaviours and checking
software against specifications.

As of 2010s, researchers aspire to go beyond Hoare's challenge and
verify the verifying compiler itself. This is in fact necessary to
bridge the gap between verification of the source program and
verification of the compiled executable.

Typically, verifying compilers involves showing that properties of the
source program are preserved in the target program. Given the same (or
appropriately related) inputs, the source program and the target
produce the same (or appropriately related) result.

Often, when compiling typed languages, the first step in verifying the
compiler is making it type-preserving, especially in the initial
compilation passes. Type-preservation, other than being a property of
a compilation pass in itself, is a prerequisite for type-indexed
correctness proof methods like logical relations.

As far as typed compilation is concerned, a pioneering paper is "From
System F to Typed Assembly Language" by Morrisett et
al. \cite{DBLP:conf/popl/MorrisettWCG98}. Building upon previous
results on typed compilation phases (like
\cite{DBLP:conf/popl/MinamideMH96}), it describes a typed RISC-like
assembly (named TAL), which is the target of the final phases of
compilation. The paper shows how to achieve end-to-end typed
compilation from System F to TAL. It does not, however, attempt to
show any properties about operational correctness of the compiler.

An early examle of a compiler which was verified for end-to-end
operational correctness was described by Adam Chlipala in his paper "A
Certified Type-Preserving Compiler from Lambda Calculus to Assembly
Language" \cite{DBLP:conf/pldi/Chlipala07}. The source language there
is a variant of the simply-typed lambda calculus (STLC). Compilation
proceeds through six phases, eventually yielding idealised assembly
code. The compiler is implemented in Coq, where terms and functions on
terms are dependently typed, guaranteeing type preservation. This is
also the approach taken in this project, except that we use Agda
instead of Coq. Operational correctness is proved by adopting
denotational semantics, unlike in this project, which uses operational
semantics.

Another example of a certified compiler is CompCert
\cite{DBLP:conf/popl/Leroy06}, which accepts a subset of C as its
source language. CompCert is notable as an early successful attempt to
verify a compiler for a real-world language (or a subset thereof). C
present multiple challenges to a compiler verifier, including
undefined behaviours and raw pointer manipulation. However, compiling
C does not involve some of the challenges which arise when compiling
functional languages, especially dealing with first-class
functions. This makes the challenges facing CompCert implementers
quite different from ones explored in this project.

A different example of a verified compiler discussed here is CakeML
\cite{POPL14}. CakeML acceppts a subset a StandardML, a functional
language which is well-suited to verification as its semantics have
been formalised. A special feature of CakeML is that it can
``bootstrap'', or verify correctness of, itself.

Finally, a good reference on verifying transformations of functional
programs is ``A Higher-Order Abstract Syntax Approach to Verified
Transformations on Functional Programs'' by Wang and Nadathur
\cite{DBLP:conf/esop/WangN16}, which verifies three compilation phases
specific to functional languages: continuation passing style (CPS)
transformation, closure conversion, and lambda lifting. Intermediate
languages are formalised in ŒªProlog, and proofs are performed in the
Abella proof assistant.

\chapter{Background}
\label{cha:background}

This chapter sets the stage for the rest of this report by introducing
relevant concepts. It starts by explaining closure conversion, then it
gives a brief overview of the Agda proof assistant, and finally, it
explains the representation of simply typed lambda calculus in Agda,
which were borrowed from ACMM \cite{DBLP:journals/pacmpl/AllaisA0MM18}
and PLFA \cite{DBLP:conf/sbmf/Wadler18}.

\section{Closure conversion}
\label{sec:closure-conversion}

Closure conversion is a compilation phase where functions or lambda
abstractions with free variables are transformed to
\textit{closures}. A closure consists of a \textit{body (code)} and
the \textit{environment}, which is a record holding the values
corresponding to the free variables in the body (code). Closure
conversion transforms abstractions to closures, and replaces
references to variables with lookups in the environment.

We present an example of closure conversion. Let the source language
be simply typed lambda calculus (STLC) with let-expressions and
arithmetic, and let the target language have environments (written
\AS{x=x, y=y}) and closures (written \AS{‚ü™ Œªx.e, \{y=y, z=z\} ‚ü´ }).

Further, suppose the target program is

\AS{let v = 2 in (Œªx. let y = 3 in (Œªz. v + x + y +z))}

Then, the closure converted form is

\AS{let v = 2 in ‚ü™ Œªx.Œªe\textsubscript{x}. let y = 3 in ‚ü™
  Œªz.Œªe\textsubscript{z}. e\textsubscript{z}.v + e\textsubscript{z}.x
  + e\textsubscript{z}.y + z , \{ v = \textsubscript{x}.v, x = x, y =
  y \} ‚ü´ , \{ v=v \} ‚ü´ } 

Notice that each abstraction became a closure, and that closure bodies
only use their single bound variable and reference the environment. In
particular, when building the environment of the inner closure, the
environment of the outer closure is referenced: \AS{v =
  \textsubscript{x}.v}.

We provide typing and conversion rules for closure conversion in
Section~\ref{sec:conversion-from-st}, which discusses the Agda
implementation of closure conversion.

Closure conversion, whether called by this name or not, is part of
most compilers which compile languages with first-class functions to
low-level code. But the first work which provided a rigorous treatment
of closure conversion was the paper "Typed Closure Conversion" by
Minamide et al. \cite{DBLP:conf/popl/MinamideMH96}. It demonstrated
type-preserving closure conversion, which is made possible by giving
closure environments existential types. The paper also contains a
proof of operational correctness of the typed closure conversion
algorithm by logical relations.

\section{The Agda proof assistant}
\label{sec:agda-proof-assistant}

Rather than developing novel proof techniques, this work's
contribution is a mechanisation of known compilation techniques and
their known verification methods using a particular representation in
the proof assistant Agda \cite{DBLP:conf/afp/Norell08}.

Agda is described as a depedently-typed, total functional language or
as a proof assistant for intuitionistic logic. In fact, those two
characterisations are equivalent --- an observation known as
Curry-Howard correspondence \cite{DBLP:journals/cacm/Wadler15}.

Fully explaining Agda is beyond the scope of this short section;
instead, we provide several simple examples, hoping that the reader
will get enough sense of programming and proving in Agda to follow
subsequent uses in this report.

The syntax of Agda is influenced by that of ML and especially
Haskell. A data type for natural numbers could be defined as follows:

\ExecuteMetaData[agda.tex]{nat}

Agda is different from ML and Haskell in having dependent types --- data
types in Agda can be indexed by values. For example, one can define a
vector data type whose type contains the length of any given vector:

\ExecuteMetaData[agda.tex]{vec}

Not only can types of data structures be indexed by values: dependent
types can also be used to provide inductive definitions for predicate
and relations. For example, the ``less than or equal'' relation for
natural numbers can be defines like this:

\ExecuteMetaData[agda.tex]{le}

Finally, since the dependent function type corresponds to universal
quantification is logic, a theorem can be specified as a type, and its
proof given by a program. We illustrate this with a simple inductive
proof of the fact that every natural number is smaller or equal than
its successor:

\ExecuteMetaData[agda.tex]{ex-proof}

Notably, in Agda, recursion over inductively defined data types
corresponds to induction.

The simple example give intuition about rather than a complete
overview of Agda. Still, the reader should be able to read many Agda
definitions in this report treating them as deduction rules.

\section{Type- and scope-safe representation of simply typed lambda
  calculus Œªst}
\label{sec:type-scope-safe}

This section discusses the representation of simply typed,
call-by-value lambda calculus (denoted with Œªst) in Agda, which is the
source language of our closure conversion. A similar encoding is used
for the closure language Œªcl, as explained in
Section~\ref{sec:closure-language-cl}.

Using a dependently-typed language like Agda as the meta language
allows us to encode certain invariants in the representation. Two such
invariants are type and scope safety. The representation is scope-safe
in the sense that all variables in a term are either bound by some
binder in the term, or explicitly accounted for in the context. It is
type-safe in the sense that terms are synonymous with their typing
derivations, which makes ill-typed terms unrepresentable. The rest of
this section shows how this is achieved in Agda. It is part of the
Background chapter as the representation closely resembles
\cite{DBLP:conf/cpp/Allais0MM17},
\cite{DBLP:journals/pacmpl/AllaisA0MM18}, and
\cite{DBLP:conf/sbmf/Wadler18}.

Œªst has a ground type and a funtion type:

\ExecuteMetaData[StateOfTheArt/Types.tex]{type}

A context is simply a list of types.

\ExecuteMetaData[StateOfTheArt/Types.tex]{context}

Variables are synonymous with proofs of context membership. Since a
variable is identified by its position in the context, it is
appropriate to call it a de~Bruijn variable. Accordingly, the
constructors of \AS{Var} are named after \textit{zero} and
\textit{successor}. Notice that the definition assumes that the
leftmost type in the context corresponds to the most recently bound
variable.

\ExecuteMetaData[StateOfTheArt/Types.tex]{var}

Terms of Œªst are synonymous with their typing derivations:

\ExecuteMetaData[StateOfTheArt/STLC.tex]{terms}

The syntactic variable \AS{V} constructor takes a de~Bruijn variable to
a term. The abstraction constructor \AS{L} requires that the body is
well-typed in the context \AS{Œì} extended with the type \AS{œÉ} of the
variable bound by the abstraction. The application constructor
\AS{A} follows the usual typing rule for application.

\section{Type- and scope-safe programs}
\label{sec:typ-scop-saf-prog}

Many useful traversals over the abstract syntax tree involve
maintaining a mapping from free variables to appropriate values. This
can be formalised with the notion of a mapping from free variables to
appropriate values, which we call an \textit{environment}.

\ExecuteMetaData[STLC.tex]{env}

A environment \AS{(Œì ‚îÄEnv) ùì• Œî} encapsulates a mapping from variables in
\AS{Œì} to values \AS{ùì•} (variables for renaming, terms for
substitution) which are well-typed and -scoped in \AS{Œî}.

An environment which maps variables to variables is important enough
to deserve its own name.

\ExecuteMetaData[STLC.tex]{thinning}

There is a notion of an empty environment \AS{Œµ}, of extending an
environment \AS{œÅ} with a value \AS{v}: \AS{œÅ ‚àô v}, and of mapping a
function \AS{f} over values in an environment \AS{œÅ}: \AS{f <\$>
  œÅ}. Finally, \AS{select ren œÅ} renames a variable with \AS{ren}
before looking it up in the environment \AS{œÅ}.

\ExecuteMetaData[STLC.tex]{envops}

Notice that those four operations on environments are defined using
copatterns \cite{DBLP:conf/popl/AbelPTS13} by ``observing'' the
behaviour of \AS{lookup}.

Two especially important traversals are simultaneous renaming and
substitution.

Simultaneous renaming takes a term \AS{N} in the context \AS{Œì}. It maintains
a mapping \AS{œÅ} from variables in the original context \AS{Œì} to
\textit{variables} in some other context \AS{Œî}. It produces a term in
\AS{Œî}, which is \AS{N} with variables renamed with \AS{œÅ}.

Similarly, simultaneous substitution takes a term \AS{N} in the context
\AS{Œì}. It maintains a mapping \AS{œÉ} from variables in the original context
\AS{Œì} to \textit{terms} in some other context \AS{Œî}. It produces a
term in \AS{Œî}, which is \AS{N} with variables substitution for with \AS{œÉ}.

Equipped with the notion of environments, we can give an
implementation of renaming and substitution:

\ExecuteMetaData[StateOfTheArt/STLC.tex]{rename}
\ExecuteMetaData[StateOfTheArt/STLC.tex]{subst}

Notice that those two traversals are indentical except (1)
\textit{renaming} wraps the result of \AS{lookup œÅ x} in \AS{V}, and (2)
\textit{renaming} and \textit{substitution} extend the environment in
a different way: \AS{s <\$> œÅ ‚àô z} vs \AS{rename (pack s) <\$> œÉ ‚àô V
  z}. The observation that renaming and substitution for STLC share a
common structure was a basis for the unpublished manuscript by McBride
\cite{mcbride2005type}, and subsequently motivated the ACMM paper
\cite{DBLP:conf/cpp/Allais0MM17}. 

Also notice how the functions \AS{ext} and \AS{exts} extend the
environment when the traversal goes under a binder.

An instance of simultaneous substitution is single
substitution. Single substitution replaces occurrences of the
last-bound variable in the context, and it is useful for defining the
beta reduction for abstractions. Single substitution environment is an
identity substitution environment extended with a single value:

\ExecuteMetaData[StateOfTheArt/STLC.tex]{id-subst}

\ExecuteMetaData[StateOfTheArt/STLC.tex]{single-subst}

\section{Small-step operational semantics}

The formalisation of small-step semantics for call-by-value lambda
calculus is adapted from \cite{DBLP:conf/sbmf/Wadler18}.

Values are terms which do not reduce further. In this most basic
version of lambda calculus language, the only values are abstractions:

\ExecuteMetaData[StateOfTheArt/STLC.tex]{values}

Our operational semantics include two kinds of reduction rules. Compatibility rules, whose
names start with \AS{Œæ}, reduce parts of the term (specifically, the LHS
and RHS of application). Beta reduction \AS{Œ≤-L}, on the other hand,
describes what an abstraction applied to a value reduces to.

\ExecuteMetaData[StateOfTheArt/STLC.tex]{reductions}

A term which can take a reduction step is called a reducible
expression, or a redex. A property of a language that every well-typed
term is either a value or a redex is called type-safety; it is
captured by the slogan ``well-typed terms don't get stuck'' and can be
proved by techniques like \textit{progress and preservation} or
\textit{logical relations}. Simply typed lambda calculus is type-safe,
and so is this formalisation. For a proof of type safety for a similar
formalisation of STLC, cf. \cite{DBLP:conf/sbmf/Wadler18}.

Operational semantics are needed in Chapter~\ref{cha:prov-corr-clos}
to prove our closure conversion correct with a bisimulation.

\chapter{Formalising closure conversion}
\label{cha:agda-development}

This chapter presents this project's formalisation of closure
conversion. It starts by discussing the closure language Œªcl, an
intermediate language which is like STLC but with abstractions
replaced by closures. Then it demonstrates a type-preserving
conversion for Œªst to Œªcl which has the property that the obtained
closure environments are \textit{minimal}.  Finally, several
properties about interactions between renaming and substitution in Œªcl
are formally established --- they are needed in proofs of correctness
in subsequent chapters.

\section{Closure language Œªcl}
\label{sec:closure-language-cl}

Some compilation phases use different source and target intermediate
representations as is the case with our closure conversion
algorithm. This section presents a formalisation of an intermediate
language with closures. The language is similar to
simply typed lambda calculus, except that abstraction with free
variables are replaced by closures with environments. What might seem
like a simple change has interesting implications for traversals like
renaming and substitution.

The closure language Œªcl shares types, contexts, and
de-Bruijn-variables-as-proofs-of-context-membership, and their
respective Agda formalisations, with the source representation. In
general, two different intermediate representations do not need to
share the same type system, but if they do, this simplifies
formalisation.

\section{Terms}
\label{sec:closure-language-cl-1}

The definition of terms of Œªcl differs from Œªst in the \AS{L}
constructor, which now holds the closure body and the closure
environment.

\ExecuteMetaData[StateOfTheArt/Closure.tex]{terms}

Notice that the typing rule for the closure constructor \AS{L}
mentions two contexts, \AS{Œì} and \AS{Œî}. We call \AS{Œì} the
\textit{outer context} and \AS{Œî} the \textit{inner context} of a
closure.

\begin{minipage}{.5\textwidth}
  \[
  \frac
  {\Gamma , x : \sigma \vdash e : \tau}
  {\Gamma \vdash \lambda x : \sigma . e : \sigma \rightarrow \tau}
  \text{T-abs}
  \]
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \[
  \frac
  {e_{ev} = subst ( \Delta \subseteq \Gamma ) \quad \quad \Delta , x : \sigma \vdash e : \tau}
  {\Gamma \vdash \langle\langle \lambda x : \sigma . e \; , \; e_{ev} \rangle\rangle : \sigma \rightarrow \tau}
  \text{T-clos}
  \]
\end{minipage}

The closure as a whole is typed in \AS{Œì}, but the closure body (also
called the \textit{closure code}) is typed in \AS{œÉ ‚à∑ Œî}. The
relationship between \AS{Œì} and \AS{Œî} is given by the closure
environment.

A closure environment is traditionally implemented as a record, and
variables in the closure code reference fields of that record. In this
development, on the other hand, the environment is represented as a
substitution environment, that is, a mapping from variables in \AS{Œî}
to terms in \AS{Œì}. This representation is isomorphic to the one using
a record, and it has several benefits, especially eliminating the need
for products in the language, and overall simplification of the
formalisation.

Finally, recall from Section~\ref{sec:closure-conversion} that in
order for a closure-converted program to be well-typed, closure
environments should have existential types. It is important to note
that in this formalisation, existential typing is achieved in the meta
language Agda, not in the object language Œªcl, which does not have
existential types.

\section{Renaming and substitution}
\label{sec:renam-subst}

Consider the case for the constructor \AS{L} of renaming and
substitution in Œªcl and how it differs from the corresponding
definition in Œªst.

\ExecuteMetaData[StateOfTheArt/Closure.tex]{rename}
\ExecuteMetaData[StateOfTheArt/Closure.tex]{subst}

Unlike in Œªst, renaming and substitution in Œªcl \textit{do not go
  under binders} (do not change the closure body). This is because
renaming and substitution take a term in a context \AS{Œì} to a term in
a context \AS{Œì'}. But the code (body) of a closure is typed in a
different context \AS{Œî}. So in the closure case, renaming and
substitution adjust the closure environment and leave the closure body
unchanged. The adjustment to the environment is \AS{rename œÅ <\$> E}
in the case of renaming and \AS{subst œÅ <\$> E} in the case of
substitution. In either case, the adjustment consists of mapping the
renaming/substitution over the values in the environment.

Just like in Œªst, we also define functions \AS{ext} and \AS{exts}:

\ExecuteMetaData[StateOfTheArt/Closure.tex]{ext}
\ExecuteMetaData[StateOfTheArt/Closure.tex]{exts}

\section{Operational semantics}
\label{sec:oper-semant}

Operational semantics for Œªcl are similar to the semantics for Œªst,
except they are adjusted to accommodate closures. Values in Œªcl are
closures, and the rule for beta reduction is different:

\ExecuteMetaData[StateOfTheArt/Closure.tex]{beta}

Recall that a closure is a function without free variables,
partially applied to an environment. When the closure argument reduces
to a value, the argument and the values in the environment get
simultaneously substituted into the closure body. The simplicity of
this reduction rule is another benefit of representing environments as
substitution environments.

\section{Conversion from Œªst to Œªcl}
\label{sec:conversion-from-st}

This project's approach to typed, or type-preserving, closure
conversion follows \cite{DBLP:conf/popl/MinamideMH96}. An important
point here is that there are many possible realisations of closure
conversion, which differ in how they construct environments. The only
requirement of any concrete closure conversion is that:

\begin{enumerate}
\item If the source term is an abstraction typed in the context
  \AS{Œì};
\item if the body of the source abstraction can be typed in a smaller
  context \AS{Œî}, such that \AS{Œî ‚äÜ Œì};
\item then the target terms is a closure whose environment is a
  substitution from \AS{Œî} to \AS{Œì}.
\end{enumerate}

This is given by the following conversion rule:

\[
  \frac
  {e_{ev} = subst (\Delta \subseteq \Gamma) \quad \quad \Delta , x : \sigma \vdash e \leadsto e' : \tau }
  {\Gamma \vdash \lambda x : \sigma . e \leadsto
    \langle\langle \lambda x : \sigma . e' \; , \; e_{ev} \rangle\rangle : \sigma \rightarrow \tau}
\]

It is up to the implementation of closure conversion to decide the
exact \AS{Œî}, on the spectrum between (1) \AS{Œî} being equal to
\AS{Œì}, and (2) \AS{Œî} being \textit{minimal}, i.e. only containing
the parts of \AS{Œì} which are necessary to type the term. We present
two Agda implementation of closure conversion, corresponding to the
two ends of the spectrum.

Closure conversion where \AS{Œî} is the same as \AS{Œì} is a simple
transformation:

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{convert}

where \AS{T.id-subst} is the identity substitution which maps a term
in \AS{Œì} to itself, defined as:

\ExecuteMetaData[StateOfTheArt/Closure.tex]{id-subst}

We call the other end of the spectrum \textit{minimising closure
  conversion}. Its implementation in Agda is rather more involved and
is described in the next section.

\section{Minimising closure conversion}
\label{sec:minim-clos-conv}

Minimising closure conversion is given by the following deduction
rules, where a statement \AS{Œì ‚ä¢ e : œÉ ‚Üù Œî ‚ä¢ e' : œÉ} should be read as:
``the term \AS{e} of type \AS{œÉ} in the context \AS{Œì} can be closure
converted to the term \AS{e'} in \AS{Œî}'':

\begin{minipage}{.5\textwidth}
  \[
    \frac
    {}
    {\Gamma \vdash x : \sigma \leadsto \emptyset , x : \sigma \vdash x : \sigma}
    \;\text{(min-V)}
  \]
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \[
    \frac
    {
      \begin{matrix}
        \Gamma \vdash e_1 : \sigma \to \tau \leadsto \Delta_1 \vdash
        e_1' : \sigma \to \tau \\
        \Gamma \vdash e_2 : \sigma \leadsto \Delta_2 \vdash e_2' :
        \sigma \\
        \Delta = merge \; \Delta_1 \; \Delta_2
      \end{matrix}
      }
    {\Gamma ‚ä¢ e_1 e_2 : \tau  \leadsto \Delta  \vdash e_1' e_2' : \tau}
    ‚ÄÑ\;\text{(min-A)}
  \]
\end{minipage}

\[
  \frac {\Gamma , x : \sigma ‚ä¢ e : \tau \leadsto \Delta , x : \tau
    \vdash e : \tau \quad \quad e_{id} = subst ( \Delta \subseteq
    \Delta )}
  {\Gamma \vdash \lambda x : \sigma . e : \sigma \to \tau \leadsto
    \Delta \vdash \langle\langle \lambda x : \sigma . e \; , \; e_{id}
    \rangle\rangle : \sigma \to \tau} \; \text{(min-L)}
\]

\textbf{min-V}: Any variables can be typed in a singleton context
containing just the type of the variable.

\textbf{min-A}: If the result \AS{e‚ÇÅ'} of converting \AS{e‚ÇÅ} can be
typed in $\Delta_1$, and the result \AS{e‚ÇÇ'} of converting \AS{e‚ÇÇ} can
be typed in \AS{Œî‚ÇÇ}, then the application \AS{e‚ÇÅ' e‚ÇÇ'} can be typed in
\AS{Œî}, where \AS{Œî} is the result of merging \AS{Œî‚ÇÅ} and \AS{Œî‚ÇÇ}.

\textbf{min-L}: If the result \AS{e'} of converting the abstraction
body \AS{e} can be typed in context \AS{œÉ ‚à∑ Œî} (or
$\Delta, x : \sigma$, using the notation with names), then the closure
resulting from the conversion of the abstraction can be typed in
\AS{Œî}, and it has the identity environment $\Delta \subseteq \Delta$.

To formalise this conversion in Agda, we need several helper
definitions.

\section{Merging subcontexts}
\label{sec:merging-subcontexts}

The deduction rules for minimising closure conversion contained
statements of the form \AS{Œî ‚äÜ Œì}, which reads: ``\AS{Œî} is a
subcontext of \AS{Œì}''. Since in this development, a context is just a
list of types, the notion of subcontexts can be captured with the
\AS{\_‚äÜ\_} (sublist) relation from Agda's standard library. The
inductive definition of the relation is:

\ExecuteMetaData[StateOfTheArt/Sublist.tex]{sublist}

This project's contribution is to define the operation of merging two
subcontexts. Given contexts \AS{Œì}, \AS{Œî}, and \AS{Œî‚ÇÅ} such that
\AS{Œî ‚äÜ Œì} and \AS{Œî‚ÇÅ ‚äÜ Œì}, the result of merging the subcontexts
\AS{Œî} and \AS{Œî‚ÇÅ} is a context \AS{Œì‚ÇÅ} which satisfies the following
conditions:

\begin{enumerate}
\item It is contained in the big context: \AS{Œì‚ÇÅ ‚äÜ Œì}.
\item It contains the small contexts: \AS{Œî ‚äÜ Œì‚ÇÅ} and \AS{Œî‚ÇÅ ‚äÜ Œì‚ÇÅ}.
\item The proof that \AS{Œî ‚äÜ Œì} obtained by transitivity from \AS{Œî ‚äÜ
    Œì‚ÇÅ} and \AS{Œì‚ÇÅ ‚äÜ Œì} is the same as the input proof that \AS{Œî ‚äÜ
    Œì}; similarly for \AS{Œî‚ÇÅ ‚äÜ Œì}.
\end{enumerate}

All those requirements are captured by the following dependent record
in Agda:

\ExecuteMetaData[StateOfTheArt/SubContext.tex]{sublistsum}

The type of the function which merges two subcontexts can be stated
as:

\ExecuteMetaData[StateOfTheArt/SubContext.tex]{merge}

Observe that the function completely captures its behaviour. The fact
that a type can completely capture the behaviour of a function is a
remarkable feature of programming with dependent types. Even more
remarkable is the fact that the logical properties of \AS{Œì‚ÇÅ} are
useful computationally. E.g the proof that \AS{Œî ‚äÜ Œì‚ÇÅ} determines a
renaming from \AS{Œî} to \AS{Œì‚ÇÅ}, which is used in the minimising
closure conversion algorithm. A further example: the fact that
\AS{‚äÜ-trans Œî‚äÜŒì‚ÇÅ Œì‚ÇÅ‚äÜŒì ‚â° Œî‚äÜŒì} is used in proofs of certain equivalences
involving subcontexts and renaming.

\section{Agda implementation of minimising closure conversion}
\label{sec:agda-impl-minim}

Recall that terms of our intermediate languages are explicitly typed
in a given context. For that reason, the result type of minimising
closure conversion must be existentially quatified over a
context. In fact, the context should be a subcontext of the input
context \AS{Œì}. This is captured with the dependent record
\AS{\_‚ä©\_}:

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{ex-subctx-trm}

For example, a term \AS{N} in a context \AS{Œî} which is a subcontext
of \AS{Œì} by \AS{Œî‚äÜŒì}, would be constructed as \AS{‚àÉ[ Œî ] Œî‚äÜŒì ‚àß N}.

With this data type, the type of the minimising closure conversion
function is:

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{min-cc}

The function definition is by cases:

\textbf{Variable case}

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{min-cc-v}

Following \textit{min-V}, a variable is typed in a singleton
context. The proof of the subcontext relation is computed from the
proof of the context membership by a function \AS{Var‚Üí‚äÜ}.

\textbf{Application case}

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{min-cc-a}

Given an application \AS{e‚ÇÅ e‚ÇÇ}, \AS{e‚ÇÅ} and \AS{e‚ÇÇ} are closure
converted recursively, resulting in terms \AS{e‚ÇÅ'} and \AS{e‚ÇÇ'}, which
are typed in \AS{Œî‚ÇÅ} and \AS{Œî‚ÇÇ}, respectively. Following
\textit{min-A}, the result of closure-converting the application is
typed in the context \AS{Œî}, which is the result of merging \AS{Œî‚ÇÅ}
and \AS{Œî‚ÇÇ}. As terms are explicitly typed in a context, \AS{e‚ÇÅ'} and
\AS{e‚ÇÇ'} have to be renamed from \AS{Œî‚ÇÅ} to \AS{Œî}, and from \AS{Œî‚ÇÇ}
to \AS{Œî}, respectively. A renaming environment is computed from the
evidence for the subcontext relation by the function \AS{‚äÜ‚ÜíœÅ} which is
given by:

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{subctx-to-ren}

\textbf{Abstraction case}

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{min-cc-l}

Following \textit{min-L}, the result of closure converting an
abstraction depends on the result \AS{N‚Ä†} of closure converting its
body. A recursive call on the body of the abstraction yields a term
typed in some context \AS{Œî}. But looking at the typing rule for
closures (\textit{T-clos}), the closure body is typed in a context
\AS{œÉ ‚à∑ Œî‚ÇÅ} (or \AS{Œî‚ÇÅ, x : œÉ} using named variables), where \AS{œÉ} is
the type of the last bound variable and \AS{Œî‚ÇÅ} is the context
corresponding to the closure environment. Thus, we need a way of
decomposing \AS{Œî} into \AS{œÉ} and \AS{Œî‚ÇÅ}, together with an
appropriate proof of membership in the input context \AS{Œì}.

This task is achieved by the function \AS{adjust-context}:

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{adjust-context-f}

whose specification is captured by its return type which uses the
dependent record \AS{AdjustContext}:

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{adjust-context-t}

The specification is: given \AS{Œî ‚äÜ A ‚à∑ Œì}, there exists a context
\AS{Œî‚ÇÅ} such that \AS{Œî‚ÇÅ ‚äÜ Œì} and \AS{Œî ‚äÜ A ‚à∑ Œî‚ÇÅ}, such that the
proof \AS{Œî ‚äÜ A ‚à∑ Œì} obtained by transitivity is the same as the input
proof.

The evidence that \AS{Œî ‚äÜ A ‚à∑ Œî‚ÇÅ} is used to rename \AS{N‚Ä†} so that
the final inherently-typed term is well-typed.

***

We also provide a wrapper function \AS{\_‚Ä†}:

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{dag}

This function is a wrapper over the \AS{min-cc} function which undoes
the minimisation on the outer level. In other words, all closures in
the term are still minimised, but the outer term is typed in the same
context as the input source term. This is useful when we need to
compare the input and output of closure conversion, and need to ensure
that they are typed in the same context.

\section{Fusion lemmas for the closure language Œªcl}

One distinct kind of lemmas about interactions between different
traversals, or semantics, are fusion lemmas.  A fusion lemma relates
three traversals: the pair we sequence and their composition. The two
traversals which have to be fused in this development are renaming and
substitution. There are four ways renaming and substitution can be
composed, and each of those four compositions can be expressed as a
single renaming or substitution:

\begin{enumerate}[nolistsep]
  \item A renaming followed by a renaming is a renaming
  \item A renaming followed by a substitution is a substitution,
  \item A substitution followed by a renaming is a substitution,
  \item A substitution followed by a substitution is a substitution.
\end{enumerate}

We state the results as signatures of Agda functions, using the
environment combinators \AS{\_<\$>\_} and \AS{select} which are described
in Section~\ref{sec:typ-scop-saf-prog}.

\ExecuteMetaData[StateOfTheArt/Closure-Thms.tex]{rename-rename}
\ExecuteMetaData[StateOfTheArt/Closure-Thms.tex]{subst-rename}  
\ExecuteMetaData[StateOfTheArt/Closure-Thms.tex]{rename-subst} 
\ExecuteMetaData[StateOfTheArt/Closure-Thms.tex]{subst-subst}

Rather than include Agda proofs of all four lemmas, here we outline
the proof structure, analyse just one of the four proofs, and compare
fusion lemmas for Œªcl with the corresponding lemmas for Œªst.

A generic technique to prove fusion lemmas about renaming and
substitution in STLC is one of the main contributions
of ACMM \cite{DBLP:conf/cpp/Allais0MM17}. Their proof uses Kripke
logical relations and it relies on the invariant that corresponding
environment values are in appropriate relations, including when
environments are extended when going under a binder.

As it turns out, fusion lemmas for the closure language are simpler,
as they do not require the logical relation machinery of ACMM. This is
because renaming and substitution in Œªcl \textit{do not happen under
  binders}, as can be seen from their definitions in
Section~\ref{sec:renam-subst}. For both renaming and substitution, in
the closure case (\AS{L}), the closure body is left untouched; only
the closure environment is modified.

We are now ready to take a closer look at the proof of the fusion
lemma stating that a renaming followed by a subsitution is a
substitution:

\ExecuteMetaData[StateOfTheArt/Closure-Thms.tex]{subst-rename} 
\ExecuteMetaData[StateOfTheArt/Closure-Thms.tex]{subst-rename-proof} 

The proof is by induction on the typing derivation of the term:

\begin{itemize}
\item In the variable case, the LHS and the RHS normalise to the same
  term, so \AS{refl} suffices.
\item In the application case, the proof is by induction.
\item In the closure case, the proof is also by induction, but an
  equational proof is required to show that the LHS and RHS act in the
  same way on the environment \AS{E}.
\end{itemize}

The equational proof proceeds as follows:

\begin{enumerate}[nolistsep]
\item It uses the fact that function composition \AS{\_‚àò\_}
  distributes through mapping over environments \AS{\_<\$>\_}: we have
  \AS{f <\$> g <\$> E ‚â° f ‚àò g <\$> E} which is capture by the lemma
  \AS{<¬†\$>-distr},
\item It uses the fact that when \AS{f} and \AS{g} are extensionally
  equal (\AS{‚àÄ \{x\} ‚Üí f x ‚â° g x}), then \AS{f <\$> E ‚â° g <\$> E} which
  is captured by the lemma \AS{<\$>-fun},
\item \AS{<\$>-fun} is instantiated with the inductive hypothesis.
\end{enumerate}

\chapter{Proving correctness of closure conversion with a
  bisimulation} 
\label{cha:prov-corr-clos}

The preceding sections defined the source and target language of
closure conversion, Œªst and Œªcl, together with reduction rules for
each, and a closure conversion function \AS{min-cc} from Œªst to Œªcl.

The \AS{min-cc} closure conversion is type- and scope-preserving by
construction. The property of type preservation provides confidence in
the compilation process, but in this theoretical development which
deals with a small, toy language, it is within our reach to prove
properties about operational correctness.

One such operational correctness property of a pair of languages
related by a translation is \textbf{bisimulation}. Intuition about
bisimulation is captured by the slogan: related terms reduce to related
terms.

This chapter starts by defining a relation between terms of Œªst and
terms of Œªcl, which we call a \textit{compatibility relation}. The
compatibility relation is syntactic: in general, two terms are
compatible when their subterms are compatible.

Then, we define what it means for a relation to be a bisimulation. A
bisimulation is a relation which has a semantic property which relates
reduction steps of source and target terms. Next, we will show that
the compatibility relation is a bisimulation.

Finally, we will link the compatibility relation to closure
conversion: we will argue that the graph relation of every sensible
closure conversion function is contained in the compatibility
relation. In particular, we will prove that this is the case for
\AS{min-cc}.

Overall, correctness of the minimising closure conversion is
established: first, by showing that the input and output of closure
conversion are related by a syntactic relation, and second, by showing
that this syntactic relation is also a semantic relation. Thus,
soundness of our closure conversion is established.

The part which shows that the compatibility relation is a bisimulation
is inspired by the ``Bisimulation'' chapter from
\cite{DBLP:conf/sbmf/Wadler18}. 

\section{Compatibility relation}
\label{sec:comp-rel}

\begin{definition}
  Given a term \AS{M} in Œªst and a term \AS{M‚Ä†} in Œªcl,
  the compatibilty relation \AS{M \tis M‚Ä†} is defined inductively as
  follows:

  \begin{itemize}
  \item (\textit{Variable}) For any given variable (proof of context
    membership) \AS{x}, we have \AS{S.` x \tis T.` x}.

  \item (\textit{Application}) If \AS{M \tis M‚Ä†} and \AS{N \tis N‚Ä†},
    then \AS{M ¬∑ N \tis M‚Ä† ¬∑ N‚Ä†}.

  \item (\textit{Abstraction}) If \AS{N ~ T.subst (T.exts E) N‚Ä†}, then
    \AS{S.L N \tis T.L N‚Ä† E}.
    
  \end{itemize}
\end{definition}

Recall that Œªst and Œªcl share types, contexts, and variables (proofs
of context membership). In fact, compatibility is only defined for source
and target terms of the same type in the same context (this is
explicit in the Agda definition).

While the variable and application cases are straightforward, the
abstraction / closure case needs some explanation. Since the body
\AS{N} of the abstraction is defined in \AS{œÉ ‚à∑ Œì}, and the body of
the closure \AS{N‚Ä†} is defined in \AS{œÉ ‚à∑ Œî}, they cannot be
compatible. However, \AS{N} can be compatible with the result of
substituting the environment \AS{E} in \AS{N‚Ä†} (the environment is
extended with a variable corresponding to \AS{œÉ} in the
context). The intuition for the abstraction/closure case is that
substituting the environment ``undoes'' the effect of closure
conversion.  

The compatibility relation is defined in Agda as follows:

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{tilde}

We have defined the syntactic compatibility relation. The next
section states what it means for a relation to be a bisimulation.

\section{Bisimulation}

Bisimulation, as the name implies, is defined in terms on two
simulations: one from source to target terms, and the other one from
target to source terms.

The following definitions relates a two languages, \AS{A} and
\AS{B}.

\begin{definition}
  Given a relation \AS{‚âà} between terms of \AS{A} and terms of \AS{B},
  we say that \AS{‚âà} is a (locks-step) \textbf{simulation} from \AS{A}
  to \AS{B} if and only if for all terms \AS{M} and \AS{N} in \AS{A},
  and \AS{M‚Ä†} in \AS{B}, if \AS{M} reduces in a single step to \AS{N},
  and \AS{M} and \AS{M‚Ä†} are in the \AS{‚âà} relation (\AS{M ‚âà M‚Ä†}),
  then there exists a term \AS{N‚Ä†} in \AS{B} such that \AS{M‚Ä†} reduces
  to \AS{N‚Ä†} in a single step, and \AS{N} is in the \AS{‚âà} relation
  with \AS{N‚Ä†}: \AS{N ‚âà N‚Ä†}.
\end{definition}

The essence of simulation can be captured in a diagram.

\[ \begin{tikzcd}
M \arrow{r}{\longrightarrow} \arrow[swap]{d}{\approx} & N \arrow{d}{\approx} \\%
M \dagger \arrow{r}{\longrightarrow}& N \dagger
\end{tikzcd}
\]

Recall that the \textit{converse} of the relation \AS{‚âà} is a relation
\AS{‚âà'} defined by \AS{y ‚âà' x} whenever \AS{x ‚âà y}.

\begin{definition}
  A relation \AS{‚âà} is a \textbf{bisimulation} if and only if it is a
  simulation and its converse is also a simulation.
\end{definition}

In Agda, we instantiate the definition of simulation twice: once for a
simulation from Œªst to Œªcl, and again for a simulation from Œªcl to
Œªst:

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{simulation}

Then we can provide an Agda definition of a bisimulation:

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{bisimulation}

To show that the compatibility relation is a bisimulation, we need to
obtain lemmas about the interactions between the compatibility
relation, values, renaming, and substitution.

\section{Compatibility, values, renaming, and substitution}
\label{sec:comp-valu-renam}

As discussed in Section~\ref{sec:gener-trav-proofs}, mechanising the
meta-theory of a language involves proving lemmas about the
interactions between various traversals and transformations, including
renaming, substitution, and compilation phases. This is also the case
for proving correctness with bisimulation, which requires establishing
lemmas about the interplay between the compatibility relation, values,
renaming, and substitution. In fact, proving those lemmas often
constitutes the biggest effort in the entire proof. In
Chapter~\ref{cha:refl-eval}, we reflect on the possibiity of
automating this effort with generic proving.

For each relevant property, we state it as an informal lemma, give its
Agda statement, and its Agda proof.

\begin{lemma}{Values commute with compatibility.}
  If \AS{M \tis M‚Ä†} and \AS{M} is a value, then \AS{M‚Ä†} is also a
  value.
\end{lemma}

The proof is by cases of term constructors.

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{val-comm}

\begin{lemma}{Renaming commutes with compatibility.}
  If \AS{œÅ} is a renaming from \AS{Œì} to \AS{Œî}, and \AS{M \tis M‚Ä†} are
  compatible terms in the context \AS{Œì}, then the results of renaming
  \AS{M} and \AS{M‚Ä†} with \AS{œÅ} are also compatible: \AS{S.rename œÅ M
    \tis T.rename œÅ M‚Ä†}.
\end{lemma}

The proof is by induction on the similarity relation.

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{rename-comm} 

The variable and application cases are straightforward, but as ever, the
abstraction case is more involved: it requires rewriting with an
instantiation of the fusion lemma \AS{rename‚àòsubst}.

\ExecuteMetaData[StateOfTheArt/Closure-Thms.tex]{lemma-ren-comm}

The final lemma is about the interplay between compatibility and
substitution. 

\begin{definition}
  Suppose \AS{œÅ} and \AS{œÅ‚Ä†} are two substitutions which take
  variables \AS{x} in \AS{Œì} to terms in \AS{Œî}, such that for all
  \AS{x} we have that \AS{lookup œÅ x \tis lookup œÅ‚Ä† x}. Then we say
  that \AS{œÅ} and \AS{œÅ‚Ä†} are \textit{pointwise compatible}.
\end{definition}

\begin{lemma}
  \textit{Substitution commutes with compatibility}. Suppose \AS{œÅ}
  and \AS{œÅ‚Ä†} are two pointwise compatible substitutions. Then given
  compatible terms \AS{M \ti M‚Ä†} in \AS{Œì}, the results of applying
  \AS{œÅ} to \AS{M} and \AS{œÅ‚Ä†} to \AS{M‚Ä†} are also compatible:
  \AS{S.subst œÅ M \ti T.subst œÅ‚Ä† M‚Ä†}.
\end{lemma}

Pointwise similarity relation between substitutions \AS{œÅ} and \AS{œÅ‚Ä†}
is defined in Agda with \AS{\ti œÉ}:

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{pointwise-sim}

We can show that pointwise similarity is preserved by
applying \AS{exts} to both substitutions:

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{pointwise-sim-exts}

In fact, exteding pointwise-similar substitutions with similar terms 
preserves pointwise similarity:

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{pointwise-sim-extend}

With the notion of pointwise similarity, we can prove that
substitution commutes with similarity:

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{subst-comm}

Just like in the lemma that renaming commutes with compatibility, the
only non-trivial case is the one about abstractions/closures, which
requires rewriting by an instatiation of the fusion lemma
\AS{subst‚àòsubst}.

\ExecuteMetaData[StateOfTheArt/Closure-Thms.tex]{lemma-subst-comm}

With those three lemmas, showing that the compatibility relation is a
bisimulation becomes straightforward.

\section{Compatibility is a bisimulation}
\label{sec:comp-relat-bisim}

The proof that the compatibility relation \AS{\ti} is a bisimulation
consists of two proofs of simulations. Given:

\noindent
\begin{minipage}{.5\textwidth}
  \AS{st-sim : ST-Simulation \_\ti\_}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \AS{ts-sim : TS-Simulation \_\ti\_}
\end{minipage}

we have that \AS{\ti} is a bisimulation:

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{bisim}

The proofs of both \AS{st-sim} and \AS{ts-sim} are by case analysis on
all possible instances of the compabiity relation \AS{\ti} and all
possible instances of the reduction relation. The Agda mechanisation
of the proof can be found in the technical appendix.

\section{Compatibility and closure conversion}
\label{sec:comp-relat-clos}

We have showed that the compability relation is a bisimulation. The
connection between closure conversion and the compatibility relation
is that we require that the graph relation of every-well behaved
closure conversion function \AS{\_‚Ä†} is contained in the compatibility
relation: \AS{M \tis M ‚Ä†}. We cannot quantify over all closure conversion function, so
instead, we must show that this is the case for every function which
we claim is a well-behaved closure conversion. In this section, we
will show that this property is possessed by the trivial closure
conversion \AS{simple-cc} and the minimising closure conversion
\AS{\_‚Ä†}.

While there are many possible closure conversion function, which
differ the big environments they construct, there is a unique
backtranslation from Œªcl to Œªst, which we call \AS{undo}. We can show
that the converse of the graph relation of \AS{undo} is contained in
the compatibility relation: \AS{undo N \tis N}.

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{undo-compat}

\noindent\fbox{%
  \parbox{\textwidth}{%
    \textbf{Side note about proving termination of proofs.}  Notice
    that several functions in this development have been annotated as
    TERMINATING. This annotation is not checked, and if a function is
    annotated incorrectly, it could cause Agda to loop forever during
    typechecking. Furthermore, non-terminating function does not
    corresponds to a proof, and allowing such functions makes Agda's
    logic inconsistent.

    In general, a function terminates if it strictly decreases in one
    of its arguments, and the type of that argument cannot decrease
    infinitely: e.g. natural numbers are bounded from below by zero.
    Agda can tell that an argument decreases when it is evident
    syntactically, but in more complicated cases, an explicit proof
    needs to be provided.

    We can tell by inspection that the \AS{undo} function
    terminates. Suppose we define a \AS{size} measure (function) on
    target terms which is defined as the number of term constructors
    in the term, including in the environment. Then it is easy to see
    that in the closure case, the argument to the recursive call to
    \AS{undo}, \AS{T.subst (T.exts E) M}, has a smaller measure than
    the input argument \AS{ T.L M E}.

    An explicit proof of termination is not of interest in this
    project. The reason for mechanising proofs is to help with
    bookkeeing and prevent errors. When we are certain about a
    property such as termination, there is little value in mechanising
    its proof.  }%
}

Recall the trivial closure conversion \AS{simple-cc} from
Section~\ref{sec:conversion-from-st}. The conversion \AS{simple-cc} is
well-behaved as its graph relation is contained in the compatibility
relation. The proof is by straightforward induction; in the
abstraction case, we need to argue that applying an identity
substitution leaves the argument term unchanged.

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{graph}

The minimising closure conversion \AS{\_‚Ä†} is also well-behaved:

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{min-cc-sim-t}

The proof of this is too long to discuss here, but the reader can find
it in the technical appendix of this report.

***

This concludes the argument that our closure conversion is correct. We
have shown that the graph relation of our closure conversion function
is contained in the compatibility relation, and that the compatibility
relation is a bisimulation. This means that when a source term and its
closure converted target term both take a reduction step, then the
terms they reduce to are also compatible.

\chapter{Proving correctness of closure conversion by logical
  relations}
\label{cha:proof-logic-relat}

Chapter~\ref{cha:prov-corr-clos} shows a correctness property of
closure conversion: the source and target of our closure conversion
are in a relation which is a bisimulation. This chapter demonstrates
another technique for proving correctness: type-indexed logical
relations. Type-indexed logical relations are characterised by using
induction on the type structure of terms.

The outline of this chapter is similar to that of
Chapter~\ref{cha:prov-corr-clos} about bisimulations. First, we
introduce a modified representations of simply typed lambda calculus
(Œªst') and the language with closures (Œªcl'), where terms are
explicitly labelled as values or reducible expressions (this helps
with mechanisation as logical relations treat values and reducible
terms differently). Unlike Œªst and Œªcl, Œªst' and Œªcl' have big-step
semantics. 

Then, the syntactic compatibility relation between Œªst' and Œªcl' is
redefined. Finally, we define a logical relation between Œªst' and
Œªcl', and we formulate the fundamental theorem for the that logical
relation. As a corollary, it follows that the compatibility relation
implies the logical relation for closed terms.

The proof by logical relations is based on
\cite{DBLP:conf/popl/MinamideMH96}, but the Agda mechanisation is this
project's contribution.

\section{Alternative representation of languages}
\label{sec:altern-form-interm}

This section presents an alternative representation of the source and
target language of closure conversion. We call the new formalisation
of the source language Œªst', and the new formalisation of the target
language - Œªcl'. Compared with Œªst and Œªcl in
Chapter~\ref{cha:agda-development}, Œªst' and Œªcl' are different in two
ways. Firstly, the distinction between values and non-values is made
explicit in the definition of terms of Œªst' and Œªcl', replacing a
predicate on terms in Œªst and Œªcl. Secondly, we give big-step
semantics for Œªst' and Œªcl', in contrast to small-step semantics for
Œªst and Œªcl. These two differences simplify mechanisation of a proof
by logica relation.

These improvements in formalisation are inspired by an Agda
formalisation accompanying \cite{DBLP:conf/cpp/McLaughlinMS18}.

The definitions of types, contexts, variables as proofs of context
membership, and environments, are similar to corresponding definitions
for Œªst and Œªcl, except that the groud type is now called \AS{`ùîπ} and
it represents boolean values. The definition of language expressions
is different, however, in that it makes an explicit distinction
between values \AS{Val} and non-values \AS{Trm}. This is achieved by
indexing the \AS{Exp} data type by a \AS{Kind}:

\ExecuteMetaData[LR/Base.tex]{kind}

\ExecuteMetaData[LR/STLC.tex]{exp}

Notice that there are four new constructors for language
expressions.

\begin{itemize}
\item \AS{`val} takes a value \AS{Val} to a term \AS{Trm} and thus
  makes it possible to use values in positions where terms are
  expected.
\item \AS{`tt} and \AS{`ff} are introduction forms for the \AS{`ùîπ}
  type.
\item \AS{`let} is a standard let construct. The let is necessary to
  make the evaluation order explicit: function application applies a
  value to a value, so nested computations need to be factored out and
  bound as values by a let expression. This representation is known as
  A-normal form \cite{DBLP:conf/lfp/SabryF92}.
\end{itemize}

Definition of renaming and substitution are similar to those for
Œªst, so we do not include the updated versions here.

We define aliases for closed values \AS{Val‚ÇÄ} and closed terms
\AS{Trm‚ÇÄ} (typed in an empty context):

\ExecuteMetaData[LR/STLC.tex]{ground}

The \AS{‚áì} relation specifies big-step semantics for Œªst'.  Given a
term \AS{M} and a value \AS{V}, the inductive definition \AS{M ‚áì V}
states the conditions for \AS{M} to reduce to a value \AS{V}:

\ExecuteMetaData[LR/STLC.tex]{big-step}

The \AS{M ‚Üí‚ÇÅ M'} data type describes part of the small-step reducton
relation and has a single constructor which captures beta reduction
for functions. The \AS{‚áìstep} constructor is similar to the transitive
closure of the small-step reduction relation: if \AS{M} reduces to
\AS{M'} in a small step, and \AS{M'} reduces to \AS{V} in a big step,
then \AS{M} reduces to \AS{V} in a big step.

Differences between Œªcl and Œªcl' are analogous.

\section{Correctness by logical relations}
\label{sec:corr-logic-relat}

This section defines two relations between terms of Œªst' and Œªcl'. The
first is just a reformulation of the compatibility relation from
Chapter~\ref{cha:prov-corr-clos}, which, as the reader may remember,
subsumes the graph relation of any closure conversion. The other is a
\textit{logical relation}, which captures the notion that related
terms reduce to related values.

Subsequently, we formulate and prove a proposition which we call
\textit{a fundamental theorem for the logical relation}. The theorem
is a statement of correctness for any closure conversion function
whose graph relation is contained in the compatibility relation.

The proof is inspired by a sketch of an argument from
\cite{DBLP:conf/popl/MinamideMH96}.

\subsection{The compatibility relation}
\label{sec:lr-compat-rel}

We denote the compatibility relation with \AS{\ti}.  In general, given a
term \AS{M‚ÇÅ} in Œªst' and a term \AS{M‚ÇÇ} in Œªcl', \AS{M‚ÇÅ} and \AS{M‚ÇÇ}
are compatible (M‚ÇÅ \tis M‚ÇÇ) when their subterms are compatible. In the
special case of abstractions/closures, the closure body is renamed
with the environment in the premise of the rule.

\ExecuteMetaData[LR/LR.tex]{compat}

For brevity, we do not include translation functions from Œªst' to
Œªcl'. The reader should convince themselves that the minimising
closure conversion from Œªst and Œªcl could be ported to Œªst' and Œªcl',
and that its graph relation woud be contained in \AS{\ti}.

\subsection{The logical relation}
\label{sec:logical-relation}

While the compatibility relation captures syntactic correspondence, we
need another relation on (closed) language expressions which captures
operational correspondence. We define a family of logical relation
\AS{‚áî} relating closed source terms (reducible expressions) to closed
target terms (\AS{‚âÖ}) and closed source values to closed target
values (\AS{‚âà}).  The relations are defined by induction on types. In
the definition, we write \AS{œÑ ‚àã M‚ÇÅ ‚âÖ M‚ÇÇ} or \AS{œÑ ‚àã M‚ÇÅ ‚âà M‚ÇÇ} to mean
that \AS{M‚ÇÅ} and \AS{M‚ÇÇ} are related at type \AS{œÑ}:

\begin{tabular}{rccl}
  \AS{œÑ ‚àã} & \AS{M‚ÇÅ ‚âÖ M‚ÇÇ}   & iff  & \AS{M‚ÇÅ ‚áì V‚ÇÅ}, \AS{M‚ÇÇ ‚áì V‚ÇÇ}, and \AS{œÑ ‚àã V‚ÇÅ ‚âà
                                     V‚ÇÇ}  \\
  \AS{`ùîπ ‚àã} & \AS{`tt ‚âà `tt} \\
  \AS{`ùîπ ‚àã} & \AS{`ff ‚âà `ff} \\
  \AS{œÉ ‚áí œÑ ‚àã} & \AS{U‚ÇÅ ‚âà U‚ÇÇ} & iff & for all \AS{œÉ ‚àã V‚ÇÅ ‚âà V‚ÇÇ}, \AS{œÑ ‚àã U‚ÇÅ
                               `\$ V‚ÇÇ ‚âÖ U‚ÇÇ `\$ V‚ÇÇ }
\end{tabular}

In Agda, \AS{‚âÖ} and \AS{‚âà} are defined as specialisations of the
\AS{‚áî} relation on closed expressions of Œªst' and Œªcl'.

\ExecuteMetaData[LR/LR.tex]{related}

We define a pointwise version of the \AS{‚âà} relation which
relates source and target substitution environments, similar to what
we did in Section~\ref{sec:comp-valu-renam}:

\ExecuteMetaData[LR/LR.tex]{pointwise-related}

We also provide a function \AS{‚àô\tss{R}} which extends two related
substitution environments with a pair of related values:

\ExecuteMetaData[LR/LR.tex]{pointwise-ext}

Finally, we can state the fundamental theorem for our logical
relation:

\begin{lemma}
  \emph{Fundamental theorem of logical relations.} Given a source
  term \AS{M}, a target term \AS{M‚Ä†}, a source substitution
  \AS{œÅ\tss{s}}, and a target substitution \AS{œÅ\tss{t}}, if \AS{M} is
  compatible with \AS{M‚Ä†}, and for all variables \AS{x} in the
  context, the corresponding values in the substitution environments
  are in the logical relation (\AS{œÅ\tss{s}(x) ‚âà œÅ\tss{t}(x)}), then
  \AS{S.subst œÅ\tss{s} M} and \AS{T.subst œÅ\tss{t} M‚Ä†} are in the
  logical relation.
\end{lemma}

\ExecuteMetaData[LR/LR.tex]{fund-t}

Observe that the Fundamental theorem, instatiated from closed terms,
is equivalent to the Correctness property for the compatibility relation.

The Agda proof of the theorem is included in the technical appendix.

\chapter{Reflections and evaluation}
\label{cha:refl-eval}

This project is a case study on verification of transformations of
functional programs using two different techniques: bisimulations and
logical relations. The implemented transformation is closure
conversion. Both proofs of operational correctness are mechanised with
state-of-the-art techniques.

Recall that the \textbf{objectives} set forth and achieved in the
project were:

\begin{enumerate}
\item To implement a compiler transformation for a variant of
  simply-typed lambda calculus in Agda.
\item To use scope-safe and well-typed representation for the object
  languages.
\item To prove that the transformation is correct: that the output
  program of the transformation behaves ``the same'' as the input
  program.
\item To use generic programming techniques from ACMM.
\end{enumerate}

The two sections of this chapter evaluate the achievements of this
project and reflect on its relationship to other work and potential
future work.

\section{Evaluation of achievements}
\label{sec:eval-achi}

\paragraph{(Objective 1) Capturing the essence of closure conversion}
The implemented tranformation --- closure conversion --- requires a
different source and target language. While the formalisation of the
source language is largely borrowed from ACMM, and the formalisation
of the target language is similar except for the difference between
abstractions and closures, this project's contribution was to capture
the essence of closure conversion in what we believe is the simplest
and most elegant way possible.

\paragraph{Inherently typed closures}
A traditional representation of closure conversion replaces variables
in the source program with references to a record containing the
environment in the target program. This project's use of scope-safe
and well-typed terms allowed for a more elegant solution where the
closure body is typed in a context corresponding to the closure's
environment, and variables remain variables.

\paragraph{Closure environments as substitution environments}
Furthermore, while a closure environment is traditionally represented
as a record which stores environment values, this project captures the
essence of an environment by representing it as a substitution
environment, i.e. a mapping from variables to values.

\paragraph{Existential types for closure environments}
Closure environment should have existential types in order for a
program with closures to be well-typed. This observation was made by
\cite{DBLP:conf/popl/MinamideMH96}, which deals with this fact by
equipping the closure language with existential types. This project
uses a different, arguably simpler approach, whereby closure
environment are existentially typed \textit{in the meta language
  (Agda)}, which allows us to keep the types of the object language
simple.

\paragraph{(Objetive 2) Scope-safe and well-typed representation}
Both the source and target language have scope-safe and well-typed
representation, which is possible thanks to using dependently-typed
Agda as the meta language. Using inherently scoped and typed terms has
many benefits, which include the fact that when programs are
synonymous with their typing derivations, transformations on programs
are synonymous with proofs of type preservation. Additionally, many
techniques for reasoning about operational correctness are
type-directed, e.g. the type-indexed logical relations which we used,
and inherently typed representations are well-matched to such
techniques.

\paragraph{(Objetive 3) The closure conversion preserves operational correctness}
This project uses two standard techniques to show that the implemented
closure conversion is correct: bisimulation and logical relations. In
an informal setting of pen-and-paper proofs, both of those techniques
have rather straighforward proofs. However, mechanisation of those
proofs involves proving several lemmas about the interactions between
renaming, substitution, closure conversion, and the compatibility
relation.

\section{Reflections and comparison with related work}
\label{sec:refl-comp-with}

\paragraph{Comparison with traditional closure conversion}
In comparison with traditional closure conversion which represents
environments as records, this formulation, which represents closure
environments as substitution environments, i.e. meta-language
functions, is further removed from the eventual target, which is
machine code. But one can imagine a subsequent compilation phase which
replaces substitution environments with records, and variables with
record lookups (the object language would need existential types
then). In general, splitting the compilation process into many
specialised passes facilitates verification, as each compilation phase
is easier to verify, and composing correctness results about phases
gives rise to a end-to-end correctness result.

\paragraph{Mechanising the meta-theory of a language}
As observed in ACMM, mechanising the meta-theory of a language most
often requires proving lemmas about the interactions between different
transformations, or semantics, like renaming and substitutions. ACMM
singles out synchronisation lemmas, which relate two semantics
(e.g. for every renaming there exists a substitution which behaves the
same), and fusion lemmas (e.g. for every composition of two
substitutions, there exists a substitution which behaves the
same). 

\paragraph{(Objetive 4) ACMM}
ACMM exploit similarity between various traversals (semantics) in
simply typed lambda calculus (STLC) to provide a generic way to
prove synchronisation and fusion lemmas for STLC.

It should be noted that the objective of using generic proving from
ACMM was met partially. This project does borrow a type-andscope safe
representation from ACMM, but it does not duplicate ACMM's effort of
setting up the machinery for generic proofs of synchronisation and
fusion lemmas. That machinery depends on the concrete representation
of STLC, and since ours is slightly different, we just postulate the
synchronisation and fusion lemmas for STLC --- ACMM shows that their
their mechanical proofs exist and can be made generic.

Intermediate languages other than STLC require their own definitions
of renaming and substitution, and proofs of correctness lemmas. For
example, the proofs of operational correctness with bisimulations and
logical relations depend on four fusion lemmas relating renaming and
substitution for the language with closures. Since ACMM did not show
anything about a language with closures, we proved the necessary
lemmas manually. In fact, mechanising those lemmas constituted the
biggest effort in the whole proof.

\paragraph{Possible remedy: AACMM and generic programming}
The problem of having to define renaming and substitution for each new
language, and proving correctness lemmas about the interactions
between renaming, substitution, and transformations, is address by
AACMM \cite{DBLP:journals/pacmpl/AllaisA0MM18}, whose contributions
were described in Section~\ref{sec:gener-trav-proofs}.

\paragraph{Feasibility of closure conversion in AACMM}
AACMM demonstrates that transformations like let-inlining and CPS
conversion can be expressed in their generic framework. They pose an
open question about which compilation passes can be implemented
generically. Unfortunately, this work suggests that closure conversion
might not fit well into the AACMM framework. Specifically, the closure
language in this project --- with features like syntax being mutually
dependent on substitution environments, or environments being
existentially quatified in the meta language (Agda) --- is not
expressible as an AACMM generic syntax. The traditional representation
of a languages with closures --- with environments as records and
existential types in the object language --- would not fit either as
syntaxes in AACMM do not support existential types.

\paragraph{Summary}
This work and ACMM/AACMM are both concerned with mechanising the
meta-theory of languages, and applying this metatheory to reason about
the language. While AACMM shows that a certain class of languages can
be treated generically, this work contributes a negative result which
indicates that a language with closures might not benefit from AACMM's
techniques for relieving the burden of mechanising meta-theory. It
is an open question, however, whether there exist feasible generic
syntaxes which would encompass a language with closures, or whether an
alternative formalisation of a language with closures exists which is
compatible with AACMM.

\chapter{Relationship to the UG4 project}
\label{cha:relat-ug4-proj}

The UG4 explored one particular kind of program transformations, which
we refer to as program derivation. In program derivation, a
tranformation is specified by a source program (or specification), and
a recipe for obtaining a result program. The UG4 project assumed a
particular style of derivations known as Bird-Meertens Formalism
\cite{gibbons1994introduction}. In BMF, derivations are based on
equational reasoning, and consist of a sequence of intermediate forms
of the program, where the steps are annotated with rules justifying
each step. This is illustrated by the template below:

\begin{verbatim}
specification
  = { justification }
intermediate form 1
  = { justification }
...
  = { justification }
intermediate form n
  = { justification }
implementation
\end{verbatim}

This year's work explores the other ends of the spectrum of
transformations on programs, which encompasses compilation
phases.

This chapter attempts to find common characteristics between
compilation phases and program derivations, but also highlight their
differences. It also looks at the middle of the spectrum, where
certain transformations on programs could be classified either way. It
ends with a reflections on lessons learned from this year's project
which would have been helpful for last year's work.

In literature, what we refer to as program derivation is sometimes
called program construction or calculation, and a program derivation
described an instance of the process. Here, we use the term
``derivation'' to describe a family of transformations on programs, in
order to avoid confusion with program transformations within compilers.

\section{Compilation phases and program derivations}
\label{sec:comp-phas-progr}

Compilation phases and derivations can be described and compared in
terms of several criteria: (a) the objective of the transformation,
(b) what the source and the target of the transformation is, (b)
required expertise from the user, (c) user's expertise and input
needed to guide the transformation, and (d) whether rules apply in all or
only selected possible places.

The following characterises compilation phases:

\begin{enumerate}
\item (\textbf{Objective} and \textbf{Source and target of
    transformation}) The objective is to generate low-level code from
  a program in the source language.
\item (\textbf{Required expertise}) Programmer only needs to know the
  source language
\item (\textbf{Input from programmer and required expertise}) Little or none, except for
  specialised annotations which are used by the compiler
\item (\textbf{Totality and selectivity}) If a compilation phases can
  be specified as a rule, then the rule is typically applied in all
  the places where its premises match
\item (\textbf{Examples}) Closure conversion, CPS transformation,
  lambda lifting, type-checking, constant expression folding, code
  generation, dead code elimination, inlining.
\end{enumerate}

The following are features of program derivation:

\begin{enumerate}
\item (\textbf{Objective}) Enable the programmer to write clear, concise,
  understandable programs which serve as specification. Methodically
  derive a correct, efficient implementation. Possibly mechanise the
  tranformation described by the derivation.
\item (\textbf{Source and target of transformation}) The source could be an
  executable functional program or a non-executable specification
  (e.g. in the categorical calculus of relations; or as a solution to
  an equation). The target is an efficient functional or imperative
  program. 
\item (\textbf{Input from programmer and required expertise}) A derivation is a
  description of a transformation which can be calculated
  mechanically, so by definition, non-trivial derivations require
  input fromt the programmer. This might require expertise beyond the
  capabilities of an average programmer.
\item (\textbf{Totality and selectivity}) Rules are applied selectively: in arbitrary places and order.
\item (\textbf{Example realisation}) Bird-Meertens formalism: equational
  reasoning, where rules justify correctness of each step \cite{gibbons1994introduction}.
\item (\textbf{Obstacles to adoption}) Few tools, hard to learn, hard to use,
  hard to understand, hard to maintain, writing the implementation by
  hand can be easier than writing the derivation.
\end{enumerate}

Benefits of employing a sort of program derivation (more or less
formal) for the programmer include: (a) a structured process of
obtaining an implementation from specification, (b) greater confidence
in the correctness of the implementation, (c) possibility of
discovering further optimisations, and finally (d) a framework for a
proof of correctness. This last use case could be explained as
follows: suppose we can prove correctness of the "specification"
program, and that each step preserves the meaning of the program. Then
we can show correctness of the "implementation" program.

\section{Rewrite rules}
\label{sec:rewrite-rules}

When equational reasoning is employed, derivation steps can be
justified with \textit{rewrite rules}. A basic example of a rewrite
rule in functional programming is \textit{map-comp}:

\AS{‚àÄ \{f g xs\} ‚Üí map f (map g xs) ‚â° map (f ‚àò g) xs}

where $f$, $g$, $xs$ are metavariables. An application of a rewrite
rule consists of unifying the LHS of the rule with a subterm of the
program (and thus obtaining a substitution œÉ), and then replacing the
subterm with the RHS of the rule, instantiated with the substitution
œÉ.

\section{Program derivations in compilers and their limitations}
\label{sec:progr-deriv-comp}

There are program transformations in existing compilers which, other
than being compilation phases, have features of program derivation.  A
good example of this is the support for rewrite rules in GHC, a
Haskell compiler. A Haskell programmer may specify a rule like
\textit{map-comp} as part of the code, and in one of early compilation
passes, GHC will apply the rule wherever possible (i.e. replace the
occurrences of the LHS with the RHS). Rewriting in GHC is a
compilation phase in the sense that rules are applied in all places
they match, but it also resembles derivations since the transformation
is guided by input from the programmer, namely, the specified rewrite
rules. Note that GHC makes no attempt to ensure that the rules
preserves the meaning, or that rewriting would terminate: a programmer
could externally check these properties, e.g. using a proof
assistant.

Yet another ambiguous situation is where program transformation
becomes a search problem. A compiler could try to find a
transformation by applying rewrite rules in a selective,
non-deterministic manner, and thus perform a search over the space of
possible derivations. The search could be guided by an objective
function, for example, a sort of static analysis of the running time,
or the compiler could evaluate programs by running them on sample
inputs. Exploring the space of derivations is the approach taken by
the Lift compiler \cite{DBLP:conf/cgo/SteuwerRD17}.

Programmer's input may or may not be involved in such search. For
example, the UG4 project delivered a graphical user interface which
allows the user to interact with the derivation process.

Unfortunately, derivation search needs a fixed set of rewrite rules,
either hardcoded in the compiler or provided by the user. This
precudes derivations which use original rewrite rules, coming up with
which would require human insight. Section~\ref{sec:progr-deriv-agda},
which discusses some of the rewrite steps from the UG4 project, has
examples of rules which were invented for a specific derivation, and
it is difficult to conceive that they could all be provided to the
compiler in advance.

\section{Program derivations in the UG4 project}
\label{sec:progr-deriv-ug4}

The UG4 project analyses two instances of program derivation in
detail. The first one is a derivation, described in
\cite{gibbons1994introduction}, of an efficient implementation of the
maximum segment sum problem (MSS). While the input specification
(which is also a runnable program) runs in cubic time in the length of
the input list, the output program runs in linear time. The asymptotic
speed-up is achieved by applying several rewrite rules involving
higher-order functions on lists such as map, foldr, and filter.

The second case study involved an original derivation of a program for
matrix-vector multiplication. The input program takes a dense matrix,
and the output program takes a sparse matrix in the compressed sparse
row (CSR) format. Or, to be precise, the input program which acts on a
dense matrix, is transformed into a composition of two programs: (a) a
conversion from a dense to a CSR-sparse matrix, and (b) a
matrix-vector multiplication program which acts on a CSR-sparse
matrix. This is because, as a rule, the input and output types of the
program must stay the same in the course of the derivation. This
second derivation was similarly accomplished with rewrite rules
involving higher-order functions.

\section{Implementation of rewriting in the UG4 project}
\label{sec:impl-rewr-ug4}

The UG4 project included a purpose-built framework for specifying
derivations. The framework included:

\begin{enumerate}
\item A simple functional language with parametric polymorphism. The
  language is point-free, that is, based on function composition
  rather than variable binders. This is because variables and
  abstraction are difficult to implement correctly, as demonstrated by
  this UG5 project, and even more difficult to rewrite.

\item A type-checker for the language.

\item Rewriting functionality and declaring derivations as sequences of
  rewrites.

\item An interpreter for the language, which was used to empirically
  verify claims about performance gains from derivations.
\end{enumerate}

Writing the framework was a good exercise in implementing routine
parts of compiler front-ends, such as type checking and
unification. Writing it in Scala made sense given the stretch
objective of compiling the language to Lift, which was not realised,
however.

Rewrite rules were stated without justification, much as postulates in
Agda. One could prove the rules externally ‚Äì but then one is pressed
to ask, why not express a derivation in a proof assistant, which
supports unification and rewriting natively? Indeed, with hindsight,
we can say with certainty that a proof assistant is perfectly suited
for the job, its only downside being that it requires considerable
expertise.  The next section presents an Agda proof of a single rewrite rule
from the UG4 project, and evaluates the benefits of doing so,
compared with the approach from last year's project.

\section{Proving rewrite rules in Agda}
\label{sec:progr-deriv-agda}

Unlike in the last year's project, we can now provide proofs of
individual rewrite rules. Here we include an example proof of the fact
that filtering out zeros from a list does not change the result of
taking the sum of the elements in the list. This is intuitively clear
as zero is a neutral element for addition, but showing this formally
requires an inductive proof:

\ExecuteMetaData[UG4.tex]{sum-filter}

\chapter{Conclusion}
\label{cha:conclusion}

The main deliverables of this project are (a) an elegant
representation of a language with closures, (b) a type-preserving
closure conversion algorithm which minimises closure environments, and
(c) mechanisations of proofs of correctness of closure conversion
using two different techniques: bisimulations and logical relations.

This work builds on a long line of research in several areas:
representating languages with bindings, type-preserving
compilation, and compiler verification. In particular, the style of
the Agda development is influenced by ACMM and PLFA.

By mechanising two different proofs of correctness of the
transfomation, the project provides a reference for comparing the
methods of bisimulation and logical relations.

It was confirmed that when the languages have a type-and-scope safe
representation, mechanisation of meta-theoretical proofs requires a
large amount of effort to establish correctness lemmas about
interactions between renaming, substitution, and other traversals.

A natural continuation for this project would be to try to apply
generic proving techniques, e.g. from AACMM, to prove meta-theoretical
lemmas for a family of intermediate languages (syntaxes) all at
once. However, this project's insights seem to imply that closure
conversion does not fit into the framework provided by AACMM, and that
further developments in generic proving would be needed to support
closure conversion.

Another possible extension of this work would be to prove correctness
properties of more complex languages with features like higher-order
functions, polymorphism, abstract data types, recursive types, mutable
state and control effects. In that case, however, use of a proof
language with tactics and automation would be more appropriate, as
scaling manual proof techniques to complex languages is known to be
extremely tedious.

In summary, the objectives of the project were met, if slightly
altered. Just one compilation phase was implemented, but it was proved
correct with two different methods. The generic proving techniques
from ACMM, although not ported to our representation of simply typed
lambda calculus, provided a basis for postulating correctness lemmas
about STLC. Using generic programming solutions from AACMM was beyond
the scope of this project, but like it was mentioned multiple times,
they would be inadequate for a languauge with closures anyway.


% use the following and \cite{} as above if you use BibTeX
% otherwise generate bibtem entries
\bibliographystyle{plain}
\bibliography{main}

\chapter{Technical appendix}
\label{cha:technical-appendix}

\section{Minimising closure conversion and the compatibility relation}
\label{sec:minim-clos-conv-2}

Below is the Agda proof that the graph relation of the minimising
closure conversion function is contained in the compatibility relation.

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{min-cc-sim}

\section{Compatibility relation is a bisimulation}
\label{sec:comp-relat-bisim-1}

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{sims}

\section{Proof of the fundamental theorem for the logical relation}
\label{sec:proof-fund-theor}

Notice that the proof in incomplete in two cases.

\ExecuteMetaData[LR/LR.tex]{fund-imp}

\end{document}
