\documentclass[bsc,frontabs,twoside,singlespacing,parskip,deptreport]{infthesis}

\batchmode
\usepackage{agda}
\usepackage{catchfilebetweentags}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{url}
\usepackage[hidelinks]{hyperref}

\input{commands}

%\usepackage{agda}

% The following packages are needed because unicode
% is translated (using the next set of packages) to
% latex commands. You may need more packages if you
% use more unicode characters:

% \usepackage{textgreek}
% \usepackage{amssymb}
% \usepackage{bbm}
% \usepackage[greek,english]{babel}
% \usepackage{amsthm}
% \usepackage{catchfilebetweentags}
% \usepackage{mathrsfs}

% This handles the translation of unicode to latex:

% \usepackage{ucs}
% \usepackage[utf8x]{inputenc}
% \usepackage{autofe}

% Some characters that are not automatically defined
% (you figure out by the latex compilation errors you get),
% and you need to define:

% \DeclareUnicodeCharacter{8988}{\ensuremath{\ulcorner}}
% \DeclareUnicodeCharacter{8989}{\ensuremath{\urcorner}}
% \DeclareUnicodeCharacter{8803}{\ensuremath{\overline{\equiv}}}
% \DeclareUnicodeCharacter{0411}{\ensuremath{\lambda}}
% \DeclareUnicodeCharacter{8718}{\ensuremath{\square}}
% \DeclareUnicodeCharacter{8759}{::}
% \DeclareUnicodeCharacter{10218}{\ensuremath{\langle\langle}}
% \DeclareUnicodeCharacter{10219}{\ensuremath{\rangle\rangle}}
% \DeclareUnicodeCharacter{120038}{\ensuremath{\mathcal{W}}}
% \DeclareUnicodeCharacter{7473}{\textsuperscript{E}}

% Add more as you need them (shouldn't happen often). 

\theoremstyle{definition}
\newtheorem*{definition}{Definition}

\begin{document}

\title{Type-preserving closure conversion of PCF in Agda (more to come)}

\author{Piotr Jander}

\course{Master of Informatics}
\project{{\bf MInf Project (Part 2) Report}}

\date{\today}

\abstract{
This is an example of {\tt infthesis} style.
The file {\tt skeleton.tex} generates this document and can be
used to get a ``skeleton'' for your thesis.
The abstract should summarise your report and fit in the space on the
first page.
%
You may, of course, use any other software to write your report,
as long as you follow the same style. That means: producing a title
page as given here, and including a table of contents and bibliography.
}

\maketitle

\section*{Acknowledgements}
Acknowledgements go here.

\tableofcontents

%\pagenumbering{arabic}


\chapter{Introduction}

The document structure should include:
\begin{itemize}
\item
The title page  in the format used above.
\item
An optional acknowledgements page.
\item
The table of contents.
\item
The report text divided into chapters as appropriate.
\item
The bibliography.
\end{itemize}

Commands for generating the title page appear in the skeleton file and
are self explanatory.
The file also includes commands to choose your report type (project
report, thesis or dissertation) and degree.
These will be placed in the appropriate place in the title page.

The default behaviour of the documentclass is to produce documents typeset in
12 point.  Regardless of the formatting system you use,
it is recommended that you submit your thesis printed (or copied)
double sided.

The report should be printed single-spaced.
It should be 30 to 60 pages long, and preferably no shorter than 20 pages.
Appendices are in addition to this and you should place detail
here which may be too much or not strictly necessary when reading the relevant section.

\section{Using Sections}

Divide your chapters into sub-parts as appropriate.

\section{Citations}

Note that citations
(like \cite{P1} or \cite{P2})
can be generated using {\tt BibTeX} or by using the
{\tt thebibliography} environment. This makes sure that the
table of contents includes an entry for the bibliography.
Of course you may use any other method as well.

\section{Options}

There are various documentclass options, see the documentation.  Here we are
using an option ({\tt bsc} or {\tt minf}) to choose the degree type, plus:
\begin{itemize}
\item {\tt frontabs} (recommended) to put the abstract on the front page;
\item {\tt twoside} (recommended) to format for two-sided printing, with
  each chapter starting on a right-hand page;
\item {\tt singlespacing} (required) for single-spaced formating; and
\item {\tt parskip} (a matter of taste) which alters the paragraph formatting so that
paragraphs are separated by a vertical space, and there is no
indentation at the start of each paragraph.
\end{itemize}






\chapter{Related literature}
\section{Closure conversion}

Closure conversion is a compilation phase where functions or lambda
abstractions with free variables are transformed to /closures/. A
closure consists of a body (code) and the /environment/, which is a
record holding the values corresponding to the free variables in the
body (code). Closure conversion transforms abstractions to closures,
and replaces references to variables with lookups in the environment.

Closure conversion was necessarily used in every compiler for a
language which supports functions with free variables (TODO wording:
scope?). But the first work which provided a rigorous treatment of
closure conversion was the paper "Typed Closure Conversion" by
Minamide et al. \cite{TODO}. It demonstrated type-preserving closure
conversion, where closure environments have existential types (TODO
wording). On top of a proof of type-safety, the paper contains a proof
of operational correctness of the typed closure conversion algorithm
by logical relations.

Another notable paper about closure conversion is "Typed Closure
Conversion Preserves Observational Equivalence" by Ahmed and Blum
\cite{TODO}. The paper's title explains its main result, so we should
explain the title.

(TODO bring up the Reynolds' paper) Within a language L, we have a
program P = C[A], where A is an implementation of an abstraction and C
is the "context", or "the rest of the program". Given some other
implementation A' of the abstraction, we say that A and A' are
contextually equivalent when for all possible contexts C, programs P =
C[A] and P' = C[A'] behave identically.

We say that another abstraction A' is contextually equivalent to A if
for all contexts C, programs C[A] and C[A'] are equivalent. This
corresponds to a programmer's intuition that A and A' behave in the
same way in all possible programs.

TODO OE matters for security and safety: If an attack would be
possible by exposing a certain implementation detail, then this detail
is made inaccessible / private, for example by using an existential
type.

Why this matters: modern software systems are made up of multiple
components, of which some might not be trusted.

// To ensure reliable and secure operation, it is important to defend
against faulty or malicious code. Language-based security is built
upon the concept of abstraction: if access to some private
implementation detail might enable an attack, then this detail is made
inaccessible by hiding it behind an abstract interface, for example
using an existential type. //

TODO I have quite a bit about the paper and we don't want to duplicate
the paper's introduction: how do I make it shorter?

\section{Verified compilation}

Closure conversion is just one possible verification phase, and its
verification constitutes part of a wider effort to verify compilation
end-to-end, which usually entails verifying type preservation or
operational correctness of all compilation phases.

As far as type safety is concerned, the reference is a paper by
Morrisett et al., "From System F to Typed Assembly Language"
\cite{TODO}. It builds upon previous results in type safety of
compilation phases (like the aforementioned \cite{TCC}) and describes
a typed RISC-like assembly (named TAL), which is the target of the
final phases of compilation. As a whole, the paper proves type safety
for a compilation pipeline from System F to TAL. It does not, however,
prove end-to-end operational correctness.

The first compiler which was verified for end-to-end operational
correctness was described by Adam Chlipala in his paper "A Certified
Type-Preserving Compiler from Lambda Calculus to Assembly
Language". The source is a variant of the simply-typed lambda calculus
(STLC). Compilation proceeds through six phases, eventually yielding
idealised assembly code. The compiler is implemented in Coq, where
terms and functions on terms are dependently typed, guaranteeing type
preservation. This is also the approach taken in this project, except
that we use Agda instead of Coq \cite{TODO}. Operational
correctness is proved by adopting denotational semantics, unlike in
this project, which uses operational semantics. Due to unfamiliarity
with operational semantics, we cannot comment on which approach is
better (TODO or can we?).

A final example of a certified compiler is CompCert \cite{TODOcompcert}, which is the result of the first successful attempt to
implement a certified compiler of a real-world (TODO wording)
language. Even compared with the simply-typed lambda calculus (STLC),
which was the source language in Chlipala's work \cite{TODO}, the C
language is in some ways simpler, especially since it does not have
first-class functions with free variables (TODO wording:
scoping?). But, being a fully-fledged language, C presents enough
challenges as the source language of a verified compiler.

\chapter{Background}

This chapter will introduce the relevant concepts. It will start with
closure conversion, then discuss compilation phases and intermediate
languages, and finally explain the Agda definitions and encodings
which were borrowed from ACMM and PLFA.

\section{Closure conversion}

TODO explain and give an example

TODO explain why existential types

\section{Compilation phases and intermediate representations}

In all but the most trivial compilers, compilation proceeds in
phases, or transformations. A compilation phase transforms the
compilation unit to bring it one step closer from the source code to
the target representation.

[diagram here]

\paragraph{Intermediate representations} As illustrated in the figure,
each compilation phase takes a source representation to a target
representation [relate to diagram]. An intermediate representation can
also be called an intermediate language, and abbreviated to IR or
IL. For some phases, the source and target representation may be the
same. Arguably, this is the case for constant expression folding.

However, other phases benefit from using different source and target
representations. An example of such transformation is closure
conversion, which as the reader may recall from [section], transforms
abstractions with free variables to so called closures, which take an
explit environement and can only reference values from that
environment.

\paragraph{Typed and untyped IRs} To question of whether closure
conversion must necessarily use different source and target languages
hinges on the distinction between typed and untyped intermediate
language. Using a typed IR requires that at each point along the
compilation pipeline, intermediate representantions are well-typed.

Suppose that closure conversion is performed on simply typed lambda
calculus (STLC) (without existential types). Then the target representation
cannot be the same (STLC), as STLC does not have existential types,
and closure environments must be existentially typed in order for
programs to be well-typed in general, as discussed in [section]. This
is why an intermediate language with existential types is necessary.

On the other hand, if the source and target representations are
untyped, then the compiler architect might get away with using the
same intermediate language as both source and target (for example
Scheme, which is sometimes used as a compilation target). But even
in this case, compilation process might benefit if the abstract syntax
has explicit closures.

\paragraph{IRs in this project} This project uses a dependently typed
meta language (Agda) to implement compilation phases (specifically,
closure conversion), so typed intermediate representations are a
natural choice. Therefore, in the following sections, we will describe
two intermediate representations, which are both variants of lambda
calculus. The source representation will be simply typed lambda
calculus, which we will refer to as STLC or λst. The target
representation will be simply typed lambda calculus with closures,
denoted with λcl.

The two intermediate representations are similar, and differ mainly in
having either abstractions with free variables in λst or closures with
environments in λcl. Unfortunately, this means that formalisations of
λst and λcl share a lot of duplication. This is a common problem in
formalising languages which has recently been addressed by
\cite{DBLP:journals/pacmpl/AllaisA0MM18}. Whether techniques from
Allais et al. are applicable to this work will be discussed in [related
work]. On the other hand, [section] demonstrates that while two
intermediate languages can only differ in a handful of syntactic
constructs and reduction steps, they can behave very differently with
respect to the ubiquitious operations of renaming and substitution.

\section{Type- and scope-safe representation of simply typed lambda
  calculus λst}

This section will discuss the encoding of simply typed lambda calculus
(abbreviated as STLC, denoted with λst), which is the source language
of closure conversion. Typing and reduction rules are standard for
call-by-value lambda calculus, so it is the encoding in Agda which is
of interest in this section. As similar encoding is used for the
closure language λcl.

Using dependently typed Agda as the meta language allows us to encode
certain invariants in the representation. Two such invariants are
scope and type safety. The representation is scope-safe in the sense
that all variables in a term are either bound by some binder in the
term, or explicitly accounted for in the context. It is type-safe in
the sense that terms are synonymous with their typing derivations,
which makes ill-typed terms unrepresentable. This kind of scope and
type safety is due to \cite{DBLP:conf/csl/AltenkirchR99}. The rest of
this section shows how this is achieved in Agda; the Agda encoding is
based on the one used in \cite{DBLP:conf/cpp/Allais0MM17},
\cite{DBLP:journals/pacmpl/AllaisA0MM18}, and
\cite{DBLP:conf/sbmf/Wadler18}.

TODO STLC as a figure here

To start with, λst typed are defined as follows.

\ExecuteMetaData[StateOfTheArt/Types.tex]{type}

The context is simply a list of types.

\ExecuteMetaData[StateOfTheArt/Types.tex]{context}

Variables are synonymous with proofs of context membership. Since a
variable is identified by its position in the context, it is
appropriate to call it a de~Bruijn variable. Accordingly, the
constructors of \AS{Var} are named after \textit{zero} and
\textit{successor}. Notice that the definition assumes that the
leftmost type in the context corresponds to the most recently bound
variable.

\ExecuteMetaData[StateOfTheArt/Types.tex]{var}

We can now present the formulation of λst terms, which is synonymous
with their typing derivations:

\ExecuteMetaData[StateOfTheArt/STLC.tex]{terms}

The syntactic variable \AS{V} constructor takes a de~Bruijn variable to
a term. The abstraction constructor \AS{L} requires that the body is
well-typed in the context \AS{Γ} extended with the type \AS{σ} of the
variable bound by the abstraction. The application constructor
\AS{A} follows the typing rule for application.

\section{Type- and scope-safe programs}

Many useful traversals of the abstract syntax tree involve maintaining
a mapping from free variables to appropriate values. Two such
traversals are simultaneous renaming and substitution.

Simultaneous renaming takes a term \AS{N} in the context \AS{Γ}. It maintains
a mapping \AS{ρ} from variables in the original context \AS{Γ} to
\textit{variables} in some other context \AS{Δ}. It produces a term in
\AS{Δ}, which is \AS{N} with variables renamed with \AS{ρ}.

Similarly, simultaneous substitution takes a term \AS{N} in the context
\AS{Γ}. It maintains a mapping \AS{σ} from variables in the original context
\AS{Γ} to \textit{terms} in some other context \AS{Δ}. It produces a
term in \AS{Δ}, which is \AS{N} with variables substitution for with \AS{σ}.

Before we can demonstrate an implementation of renaming and
substitution, we need to formalise the notion of a mapping from free
variables to appropriate values, which we call the
\textit{environment}.

\ExecuteMetaData[STLC.tex]{env}

A environment \AS{(Γ ─Env) 𝓥 Δ} encapsulates a mapping from variables in
\AS{Γ} to values \AS{𝓥} (variables for renaming, terms for
substitution) which are well-typed and -scoped in \AS{Δ}.

An environment which maps variables to variables is important enough
to deserve its own name.

\ExecuteMetaData[STLC.tex]{thinning}

There is a notion of an empty environment \AS{ε}, of extending an
environment \AS{ρ} with a value \AS{v}: \AS{ρ ∙ v}, and of mapping a
function \AS{f} over an environment \AS{ρ}: \AS{f <\$> ρ},
corresponding to the analogous operations on contexts (which are just
lists).

\ExecuteMetaData[STLC.tex]{envops}

Notice that those three operations on environments are defined using
copatterns \cite{DBLP:conf/popl/AbelPTS13} by `observing` the
behaviour of \AS{lookup}.

Equipped with the notion of environments, we can give an
implementation of renaming and substitution:

\ExecuteMetaData[StateOfTheArt/STLC.tex]{rename}
\ExecuteMetaData[StateOfTheArt/STLC.tex]{subst}

Notice that those two traversals are indentical except (1)
\textit{renaming} wraps the result of \AS{lookup ρ x} in \AS{V}, and
\textit{renaming} and \textit{substitution} extend the environment in
a different way: \AS{s <\$> ρ ∙ z} vs \AS{rename (pack s) <\$> σ ∙ V
  z}. The observation that renaming and substitution for STLC share a
common structure was a basis was the unpublished manuscript by McBride
\cite{mcbride2005type}, and subsequently motivated the ACMM paper
\cite{DBLP:conf/cpp/Allais0MM17}. In [section], we will show how ACMM
abstracts this common structure of renaming and substitution into a
notion of a semantics.

To see the usefulness of simultaneous renaming and substitution,
consider that once an identity substitution is defined (one which
leaves its argument unchanged):

\ExecuteMetaData[StateOfTheArt/STLC.tex]{id-subst}

Then defining a single substitution is simple. (A single substitution
replaces occurrences of the last-bound variable in the context, and it
is useful for defining the beta reduction for abstractions).

\ExecuteMetaData[StateOfTheArt/STLC.tex]{single-subst}

\section{ACMM's notion of a semantics}

TODO ACMM, synch, fusions

\section{Small-step operational semantics}

The formalisation of small step semantics for a call-by-value lambda
calculus is adapted from \cite{DBLP:conf/sbmf/Wadler18}.

Values are terms which do not reduce further. In this most basic
version of lambda calculus language, the only values are abstractions:

\ExecuteMetaData[StateOfTheArt/STLC.tex]{values}

Our operational semantics include two kinds of reduction rules. Compatibility rules, whose
names start with \AS{ξ}, reduce parts of the term (specifically, the LHS
and RHS of application). Beta reduction \AS{β-L}, on the other hand,
describes what an abstraction applied to a value reduces to.

\ExecuteMetaData[StateOfTheArt/STLC.tex]{reductions}

A term which can take a reduction step is called a reducible
expression, or a redex. A property of a language that every well-typed
term is either a value or a redux is called type-safety. This property
is captured by a slogan `well-typed terms don't get stuck` and can be
proved by techniques like `progress and preservation` or logical
relations. Simply typed lambda calculus is type-safe, and so is this
formalisation. For a proof of type safety for a similar formalisation
of STLC, cf. \cite{DBLP:conf/sbmf/Wadler18}.

Operational semantics are needed for the treatment of bisimulation.

\chapter{The Agda development}

This chapter presents the parts of the Agda development which are
original to this project. It starts by discussing the closure language
λcl, an intermediate language which is like STLC but with abstractions
replaced by closures. Then it demonstrates a type-preserving
conversion for λst to λcl which has the property that the obtained
closure environments are `minimal`.  Finally, a result is presented
about the source and target programs of closure conversion being in a
bisimulation.

\section{Closure language λcl}
\label{sec:closure-language-cl}

As discussed in the Background [or maybe Intro?] chapter, some
compilation phases must use different source and target intermediate
representations. This is the case with closure conversion, and this
section presents a formalisation of an intermediate language with
closures. The language is very similar the formalised simply typed
lambda calculus, except that abstraction with free variables are
replaced by closures with environments. What might seem like a simple
change has interesting implications for traversals like renaming and
substitution.

The closure language λcl shares types, contexts, and
de-Bruijn-variables-as-proofs-of-context-membership, and their
respective Agda formalisations, with the source representation. In
general, two different intermediate representations do not need to
share the same type system, but if they do, this simplifies
formalisation. The descriptions of those formalisations can be found
in Section~[TODO].

\subsection{Terms}
\label{sec:closure-language-cl-1}

The definition of terms of λcl differs from terms of λst in the \AS{L}
constructor, which, in λcl, holds the closure body and the closure
environment.

\ExecuteMetaData[StateOfTheArt/Closure.tex]{terms}

Notice that the typing rule for the closure constructor \AS{L}
mentions two contexts, \AS{Γ} and \AS{Δ}. We call \AS{Γ} the
\textit{outer context} and \AS{Δ} the \textit{inner context} of a
closure.

\begin{minipage}{.5\textwidth}
  \[
  \frac
  {\Gamma , x : \sigma \vdash e : \tau}
  {\Gamma \vdash \lambda x : \sigma . e : \sigma \rightarrow \tau}
  \text{T-abs}
  \]
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \[
  \frac
  {e_{ev} = subst ( \Delta \subseteq \Gamma ) \quad \quad \Delta , x : \sigma \vdash e : \tau}
  {\Gamma \vdash \langle\langle \lambda x : \sigma . e \; , \; e_{ev} \rangle\rangle : \sigma \rightarrow \tau}
  \text{T-clos}
  \]
\end{minipage}

The closure as a whole is typed in \AS{Γ}, but the closure body (also
called the \textit{closure code}) is typed in \AS{σ ∷ Δ}. The
relationship between \AS{Γ} and \AS{Δ} is given by the closure
environment.

A closure environment is traditionally implemented as a record, and
variables in the closure code reference fields of that record. In this
development, on the other hand, the environment is represented as a
substitution environment, that is, a mapping from variables in \AS{Δ}
to terms in \AS{Γ}. This representation is isomorphic to the one using
a record, and it has several benefits, especially eliminating the need
for products in the language, and overall simplification of the
formalisation.

Finally, recall from [section] that in order for a closure-converted
program to be well-typed, a closure environment should have an
existential type. It is important to note that in this formalisation,
existential typing is achieved in the meta language Agda, not in the
object language λcl, which does not have existential types. Indeed,
existential quantification (including over types) can in achieved in
Agda through dependent products, a datatype constructor is a dependent
product, and the environment is a parameter to the \AS{L} constructor.

\subsection{Renaming and substitution}
\label{sec:renam-subst}

Consider the case for the constructor \AS{L} of renaming and
substitution in λcl and how it is different from the corresponding
definition in λst.

\ExecuteMetaData[StateOfTheArt/Closure.tex]{rename}
\ExecuteMetaData[StateOfTheArt/Closure.tex]{subst}

Unlike in λst, renaming and substitution in λcl \textit{do not go
  under binders} (do not change the closure body). This is because
renaming and substitution take a term in a context \AS{Γ} to a term in
a context \AS{Γ'}. But the code (body) of a closure is typed in a
different context \AS{Δ}. So upon recursing on a closure, renaming and
substitution adjust the closure environment and leave the closure body
unchanged. The adjustment to the environment is \AS{rename ρ <\$> E}
in the case of renaming and \AS{subst ρ <\$> E} in the case of
substitution. In either case, the adjustment consists of mapping the
renaming/substitution over the values in the environment.

The fact that in λcl, renaming and substitution do not go under
binders will allow us to prove `fusion lemmas` in [section] without
using the machinery of ACMM, which will significantly simplify the
proofs.

\subsection{Operational semantics}
\label{sec:oper-semant}

Operational semantics are similar to the semantics for λst, except for
adjustments for closures. Values in λcl are closures, and the rule for
beta reduction is different:

\ExecuteMetaData[StateOfTheArt/Closure.tex]{subst}

Recall that a closure is a function without free variables,
partially applied to an environment. When the closure argument reduces
to a value, the argument and the values in the environment get
simultaneously substituted into the closure body. The simplicity of
this reduction rule is another benefit of representing environments as
substitution environments.

\subsection{Conversion from λst to λcl}
\label{sec:conversion-from-st}

This project's approach to typed, or type-preserving, closure
conversion follows \cite{DBLP:conf/popl/MinamideMH96}. An important
point here is that the specification of typed closure conversion
allows for different implementations which might differ in their
treatment of environments. The only requirement in the specification
is that

\begin{enumerate}
\item If the source term is an abstraction typed in the context
  \AS{Γ};
\item if the body of the source abstraction can be typed in a smaller
  context \AS{Δ}, such that \AS{Δ ⊆ Γ};
\item then the target terms is a closure whose environment is a
  substitution from \AS{Δ} to \AS{Γ}.
\end{enumerate}

This is given by the following conversion rule:

\[
  \frac
  {e_{ev} = subst (\Delta \subseteq \Gamma) \quad \quad \Delta , x : \sigma \vdash e \leadsto e' : \tau }
  {\Gamma \vdash \lambda x : \sigma . e \leadsto
    \langle\langle \lambda x : \sigma . e' \; , \; e_{ev} \rangle\rangle : \sigma \rightarrow \tau}
\]

It is up to the implementation of closure conversion to decide how big
to make \AS{Δ}, on the spectrum between (1) \AS{Δ} being equal to
\AS{Γ}, and (2) \AS{Δ} being `minimal`, i.e. only containing the parts
of \AS{Γ} which are necessary to type the term. We present two Agda
implementation of closure conversion, corresponding to the two ends of
the spectrum.

Closure conversion where \AS{Δ} is the same as \AS{Γ} is a simple
transformation:

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{convert}

where \AS{T.id-subst} is the identity substitution which maps a term
in \AS{Γ} to itself, defined as:

\ExecuteMetaData[StateOfTheArt/Closure.tex]{id-subst}

We call the other end of the spectrum \textit{minimising closure
  conversion}. Its implementation in Agda is rather more involved and
is described in the next section.

\subsection{Minimising closure conversion}
\label{sec:minim-clos-conv}

Minimising closure conversion is given by the following deduction
rules, where a statement \AS{Γ ⊢ e : σ ↝ Δ ⊢ e' : σ} should be read as:
`the term \AS{e} of type \AS{σ} in the context \AS{Γ} can be closure
converted to the term \AS{e'} in \AS{Δ}`:

\begin{minipage}{.5\textwidth}
  \[
    \frac
    {}
    {\Gamma \vdash x : \sigma \leadsto \emptyset , x : \sigma \vdash x : \sigma}
    \;\text{(min-V)}
  \]
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \[
    \frac
    {
      \begin{matrix}
        \Gamma \vdash e_1 : \sigma \to \tau \leadsto \Delta_1 \vdash
        e_1' : \sigma \to \tau \\
        \Gamma \vdash e_2 : \sigma \leadsto \Delta_2 \vdash e_2' :
        \sigma \\
        \Delta = merge \; \Delta_1 \; \Delta_2
      \end{matrix}
      }
    {\Gamma ⊢ e_1 e_2 : \tau  \leadsto \Delta  \vdash e_1' e_2' : \tau}
     \;\text{(min-A)}
  \]
\end{minipage}

\[
  \frac {\Gamma , x : \sigma ⊢ e : \tau \leadsto \Delta , x : \tau
    \vdash e : \tau \quad \quad e_{id} = subst ( \Delta \subseteq
    \Delta )}
  {\Gamma \vdash \lambda x : \sigma . e : \sigma \to \tau \leadsto
    \Delta \vdash \langle\langle \lambda x : \sigma . e \; , \; e_{id}
    \rangle\rangle : \sigma \to \tau} \; \text{(min-L)}
\]

\textbf{min-V}: Any variables can be typed in a singleton context
containing just the type of the variable.

\textbf{min-A}: If the conversion \AS{e₁'} of \AS{e₁} can be typed in
$\Delta_1$, and the conversion \AS{e₂'} of \AS{e₂} can be typed in \AS{Δ₂},
then the application \AS{e₁' e₂'} can be typed in \AS{Δ}, where \AS{Δ}
is the result of merging \AS{Δ₁} and \AS{Δ₂}.

\textbf{min-L}: If the conversion \AS{e'} of the abstraction body
\AS{e} can be typed in context \AS{σ ∷ Δ} (or $\Delta, x : \sigma$,
using the notation with names), then the closure resulting from the
conversion of the abstraction can be typed in \AS{Δ}, and it has the
identity environment $\Delta \subseteq \Delta$.

To formalise this conversion in Agda, we need several helper
definitions.

\subsubsection{Merging subcontexts}
\label{sec:merging-subcontexts}

The deduction rules for minimising closure conversion contained
statements of the form \AS{Δ ⊆ Γ}, which reads: `\AS{Δ} is a
subcontext of \AS{Γ}`. Since in this development, a context is just a
list of types, the notion of subcontexts can be captured with the
\AS{\_⊆\_} (sublist) relation from Agda's standard library. The
inductive definition of the relation is:

\ExecuteMetaData[StateOfTheArt/Sublist.tex]{sublist}

This project's contribution is to define the operation of merging two
subcontexts. Given contexts \AS{Γ}, \AS{Δ}, and \AS{Δ₁} such that
\AS{Δ ⊆ Γ} and \AS{Δ₁ ⊆ Γ}, the result of merging the subcontexts
\AS{Δ} and \AS{Δ₁} is a context \AS{Γ₁} which satisfies the following
conditions:

\begin{enumerate}
\item It is contained in the big context: \AS{Γ₁ ⊆ Γ}.
\item It contains the small contexts: \AS{Δ ⊆ Γ₁} and \AS{Δ₁ ⊆ Γ₁}.
\item The proof that \AS{Δ ⊆ Γ} obtained by transitivity from \AS{Δ ⊆
    Γ₁} and \AS{Γ₁ ⊆ Γ} is the same as the input proof that \AS{Δ ⊆
    Γ}; similarly for \AS{Δ₁ ⊆ Γ}.
\end{enumerate}

All those requirements are captured by the following dependent record
in Agda:

\ExecuteMetaData[StateOfTheArt/SubContext.tex]{sublistsum}

The type of the function which merges two subcontexts can be stated
as:

\ExecuteMetaData[StateOfTheArt/SubContext.tex]{merge}

We argue that the type of the function completely captures its
behaviour (TODO how would we prove this?). The fact that a type can
completely capture the behaviour of a function is a remarkable feature
of programming with dependent types. Even more remarkable is the fact
that the logical properties of \AS{Γ₁} are useful computationally. E.g
the proof that \AS{Δ ⊆ Γ₁} determines a renaming from \AS{Δ} to
\AS{Γ₁}, which is used in the minimising closure conversion
algorithm. A further example: the fact that \AS{⊆-trans Δ⊆Γ₁ Γ₁⊆Γ ≡
  Δ⊆Γ} is used in proofs of certain equivalences involving subcontexts
and renaming.

\subsection{Agda implementation of minimising closure conversion}
\label{sec:agda-impl-minim}

Recall that terms of our intermediate languages are explicitly typed
in a given context. For that reason, the result type of minimising
closure conversion must be existentially quatified over a
context. In fact, the context should be a subcontext of the input
context \AS{Γ}. This is captured with the dependent record
\AS{\_⊩\_}:

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{ex-subctx-trm}

For example, a term \AS{N} in a context \AS{Δ} which is a subcontext
of \AS{Γ} by \AS{Δ⊆Γ}, would be constructed as \AS{∃[ Δ ] Δ⊆Γ ∧ N}.

With this data type, the type of the minimising closure conversion
function is:

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{min-cc}

The function definition is by cases:

\textbf{Variable case}

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{min-cc-v}

Following \textit{min-V}, a variable is typed in a singleton
context. The proof of the subcontext relation is computed from the
proof of the context membership by a function \AS{Var→⊆}.

\textbf{Application case}

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{min-cc-a}

Given an application \AS{e₁ e₂}, \AS{e₁} and \AS{e₂} are closure
converted recursively, resulting in terms \AS{e₁'} and \AS{e₂'}, which
are typed in \AS{Δ₁} and \AS{Δ₂}, respectively. Following
\textit{app-V}, the result of closure-converting the application is
typed in the context \AS{Δ}, which is the result of merging \AS{Δ₁}
and \AS{Δ₂}. As terms are explicitly typed in a context, \AS{e₁'} and
\AS{e₂'} have to be renamed from \AS{Δ₁} to \AS{Δ}, and from \AS{Δ₂}
to \AS{Δ}, respectively. A renaming environment is computed from a
subcontext relation proof by the function \AS{⊆→ρ} which is given by:

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{subctx-to-ren}

\textbf{Abstraction case}

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{min-cc-l}

Following \textit{min-A}, the result of closure-converting an
abstraction depends on the result \AS{N†} of closure-clonverting its
body. A recursive call on the body of the abstraction yields a term
typed in some context \AS{Δ}. But looking at the typing rule for
closures (\textit{T-clos}), the closure body is typed in a context
\AS{σ ∷ Δ₁} (or \AS{Δ₁, x : σ} using named variables), where \AS{σ} is
the type of the last bound variable and \AS{Δ₁} is the context
corresponding to the closure environment. Thus, we need a way of
decomposing \AS{Δ} into \AS{σ} and \AS{Δ₁}, together with an
appropriate proof of membership in the input context \AS{Γ}.

This task is achieved by the function \AS{adjust-context}:

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{adjust-context-f}

whose specification is captured by its return type which uses the
dependent record \AS{AdjustContext}:

\ExecuteMetaData[StateOfTheArt/ClosureConversion.tex]{adjust-context-t}

The specification is: given \AS{Δ ⊆ A ∷ Γ}, there exists a context
\AS{Δ₁} such that \AS{Δ₁ ⊆ Γ} and \AS{Δ ⊆ A ∷ Δ₁}, such that the
proof \AS{Δ ⊆ A ∷ Γ} obtained by transitivity is the same as the input
proof.

The proof that \AS{Δ ⊆ A ∷ Δ₁} is used to rename \AS{N†} so that the
final inherently-typed term is well-typed.

\chapter{Bisimulation}

In the previous chapters, we defined the source and target languages
of the closure conversion, together with reduction rules for each, and
a translation function from source to target.

Our implementation of closure conversion is type and scope-preserving
by construction. The property of type preservation would be considered
a strong indication of correctness in a real-world compiler, but in
this theoretical development which deals with a small, toy language,
we prove stronger correctness properties which speak about operation
correctness.

One such property of operational correctness of a pair of languages is
bisimulation. Intuition about bisimulation is captured by a slogan:
pairwise similar terms reduce to pairwise similar terms. 

\section{Similarity relation}

Before we can precisely define
bisimulation, we need a definition of similarity between source
and target terms of closure conversion.

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{tilde}

\begin{definition}
  Given a source language term \AS{M} and a target language term \AS{M†},
  the similarity relation \AS{M \ti M†} is defined inductively as
  follows:

  \begin{itemize}
  \item
    (Variable) For any given variable (proof of context membership) \AS{x}, we have
    \AS{S.` x \ti T.` x}.

  \item
    (Application) If \AS{M \ti M†} and \AS{N \ti N†},
    then \AS{M S.· N \ti M† T.· N†}.

  \item
    (Abstraction) If \AS{N \ti T.subst (T.exts E) N†},
    then \AS{λ N \ti ⟪ N† , E ⟫}.
  \end{itemize}
\end{definition}

We unpack the definition here. Recall that in our definition of
closure conversion, source and target languages share the same (meta)
type of (object) types, contexts, and variables (proofs of context
membership). In fact, similarity is only defined for source and target
terms of the same type in the same context (this is explicit in the Agda
definition). 

Therefore, similarity of (syntactic) variables can be defined in terms
of identity of proofs of membership.

Similarity of applications is defined by congruence.

Finally, the non-trivial case of abstractions. What are the necessary
conditions for \AS{λ N \ti ⟪ N† , E ⟫}, where \AS{λ N} and 
\AS{⟪ N† , E ⟫} are defined in context \AS{Γ}? Clearly, we cannot require
that \AS{N \ti N†}, as the context \AS{Δ} in which the closure body is
defined is existentially quantified. However, recall that the closure environment
\AS{E} is defined as a substitution from \AS{Δ} to \AS{Γ}. Applying
this substitution to the closure body (\AS{T.subst (T.exts E) N†})
results in a term in \AS{Γ} which can be in a similarity relation with
\AS{N}, and this is precisely what we require in the definition.

(Note: the \AS{exts} accounts for the fact that the closure body is
defined in the context \AS{Δ} extended by the variable bound by the
abstraction, similarly to the lambda body.)

It is natural to ask what the relationship between a closure
conversion function and the similarity relation. Is the similarity
relation as graph relation of a closure conversion function? It is
not. Recall that closure conversion can be implemented by any function
which takes source terms to target terms and preserves the type and
context. But an implementation has freedom in how it deals with
closure environments; the meta language type of closures only requires
that the environment is \textit{some} substitution from the closure
body context \AS{Δ} to the outer context \AS{Γ}.

For example, the closure conversion transformation we described in
Chapter ??? had the property that closure environments were minimal:
they only contained parts of context actually used by the closure
body. In contrast, the simplest possible closure conversion could use
identity environments:

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{convert}

We require that for every well-behaved closure conversion function \AS{c}, any
source term \AS{N} is similar to the result of its translation with
\AS{c}: \AS{N \ti c N}. This is indeed the case for the \AS{convert}
function. The proof is by straightforward induction; in the
abstraction case, we need to argue that applying an identity substitution
leaves the term unchanged.

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{graph}

A similar result could be obtained for the closure conversion
algorithm which minimises environments from Chapter ???, but the proof
would be longer.

However, given \AS{M \ti M†}, it is not
necessarily the case that \AS{M† ≡ c M} for any \textit{fixed}
function \AS{c}; instead, \AS{M† ≡ c M} holds for \textit{some}
function \AS{c}. Therefore, the similarity relation is not the graph
relation of any \textit{specific} conversion \AS{c}.

Having defined the notion of similarity, we are in a position to
define bisimulation.

\section{Bisimulation}

Bisimulation is a two-way property which is defined in terms of a
simpler one-way property of simulation.

\begin{definition}{}
Given two languages \AS{S} and \AS{T} and a similarity relation
\AS{\_\textasciitilde\_}
between them, \AS{S} and \AS{T} are in \textbf{simulation} if and only
if the following holds:
Given source language terms \AS{M} and \AS{N}, and a target language
term \AS{M†} such that \AS{M} reduces to \AS{N} in a single step (\AS{M —→ N}) and
\AS{M} is similar to \AS{M†} (\AS{M \~ M†}), there exists a target
language term \AS{N†} such that \AS{M†} reduces to \AS{N†} in some
number of steps (\AS{M† —→* N†}) and \AS{N} is similar to \AS{N†} (\AS{N \~ N†}).
\end{definition}

\begin{definition}
Given two languages \AS{S} and \AS{T},  \AS{S} and \AS{T} are in a
\textbf{bisimulation} if and only if \AS{S} is in a simulation with \AS{T} and
\AS{T} is in a simulation with \AS{S}.
\end{definition}

The essence of simulation can be captured in a diagram.

TODO diagram here

TODO give names to the source and target langs

In fact, our source and target languages of closure conversion have a
stronger property: \textit{lock-step} bisimulation, which is defined
in terms of \textit{lock-step} simulations. A lock-step simulation is
one where for each reduction step of the source term, there is exactly
one corresponding reduction step in the target language. We illustrate
this at another diagram:

TODO another diagram

Before we can prove that λ and λT are in simulation, we need three
lemmas:

\begin{enumerate}
\item
  Values commute with similarity. If \AS{M \ti M†} and \AS{M} is a
  value, then \AS{M†} is also a value.

\item
  Renaming commutes with similarity. If \AS{ρ} is a renaming from
  \AS{Γ} to \AS{Δ}, and \AS{M \ti M†} are similar terms in the context
  \AS{Γ}, then the results of renaming \AS{M} and \AS{M†} with \AS{ρ}
  are also similar: \AS{S.rename ρ M \ti T.rename ρ M†}.

\item
  Substitution commutes with similarity. Suppose \AS{ρ} and \AS{ρ†} are two
  substitutions which take variables \AS{x} in \AS{Γ} to terms in \AS{Δ},
  such that for all \AS{x} we have that \AS{lookup ρ x \ti lookup ρ†
    x}. Then given similar terms \AS{M \ti M†} in \AS{Γ}, the results
  of applying \AS{ρ} to \AS{M} and \AS{ρ†} to \AS{M†} are also
  similar: \AS{S.subst ρ M \ti T.subst ρ† M†}.
\end{enumerate}

The proof that values commute with similarity is straightforward.

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{val-comm} 

Before we will be able to prove the lemmas about renaming and substitution, we need an interlude
where we discuss so-called fusion lemmas for the closure language λT.

TODO acknowledge PLFA here


\section{Fusion lemmas for the closure language λT}

When studying the meta-theory of a calculus, one systematically needs
to prove fusion lemmas for various semantics (TODO wording) (recall
that a semantic is a traversal of a term). A fusion
lemma relates three semantics: the pair we sequence and their
sequential composition (TODO wording). In our proof of bimisulation
for closure conversion, we have interactions of two semantics:
renaming and substitution. In fact, we need fusion lemmas for all four
combinations of them:

\begin{enumerate}
  \item A renaming followed by a renaming,
  \item A renaming followed by a substitution,
  \item A substitution followed by a renaming,
  \item A substitution followed by a substitution.
  \end{enumerate}

As it turns out, the first composition (a renaming followed by a
renaming) is equivalent to a renaming, and the other three compositions
are equivalent to substitutions. 

We state the results as signatures of Agda functions, using the
environment combinators \AS{\_<\$>\_} and \AS{select} we described in Section~??.

\ExecuteMetaData[StateOfTheArt/Closure-Thms.tex]{rename-rename}
\ExecuteMetaData[StateOfTheArt/Closure-Thms.tex]{subst-rename}  
\ExecuteMetaData[StateOfTheArt/Closure-Thms.tex]{rename-subst} 
\ExecuteMetaData[StateOfTheArt/Closure-Thms.tex]{subst-subst}

The Agda proofs of those four lemmas can be found in the appendix
(TODO ref); here we outline the proof structure, analyse one of the
four lemmas, and compare
fusion lemmas for \lcl with the corresponding lemmas for STLC.

A generic technique to prove fusion lemmas for STLC,
including the ones about renaming and substitution, is one of the main
contributions of ACMM \cite{DBLP:conf/cpp/Allais0MM17}. Their proof
uses logical relations (TODO explain) and it relies on the invariant
that corresponding environment values are in appropriate relations,
including when environments are extended when going under a
binder. Maintaining this invariant is possible thanks to the generic
framework for writing traversals (semantics) introduced by ACMM.

As it turns out, fusion lemmas for the closure language are simpler,
as they do not require the logical relation machinery of ACMM. This is
because renaming and substitution in \lcl `do not go under binders`,
as can be seen from their definitions:

\ExecuteMetaData[StateOfTheArt/Closure.tex]{syntactic}
\ExecuteMetaData[StateOfTheArt/Closure.tex]{rename}
\ExecuteMetaData[StateOfTheArt/Closure.tex]{subst}

For both renaming and substitution, in the closure case (\AS{L}), the
closure body is left untouched; only the closure environment is
modified.

We are now ready to take a closer look at the proof of the fusion
lemma for a renaming followed by a subsitution:

\ExecuteMetaData[StateOfTheArt/Closure-Thms.tex]{subst-rename} 
\ExecuteMetaData[StateOfTheArt/Closure-Thms.tex]{subst-rename-proof} 

The proof is by induction on the typing derivation of the term:

\begin{itemize}
\item In the variable case, the LHS and the RHS normalise to the same
  term, so \AS{refl} suffices.
\item In the application case, the proof is by congruence (TODO
  wording).
\item In the closure case, the proof is also by congruence, but
  an equational proof is required to show that the LHS and RHS act the
  same of the environment \AS{E}.
\end{itemize}

The equational proof for \AS{E} proceeds as follows:

\begin{enumerate}
\item It uses the fact that function composition \AS{\_∘\_}
  distributes through mapping over environments \AS{\_<\$>\_}: we have
  \AS{f <\$> g <\$> E ≡ f ∘ g <\$> E} which is capture by the lemma
  \AS{< \$>-distr},
\item It uses the fact that when \AS{f} and \AS{g} are extensionally
  equal (\AS{∀ \{x\} → f x ≡ g x}), then \AS{f <\$> E ≡ g <\$> E} which
  is captured by the lemma \AS{<\$>-fun},
\item \AS{<\$>-fun} is instantiated with the inductive hypothesis.
\end{enumerate}

Unfortunately, Agda does not recognise our fusion lemmas as
terminating, and we were unable to provide a termination proof. Still,
we believe that the proof function is in fact terminating.

\section{Back to \ti rename and \ti subst}

Recall the two result which we said would be needed for showing
bisimulation. We start with:

\textit{Renaming commutes with similarity.} If \AS{ρ} is a renaming from
\AS{Γ} to \AS{Δ}, and \AS{M \ti M†} are similar terms in the context
\AS{Γ}, then the results of renaming \AS{M} and \AS{M†} with \AS{ρ}
are also similar: \AS{S.rename ρ M \ti T.rename ρ M†}.

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{rename-comm} 

The proof is by induction on the similarity relation. The variable and
application cases are easy, but as ever, the abstraction case is worth
looking at. Recall that a source abstraction is similar to the target
closure \AS{S.L N \ti T.L N† E} when \AS{N \ti T.subst (T.exts E) N†}
by the inductive constructor \AS{\ti L}.

In the abstraction case, we have that

\AS{S.L N \ti T.L N† E} (1)

We need to show that

\AS{S.rename ρ (S.L N) \ti T.rename ρ (T.L N† E)} (2)

but (2) simplifies to

\AS{S.L (S.rename (S.exts ρ) N) \ti T.L N† (T.rename ρ <\$> E)} (3)

which holds by \AS{\ti L} when the following holds

\AS{S.rename (S.exts ρ) N \ti T.subst (T.exts (T.rename ρ <\$> E)) N†} (4)

On the other hand, from (1) by \AS{\ti L} we have that

\AS{N \ti T.subst (T.exts E) N†} (5)

Applying the induction hypotesis to (5), we get

\AS{S.rename (S.exts ρ) N \ti T.rename (S.exts ρ) (T.subst (T.exts E) N†)} (6)

Thus we require (4) and have (6), so to complete the proof, we need to
show that 

\AS{T.subst (T.exts (T.rename ρ <\$> E)) N† ≡ T.rename (S.exts ρ) (T.subst
  (T.exts E) N†)}

or, in Agda

\ExecuteMetaData[StateOfTheArt/Closure-Thms.tex]{lemma-ren-comm}

This indeed holds, and the proof uses the fusion lemma lemmas for
renaming and substitution in several places.

The remaining result to prove is quite similar, but the concept of a
pointwise similar substitution makes it worth analysing.

\textit{Substitution commutes with similarity.} Suppose \AS{ρ} and \AS{ρ†}
are two 
substitutions which take variables \AS{x} in \AS{Γ} to terms in \AS{Δ},
such that for all \AS{x} we have that \AS{lookup ρ x \ti lookup ρ†
  x}. Then given similar terms \AS{M \ti M†} in \AS{Γ}, the results
of applying \AS{ρ} to \AS{M} and \AS{ρ†} to \AS{M†} are also
similar: \AS{S.subst ρ M \ti T.subst ρ† M†}.

The notion of pointwise-similar substitutions \AS{ρ} and \AS{ρ†} from
\AS{Γ} to \AS{Δ}
can be captured by a function which, for each variables \AS{x} in
\AS{Γ}, produces a proof that that the corresponsing terms are
similar: \AS{lookup ρ x \ti lookup ρ† x}. We encapsulate this function
in an Agda record:

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{pointwise-sim}

The notion of a pointwise relation between environments (like
substitutions) is used by ACMM to prove synchronisation and fusion
lemmas for STLC. Unlike for STLC, the closure language \lcl does not
require us to prove that pointwise similarity is preserved as a
traversal goes under a binder.

We can, however, show that pointwise similarity is preserved by
applying \AS{exts} to both substitutions:

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{pointwise-sim-exts}

In fact, exteding pointwise-similar substitutions with similar terms
preserves pointwise similarity:

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{pointwise-sim-extend}

With the notion of pointwise similarity, we can prove that
substitution commutes with similarity:

\ExecuteMetaData[StateOfTheArt/Bisimulation.tex]{subst-comm}

TODO finish

\chapter{Relationship to the UG4 project}

This work is a natural continuation of the UG4 project, but it
admittedly takes the project in a new direction. In terms of its
goals, the UG4 project was concerned with program
derivations. Derivations are distinct from program transformations or
traversals, as found in compilers or in this UG5 project.

Program transformations like closure conversion, or continuation
passing style (CPS) transformation, have two distinct characteristics.

\section{Program transformation vs derivation}
\label{sec:progr-transf-vs}

Firstly, the source and target languages can be different, e.g. the
source language may have lambda abstractions with free variables, and
the target language may have closures with environments. Of course,
many transformations in compilers happen within the same language,
e.g. constant expression folding.

Secondly, compiler transformations are usually characterised by
replacing one program construct with another, in a one-way fashion,
e.g. lambda abstractions with closures. It would be strange if changes
happened both ways. One might imagine a language with both lambda
abstractions and closures, and a transformation which replaces some
closures with lambda abstractions, and some abstraction with closures,
according to arbitrary rules. It is difficult to see how such a
transformation would be useful in a compiler.

Of course, one might imagine a transformation which replaces some
occurrences of constructs A and constructs B, and vice versa, in order
to optimise the program. But any such optimising transformation is
still guided by some measure of performance.

In contrast, program derivation consists of transforming a program in
arbitrary places, and using arbitrary rules, with a specific goal of
obtaining one program from another, so that the obtained program has
some desirable features, like efficiency, or even faster asymptotic
running time. Importantly, the derivation happens within a single
language. (TODO but specification can be in terms of relations, which
are not part of the language).

\section{Program derivations in the UG4 project}
\label{sec:progr-deriv-ug4}

The UG4 project analyses two instances of program derivation in
detail. The first one is a derivation of an efficient implementation
of the maximum segment sum problem (MSS). While the input
specification (which is also a runnable program) runs in cubic time in
the length of the input list, the output program runs in linear
time. The asymptotic speed-up is achieved by applying several "rewrite
rules" involving higher-order functions on lists such as map, foldr,
and filter.

The second case study involved a derivation for a program for
matrix-vector multiplication. The input program takes a dense matrix,
and the output program takes a sparse matrix in the compressed sparse
row (CSR) format. Or, to be precise, the input program which acts on a
dense matrix, is transformed into a composition of two programs: (a) a
conversion from a dense to a CSR-sparse matrix, and (b) a
matrix-vector multiplication program which acts on a CSR-sparse
matrix. This is because, as a rule, the input and output types of the
program must stay the same in the course of the derivation. This
second derivation was similarly accomplished with "rewrite rules"
involving higher-order functions.

\section{Rewrite rules}
\label{sec:rewrite-rules}

The notion of a rewrite rule is central to program derivation. A
classical example of a rewrite rule for a function program is one
stating: "a composition of mappings is a mapping with a composition":

\begin{verbatim}
forall f g xs. map f (map g xs) == map (f . g) xs (*)
\end{verbatim}

where f, g, xs are metavariables. An application of a rewrite rule
consists of unifying the LHS of the rule with a subterm of the program
(and thus obtaining a substitution σ), and then replacing the
subterm with the RHS of the rule, instantiated with the substitution
σ.

Notably, such simple form of a rewrite rule only supports first-order
abstract syntax trees, but not higher-order abstract syntax. (TODO
elaborate on what it would mean to have a context and go under a
binder in a rewrite rule). (TODO can't express conditions)

In their simplest form, a program derivation is a sequence of
intermediate forms of the program, intertwined with rewrite rules
which justify each step of the derivation. For examples, the reader
may consult the UG4 project \cite{TODO}.

\section{Program derivations in compilers and their limitations}
\label{sec:progr-deriv-comp}

While we argued that compiler transformations and program derivations
are distinct in their character, the lines may arguably be blurry at
times. A good example of this is the support for rewrite rules in GHC,
a Haskell compiler. A Haskell programmer may specify a rule like (*)
as part of the code, and in one of early compilation passes, GHC will
apply the rule wherever possible (i.e. replace the occurrences of the
LHS with the RHS). Such compiler pass may be considered an instance of
program derivation, and it would go some way towards deriving the
aforementioned efficient implementation for the maximum segment sum
problem (MSS).

However, rewriting as implemented in compilers is too limited to carry
out most derivations. To see one limitation, consider a derivation
which needs to apply the rule (*) right to left: replace an occurrence
of the RHS with the LHS. But clearly, unguided application of rewrite
rules can only be one way, otherwise it would not terminate.

Another limitation is the fact that "the right" derivation can require
that rules be applied in a specific order. Thus, rewriting a program
becomes a search problem, where rewrites are applied until a program
satisfying some (performance) objective is found. This is the approach
taken by the Lift compiler \cite{TODO}

Yet another limitation is that many derivations elude the notion of an
objective function, thus rendering a search futile. It seems that
human insight is required to guide a derivation.

A final limitation is that there are conceivable rewrite rules which
are only applicable when certain conditions are met. Such conditions
could range algebraic properties of operators to general predicates
and relations on terms. And these are undecidable in general (TODO how
to phrase this).

These considerations, taken together, mean that the technique of
program derivation is more useful to the programmer than the
compiler. Benefits of employing a sort of program derivation (more or
less formal) for the programmer include: (a) a structured process of
obtaining an implementation from specification, (b) greater confidence
about correctness of an implementation, (c) possibility of discovering
further optimisations, and finally (d) a framework for a proof of
correctness. This last use case could be explained as follows: suppose
we can prove the correctness of the "specification" program, and the
correctness of each rewrite rule. Then we can obtain correctness of
the "implementation" program.

\section{Implementation of rewriting in the UG4 project}
\label{sec:impl-rewr-ug4}

The UG4 project included a purpose-built framework for specifying
derivations. The framework included:

\begin{enumerate}
\item A simple functional language with parametricity. The language is
point-free, that is, based on function composition rather than on
lambda abstractions. This was because variables and abstraction are
difficult to implement correctly, as demonstrated by this UG5 project,
and even more difficult to rewrite.

\item A type-checker for the language.

\item Rewriting functionality and declaring derivations as sequences of
  rewrites.

\item An interpreter for the language, which was used to empirically
  verify claims about performance gains from derivations.
\end{enumerate}

Writing the framework was a good exercise in implementing routine
parts of compiler front-ends, such as type checking and
unification. Writing it in Scala made sense given the stretch
objective of compiling the language to Lift, which was not realised,
however.

Importantly, rewrite rules were stated without justification, much as
postulates in Agda. One could prove the rules externally – but then
one is pressed to ask, why not express a derivation in a proof
assistant, which supports unification and rewriting natively? Indeed,
with hindsight, we can say with certainty that a proof assistant is
perfectly suited for the job, its only downside being that it requires
considerable expertise, which I did not have during my fourth
year. (TODO I/me?)

\section{Program derivation in Agda: sparse matrix-vector
  multiplication}
\label{sec:progr-deriv-agda}

To complete last year's work, we conduct a derivation of the program
for matrix-vector multiplication which acts on CSR-sparse
matrices. Unlike last year, we can now provide proofs of individual
rewrite rules. Indeed, some proofs are quite involved. TODO whether
and how to do it.

% use the following and \cite{} as above if you use BibTeX
% otherwise generate bibtem entries
\bibliographystyle{plain}
\bibliography{main}

\end{document}
